{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a062a1-c42c-408b-906e-99523f0f9066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV: gs://final_data_kgc2/Final_data/Pigeon_pixel_80m_binary.csv  (use_gcsfs=False)\n",
      "Loaded CSV via google-cloud-storage client\n",
      "Found 281 .tif files in gs://final_data_kgc2/Final_data/Pigeon_80m/\n",
      "Parsed 281 unique satellite dates.\n",
      "Filtered to satellite days: 2364855 → 2364855\n",
      "Built 2214239 samples (dropped_no_history=0, dropped_no_future=150616)\n",
      "Dropped 468717 samples due to NaNs in features or y_raw\n",
      "Saved: gs://final_data_kgc2/models/Pigeon_binary/samples_all_strict.csv\n",
      "Saved: gs://final_data_kgc2/models/Pigeon_binary/samples_train.csv\n",
      "Saved: gs://final_data_kgc2/models/Pigeon_binary/samples_val.csv\n",
      "Saved: gs://final_data_kgc2/models/Pigeon_binary/samples_test.csv\n",
      "Split → train=948281, val=74421, test=495447\n",
      "Saved imputer to: gs://final_data_kgc2/models/Pigeon_binary/iterative_imputer_binary.joblib\n",
      "Standardizing columns: ['chl_a_t', 'histmean_Day', 'histmean_Month', 'histmean_Year', 'histmean_ODO mg/L', 'histmean_ODO % local', 'histmean_Turbidity NTU', 'histmean_Water Temperature °C', 'histmean_SpCond µS/cm', 'histmean_pH', 'histmean_ORP mV', 'histmean_Chlorophyll µg/L', 'histmean_BGA-PC µg/L', 'histmean_Temp -1 meter °C', 'histmean_Temp -2 meter °C', 'histmean_Temp-3 meter °C', 'histmean_Temp - 4 meter °C', 'histmean_Temp -5 meter °C', 'histmean_Temp -6 meter °C', 'histmean_DO - 6 meter mg/L', 'histmean_Temp -7 meter °C', 'histmean_DO - 7 meter mg/L', 'histmean_Temp -8 meter °C', 'histmean_DO - 8 meter mg/L', 'histmean_Air Temperature °C', 'histmean_Barometric Pressure hPa', 'histmean_Abs Humidity g/m3', 'histmean_Relative Humidity %', 'histmean_Air Density kg/m3', 'histmean_Wind Speed km/h', 'histmean_Wind Gust km/h', 'histmean_Wind Direction deg', 'histmean_Layer1_Weighted_Chl_a', 'histmean_Layer2_Weighted_Chl_a', 'histmean_Chl_a_binary', 'histmean_Chl_a']\n",
      "Saved scaler to: gs://final_data_kgc2/models/Pigeon_binary/standard_scaler_binary.joblib\n",
      "Saved processed dataset to: gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Pigeon Lake data preparation — GCS-friendly, binary-ready (storage-client preferred)\n",
    "\n",
    "Updates in this copy:\n",
    "- Default CSV / tif_dir / out_dir set to your bucket (kgc2-arch) but targeting Pigeon Lake naming.\n",
    "- upload_file_to_gcs now checks that the destination bucket exists and raises a\n",
    "  clear error if not (instead of surfacing a raw 404 stack trace).\n",
    "- All other behavior unchanged: falls back to CSV dates if no tifs, produces\n",
    "  processed_dataset_binary.npz and joblibs named with \"binary\".\n",
    "\n",
    "Run example:\n",
    "python3 pigeon_prepare_binary_simple_gcs.py \\\n",
    "  --csv \"gs://final_data_kgc2/Final_data/Pigeon_pixel_80m_binary.csv\" \\\n",
    "  --tif_dir \"gs://final_data_kgc2/Final_data/Pigeon_80_binary/\" \\\n",
    "  --out_dir \"gs://final_data_kgc2/models/Pigeon_binary/\" \\\n",
    "  --random_state 0\n",
    "\n",
    "If you prefer to write outputs to a local directory, pass --out_dir /tmp/pigeon_out\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import joblib\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "# IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optional: GCS support (gcsfs)\n",
    "try:\n",
    "    import gcsfs\n",
    "except Exception:\n",
    "    gcsfs = None\n",
    "\n",
    "# ---------------------------\n",
    "# Defaults (set to your kgc2-arch bucket)\n",
    "# ---------------------------\n",
    "DEFAULT_CSV = \"gs://final_data_kgc2/Final_data/Pigeon_pixel_80m_binary.csv\"\n",
    "SAT_TIF_DIR = \"gs://final_data_kgc2/Final_data/Pigeon_80m/\"\n",
    "DEFAULT_OUT_DIR = \"gs://final_data_kgc2/models/Pigeon_binary/\"\n",
    "\n",
    "META_COLS = [\"Date\", \"X\", \"Y\", \"Pixel_ID\"]\n",
    "TARGET_COL = \"Chl_a\"\n",
    "BLOOM_THRESHOLD_DEFAULT = 20.0  # used only if binary label must be derived\n",
    "\n",
    "# ---------------------------\n",
    "# GCS / IO helpers (storage-client preferred)\n",
    "# ---------------------------\n",
    "def is_gcs_path(path: str) -> bool:\n",
    "    return isinstance(path, str) and path.startswith(\"gs://\")\n",
    "\n",
    "def gcs_join(prefix: str, *parts: str) -> str:\n",
    "    p = prefix.rstrip('/')\n",
    "    for part in parts:\n",
    "        p = p.rstrip('/') + '/' + str(part).lstrip('/')\n",
    "    return p\n",
    "\n",
    "def upload_file_to_gcs(local_path: str, gs_uri: str, requester_pays_project: str | None = None):\n",
    "    \"\"\"Upload local_path -> gs:// path using google-cloud-storage client (preferred).\n",
    "    This function now checks that the destination bucket exists and raises a clear error\n",
    "    with guidance if it does not.\n",
    "    \"\"\"\n",
    "    if not is_gcs_path(gs_uri):\n",
    "        raise ValueError(\"gs_uri must be a gs:// path\")\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "    except Exception as e:\n",
    "        if gcsfs is not None:\n",
    "            # best-effort fallback to gcsfs\n",
    "            fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "            fs.put(local_path, gs_uri)\n",
    "            return\n",
    "        raise RuntimeError(\"Install google-cloud-storage or gcsfs to upload to GCS\") from e\n",
    "\n",
    "    client_kwargs = {}\n",
    "    if requester_pays_project:\n",
    "        client_kwargs[\"project\"] = requester_pays_project\n",
    "    client = storage.Client(**client_kwargs)\n",
    "\n",
    "    _, rest = gs_uri.split(\"gs://\", 1)\n",
    "    bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "\n",
    "    # explicit bucket existence check to give a helpful error instead of 404 stacktrace\n",
    "    bucket = client.lookup_bucket(bucket_name)\n",
    "    if bucket is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Destination bucket does not exist: gs://{bucket_name}\\n\\n\"\n",
    "            \"Fixes:\\n\"\n",
    "            f\" - Use an existing bucket (e.g. --out_dir gs://kgc2-arch/models/Pigeon_binary/),\\n\"\n",
    "            \" - Or create the bucket in your project with gsutil mb gs://<bucket>/,\\n\"\n",
    "            \" - Ensure the service account running this job has storage permissions on that bucket.\"\n",
    "        )\n",
    "\n",
    "    blob = bucket.blob(blob_path)\n",
    "    try:\n",
    "        blob.upload_from_filename(local_path)\n",
    "    except Exception as e:\n",
    "        # re-raise with helpful context\n",
    "        raise RuntimeError(f\"Upload failed for {local_path} -> {gs_uri}: {e}\") from e\n",
    "\n",
    "def save_local_or_gcs(local_src_path: str, out_dir: str, dest_basename: str | None = None, requester_pays_project: str | None = None) -> str:\n",
    "    \"\"\"Move local_src_path into out_dir (local) or upload to gs://out_dir/dest_basename. Returns destination path.\"\"\"\n",
    "    if dest_basename is None:\n",
    "        dest_basename = os.path.basename(local_src_path)\n",
    "    if is_gcs_path(out_dir):\n",
    "        gs_dest = gcs_join(out_dir, dest_basename)\n",
    "        upload_file_to_gcs(local_src_path, gs_dest, requester_pays_project=requester_pays_project)\n",
    "        try:\n",
    "            os.remove(local_src_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return gs_dest\n",
    "    else:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        dst = os.path.join(out_dir, dest_basename)\n",
    "        shutil.move(local_src_path, dst)\n",
    "        return dst\n",
    "\n",
    "# ---------------------------\n",
    "# Robust CSV loader + listing (storage-client preferred)\n",
    "# ---------------------------\n",
    "def load_csv(csv_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV from local path or GCS.\n",
    "    use_gcsfs: if True attempt gcsfs first (may show gcsfs traces on bad credentials).\n",
    "    Otherwise prefer google-cloud-storage client (recommended on Google Cloud).\n",
    "    \"\"\"\n",
    "    print(f\"Loading CSV: {csv_path}  (use_gcsfs={use_gcsfs})\")\n",
    "    if not is_gcs_path(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.normalize()\n",
    "        return df\n",
    "\n",
    "    # Try google-cloud-storage client first (preferred)\n",
    "    if not use_gcsfs:\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "        except Exception as e:\n",
    "            # fallback to gcsfs if storage client not available\n",
    "            if gcsfs is not None:\n",
    "                use_gcsfs = True\n",
    "            else:\n",
    "                raise RuntimeError(\"Install google-cloud-storage or gcsfs to read CSV from GCS\") from e\n",
    "        if not use_gcsfs:\n",
    "            _, rest = csv_path.split(\"gs://\", 1)\n",
    "            bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "            client_kwargs = {}\n",
    "            if requester_pays_project:\n",
    "                client_kwargs[\"project\"] = requester_pays_project\n",
    "            client = storage.Client(**client_kwargs)\n",
    "            bucket = client.lookup_bucket(bucket_name)\n",
    "            if bucket is None:\n",
    "                raise FileNotFoundError(f\"Bucket not found or access denied: gs://{bucket_name}\")\n",
    "            blob = bucket.blob(blob_path)\n",
    "            if not blob.exists():\n",
    "                raise FileNotFoundError(f\"CSV blob not found: {csv_path}\")\n",
    "            data = blob.download_as_bytes()\n",
    "            df = pd.read_csv(io.BytesIO(data))\n",
    "            df.columns = [c.strip() for c in df.columns]\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.normalize()\n",
    "            print(\"Loaded CSV via google-cloud-storage client\")\n",
    "            return df\n",
    "\n",
    "    # If we fall through here, attempt gcsfs\n",
    "    if gcsfs is None:\n",
    "        raise RuntimeError(\"gcsfs is not installed; install gcsfs or enable google-cloud-storage\")\n",
    "    try:\n",
    "        fs_kwargs = {}\n",
    "        if requester_pays_project:\n",
    "            fs_kwargs[\"project\"] = requester_pays_project\n",
    "        fs = gcsfs.GCSFileSystem(token=\"google\", **fs_kwargs)\n",
    "        with fs.open(csv_path, \"rb\") as f:\n",
    "            df = pd.read_csv(f)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.normalize()\n",
    "        print(\"Loaded CSV via gcsfs\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"gcsfs read failed (and google-cloud-storage fallback wasn't used):\", repr(e))\n",
    "        raise\n",
    "\n",
    "def list_tif_files(tif_dir: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> list:\n",
    "    \"\"\"List .tif files either locally or from GCS. Prefers storage client unless use_gcsfs=True.\"\"\"\n",
    "    if not is_gcs_path(tif_dir):\n",
    "        return sorted([os.path.join(tif_dir, f) for f in os.listdir(tif_dir) if f.lower().endswith('.tif')])\n",
    "\n",
    "    # Prefer google-cloud-storage client\n",
    "    if not use_gcsfs:\n",
    "        try:\n",
    "            from google.cloud import storage\n",
    "        except Exception as e:\n",
    "            if gcsfs is not None:\n",
    "                use_gcsfs = True\n",
    "            else:\n",
    "                raise RuntimeError(\"Install google-cloud-storage or gcsfs to list GCS tifs\") from e\n",
    "        if not use_gcsfs:\n",
    "            _, rest = tif_dir.split(\"gs://\", 1)\n",
    "            bucket_name, _, prefix = rest.partition(\"/\")\n",
    "            client_kwargs = {}\n",
    "            if requester_pays_project:\n",
    "                client_kwargs[\"project\"] = requester_pays_project\n",
    "            client = storage.Client(**client_kwargs)\n",
    "            bucket = client.lookup_bucket(bucket_name)\n",
    "            if bucket is None:\n",
    "                raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "            prefix = prefix.rstrip('/') + '/'\n",
    "            blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "            files = []\n",
    "            for b in blobs:\n",
    "                if b.name.lower().endswith('.tif'):\n",
    "                    files.append(\"gs://\" + bucket_name + \"/\" + b.name)\n",
    "            return sorted(files)\n",
    "\n",
    "    # fallback to gcsfs\n",
    "    if gcsfs is None:\n",
    "        raise RuntimeError(\"gcsfs is not installed; install gcsfs or enable google-cloud-storage\")\n",
    "    fs_kwargs = {}\n",
    "    if requester_pays_project:\n",
    "        fs_kwargs[\"project\"] = requester_pays_project\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google\", **fs_kwargs)\n",
    "    pattern = tif_dir.rstrip('/') + '/*.tif'\n",
    "    files = fs.glob(pattern)\n",
    "    files = [f if f.startswith(\"gs://\") else \"gs://\" + f for f in files]\n",
    "    return sorted(files)\n",
    "\n",
    "def parse_sat_dates_from_tifs(tif_dir: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> list:\n",
    "    tif_files = list_tif_files(tif_dir, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "    print(f\"Found {len(tif_files)} .tif files in {tif_dir}\")\n",
    "    dates = set()\n",
    "    date_pat1 = re.compile(r'(\\d{4}-\\d{2}-\\d{2})')\n",
    "    date_pat2 = re.compile(r'(\\d{8})')\n",
    "    for fp in tif_files:\n",
    "        bn = os.path.basename(fp)\n",
    "        for pat, fmt in [(date_pat1, None), (date_pat2, \"%Y%m%d\")]:\n",
    "            m = pat.search(bn)\n",
    "            if m:\n",
    "                try:\n",
    "                    d = pd.to_datetime(m.group(1), format=fmt).normalize()\n",
    "                    dates.add(pd.Timestamp(d))\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "    dates = sorted(list(dates))\n",
    "    print(f\"Parsed {len(dates)} unique satellite dates.\")\n",
    "    return dates\n",
    "\n",
    "# ---------------------------\n",
    "# Data engineering (binary-aware)\n",
    "# ---------------------------\n",
    "def filter_to_sat_days(df: pd.DataFrame, sat_dates: list) -> pd.DataFrame:\n",
    "    before = len(df)\n",
    "    df_sat = df[df[\"Date\"].isin(set(sat_dates))].copy()\n",
    "    after = len(df_sat)\n",
    "    print(f\"Filtered to satellite days: {before} → {after}\")\n",
    "    return df_sat\n",
    "\n",
    "def build_samples(df_sat: pd.DataFrame, sat_dates: list, meta_cols: list, target_col: str, hist_days=14, fut_days=10, threshold=BLOOM_THRESHOLD_DEFAULT):\n",
    "    excluded = set(meta_cols + [target_col, \"y_bin\"])\n",
    "    num_cols = [c for c in df_sat.columns if c not in excluded and pd.api.types.is_numeric_dtype(df_sat[c])]\n",
    "    sat_dates = sorted(pd.to_datetime(sat_dates))\n",
    "    df_indexed = df_sat.set_index([\"Pixel_ID\", \"Date\"])\n",
    "    samples = []\n",
    "    dropped_no_history = dropped_no_future = 0\n",
    "    for pid in df_indexed.index.get_level_values(\"Pixel_ID\").unique():\n",
    "        idx = df_indexed.loc[pid]\n",
    "        pid_dates = sorted(idx.index)\n",
    "        for t in pid_dates:\n",
    "            t_ts = pd.Timestamp(t)\n",
    "            hist_start = t_ts - timedelta(days=hist_days)\n",
    "            hist_rows = idx.loc[(idx.index > hist_start) & (idx.index <= t_ts)]\n",
    "            if hist_rows.empty:\n",
    "                dropped_no_history += 1\n",
    "                continue\n",
    "            hist_means = hist_rows[num_cols + ([target_col] if target_col in idx.columns else [])].mean(axis=0, skipna=True)\n",
    "            fut_end = t_ts + timedelta(days=fut_days)\n",
    "            fut_candidates = [s for s in pid_dates if t_ts < s <= fut_end]\n",
    "            if not fut_candidates:\n",
    "                dropped_no_future += 1\n",
    "                continue\n",
    "            s_star = min(fut_candidates)\n",
    "            y_raw = idx.loc[s_star, target_col] if target_col in idx.columns else np.nan\n",
    "            if pd.isna(y_raw):\n",
    "                dropped_no_future += 1\n",
    "                continue\n",
    "            y_bin = None\n",
    "            if \"y_bin\" in idx.columns:\n",
    "                try:\n",
    "                    y_bin = int(idx.loc[s_star, \"y_bin\"])\n",
    "                except Exception:\n",
    "                    y_bin = None\n",
    "            if y_bin is None:\n",
    "                if np.isin(y_raw, [0, 1]) or (pd.api.types.is_integer_dtype(type(y_raw)) and (y_raw in (0, 1))):\n",
    "                    try:\n",
    "                        y_bin = int(y_raw)\n",
    "                    except Exception:\n",
    "                        y_bin = int(y_raw >= threshold)\n",
    "                else:\n",
    "                    y_bin = int(y_raw >= threshold)\n",
    "            row_meta = idx.loc[t_ts]\n",
    "            feature_dict = {\n",
    "                \"Pixel_ID\": pid,\n",
    "                \"date_t\": t_ts,\n",
    "                \"date_target\": s_star,\n",
    "                \"horizon\": (s_star - t_ts).days,\n",
    "                \"X\": row_meta.get(\"X\", np.nan),\n",
    "                \"Y\": row_meta.get(\"Y\", np.nan),\n",
    "                \"chl_a_t\": row_meta.get(target_col, np.nan),\n",
    "                \"y_raw\": y_raw,\n",
    "                \"y_bin\": y_bin\n",
    "            }\n",
    "            for c in hist_means.index:\n",
    "                feature_dict[f\"histmean_{c}\"] = hist_means[c]\n",
    "            samples.append(feature_dict)\n",
    "    print(f\"Built {len(samples)} samples (dropped_no_history={dropped_no_history}, dropped_no_future={dropped_no_future})\")\n",
    "    samples_df = pd.DataFrame(samples)\n",
    "    feature_cols = [c for c in samples_df.columns if c not in [\"Pixel_ID\", \"date_t\", \"date_target\", \"y_raw\", \"y_bin\"]]\n",
    "    return samples_df, feature_cols\n",
    "\n",
    "def strict_drop_and_split(samples_df: pd.DataFrame, feature_cols: list, out_dir: str, requester_pays_project: str | None = None):\n",
    "    before = len(samples_df)\n",
    "    samples_clean = samples_df.dropna(subset=feature_cols + [\"y_raw\"]).copy()\n",
    "    print(f\"Dropped {before - len(samples_clean)} samples due to NaNs in features or y_raw\")\n",
    "    samples_clean[\"year_t\"] = samples_clean[\"date_t\"].dt.year\n",
    "    train_df = samples_clean[samples_clean[\"year_t\"] <= 2022].copy()\n",
    "    val_df = samples_clean[samples_clean[\"year_t\"] == 2023].copy()\n",
    "    test_df = samples_clean[samples_clean[\"year_t\"] == 2024].copy()\n",
    "    tmpdir = tempfile.mkdtemp(prefix=\"pigeon_samples_\")\n",
    "    try:\n",
    "        samples_clean.to_csv(os.path.join(tmpdir, \"samples_all_strict.csv\"), index=False)\n",
    "        train_df.to_csv(os.path.join(tmpdir, \"samples_train.csv\"), index=False)\n",
    "        val_df.to_csv(os.path.join(tmpdir, \"samples_val.csv\"), index=False)\n",
    "        test_df.to_csv(os.path.join(tmpdir, \"samples_test.csv\"), index=False)\n",
    "        for fname in [\"samples_all_strict.csv\", \"samples_train.csv\", \"samples_val.csv\", \"samples_test.csv\"]:\n",
    "            local_fp = os.path.join(tmpdir, fname)\n",
    "            save_path = save_local_or_gcs(local_fp, out_dir, dest_basename=fname, requester_pays_project=requester_pays_project)\n",
    "            print(\"Saved:\", save_path)\n",
    "    finally:\n",
    "        shutil.rmtree(tmpdir, ignore_errors=True)\n",
    "    print(f\"Split → train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def fit_imputer_and_scaler(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame, feature_cols: list, out_dir: str, random_state=0, requester_pays_project: str | None = None):\n",
    "    if \"chl_a_t\" not in feature_cols:\n",
    "        feature_cols.append(\"chl_a_t\")\n",
    "        print(\"Added chl_a_t to feature list.\")\n",
    "    X_train = train_df[feature_cols].values\n",
    "    X_val = val_df[feature_cols].values\n",
    "    X_test = test_df[feature_cols].values\n",
    "    imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=100, random_state=random_state)\n",
    "    X_train_imp = imputer.fit_transform(X_train)\n",
    "    X_val_imp = imputer.transform(X_val)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "    tmp_im = tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\")\n",
    "    tmp_im.close()\n",
    "    joblib.dump(imputer, tmp_im.name)\n",
    "    imputer_dest = save_local_or_gcs(tmp_im.name, out_dir, dest_basename=\"iterative_imputer_binary.joblib\", requester_pays_project=requester_pays_project)\n",
    "    print(\"Saved imputer to:\", imputer_dest)\n",
    "    df_train_imp = pd.DataFrame(X_train_imp, columns=feature_cols)\n",
    "    df_val_imp = pd.DataFrame(X_val_imp, columns=feature_cols)\n",
    "    df_test_imp = pd.DataFrame(X_test_imp, columns=feature_cols)\n",
    "    cols_to_standardize = [c for c in feature_cols if (\"histmean_\" in c) or (c == \"chl_a_t\")]\n",
    "    print(\"Standardizing columns:\", cols_to_standardize)\n",
    "    scaler = StandardScaler()\n",
    "    if cols_to_standardize:\n",
    "        scaler.fit(df_train_imp[cols_to_standardize])\n",
    "        df_train_imp[cols_to_standardize] = scaler.transform(df_train_imp[cols_to_standardize])\n",
    "        df_val_imp[cols_to_standardize] = scaler.transform(df_val_imp[cols_to_standardize])\n",
    "        df_test_imp[cols_to_standardize] = scaler.transform(df_test_imp[cols_to_standardize])\n",
    "    tmp_sc = tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\")\n",
    "    tmp_sc.close()\n",
    "    joblib.dump(scaler, tmp_sc.name)\n",
    "    scaler_dest = save_local_or_gcs(tmp_sc.name, out_dir, dest_basename=\"standard_scaler_binary.joblib\", requester_pays_project=requester_pays_project)\n",
    "    print(\"Saved scaler to:\", scaler_dest)\n",
    "    y_train = np.log1p(train_df[\"y_raw\"].values)\n",
    "    y_val = np.log1p(val_df[\"y_raw\"].values)\n",
    "    y_test = np.log1p(test_df[\"y_raw\"].values)\n",
    "    y_train_bin = train_df[\"y_bin\"].astype(int).values\n",
    "    y_val_bin = val_df[\"y_bin\"].astype(int).values\n",
    "    y_test_bin = test_df[\"y_bin\"].astype(int).values\n",
    "    tmp_npz = tempfile.NamedTemporaryFile(delete=False, suffix=\".npz\")\n",
    "    tmp_npz.close()\n",
    "    np.savez_compressed(\n",
    "        tmp_npz.name,\n",
    "        X_train=df_train_imp.values, X_val=df_val_imp.values, X_test=df_test_imp.values,\n",
    "        y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "        y_train_bin=y_train_bin, y_val_bin=y_val_bin, y_test_bin=y_test_bin,\n",
    "        feature_cols=feature_cols\n",
    "    )\n",
    "    dest_npz = save_local_or_gcs(tmp_npz.name, out_dir, dest_basename=\"processed_dataset_binary.npz\", requester_pays_project=requester_pays_project)\n",
    "    print(\"Saved processed dataset to:\", dest_npz)\n",
    "    return df_train_imp, df_val_imp, df_test_imp, y_train, y_val, y_test, y_train_bin, y_val_bin, y_test_bin\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main(args):\n",
    "    use_gcsfs = bool(args.use_gcsfs)\n",
    "    # load CSV first\n",
    "    df = load_csv(args.csv, requester_pays_project=args.requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "\n",
    "    # Try to parse sentinel dates from tif files. If none found, fall back to using CSV dates.\n",
    "    sat_dates = parse_sat_dates_from_tifs(args.tif_dir, requester_pays_project=args.requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "    if len(sat_dates) == 0:\n",
    "        # Fallback: use unique dates in CSV (useful when tifs are not present or data is already pre-aligned)\n",
    "        csv_unique_dates = sorted(df[\"Date\"].dropna().unique())\n",
    "        if len(csv_unique_dates) == 0:\n",
    "            print(\"ERROR: No satellite .tif dates found and CSV contains no valid dates. Cannot proceed.\", file=sys.stderr)\n",
    "            sys.exit(1)\n",
    "        sat_dates = [pd.Timestamp(d).normalize() for d in csv_unique_dates]\n",
    "        print(f\"Warning: No .tif files found in {args.tif_dir}. Falling back to using unique dates from CSV ({len(sat_dates)} dates).\")\n",
    "\n",
    "    df_sat = filter_to_sat_days(df, sat_dates)\n",
    "    samples_df, feature_cols = build_samples(df_sat, sat_dates, META_COLS, args.target_col, hist_days=args.hist_days, fut_days=args.fut_days, threshold=args.threshold)\n",
    "\n",
    "    # If building samples yields none, exit with helpful message instead of raising KeyError later\n",
    "    if samples_df.empty:\n",
    "        print(\"ERROR: No samples were built. This can happen when:\\n\"\n",
    "              \" - the tif_dir path is incorrect or contains no .tif files, or\\n\"\n",
    "              \" - the CSV and tif dates do not overlap, or\\n\"\n",
    "              \" - the CSV has missing y_raw values for future horizons.\\n\"\n",
    "              \"Please verify the tif_dir and csv paths, or re-run data prep with --use_gcsfs to debug gcsfs listing.\\n\", file=sys.stderr)\n",
    "        # Helpful listing suggestion for debugging\n",
    "        print(\"Quick checks you can run:\\n\"\n",
    "              f\"  gsutil ls gs://<bucket>/{args.tif_dir.split('gs://')[-1].lstrip('/')}  # list objects under tif_dir\\n\"\n",
    "              f\"  gsutil ls gs://{args.csv.split('gs://')[-1].split('/')[0]}/  # list buckets prefix\\n\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    train_df, val_df, test_df = strict_drop_and_split(samples_df, feature_cols, args.out_dir, requester_pays_project=args.requester_pays_project)\n",
    "    fit_imputer_and_scaler(train_df, val_df, test_df, feature_cols, args.out_dir, args.random_state, requester_pays_project=args.requester_pays_project)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Pigeon Lake Data Preparation (binary-aware, GCS-friendly; storage-client preferred)\")\n",
    "    parser.add_argument(\"--csv\", type=str, default=DEFAULT_CSV, help=\"Path to CSV (local or gs://)\")\n",
    "    parser.add_argument(\"--tif_dir\", type=str, default=SAT_TIF_DIR, help=\"Directory of .tif files (local or gs://)\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=DEFAULT_OUT_DIR, help=\"Output directory (local path or gs:// prefix)\")\n",
    "    parser.add_argument(\"--target_col\", type=str, default=TARGET_COL, help=\"Name of continuous target column in CSV (if present)\")\n",
    "    parser.add_argument(\"--hist_days\", type=int, default=14, help=\"History window (days)\")\n",
    "    parser.add_argument(\"--fut_days\", type=int, default=10, help=\"Horizon search window (days)\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=BLOOM_THRESHOLD_DEFAULT, help=\"Threshold to derive binary label if needed\")\n",
    "    parser.add_argument(\"--random_state\", type=int, default=0)\n",
    "    parser.add_argument(\"--requester_pays_project\", type=str, default=None, help=\"GCP project id to use for requester-pays buckets (optional)\")\n",
    "    parser.add_argument(\"--use_gcsfs\", action=\"store_true\", help=\"Attempt gcsfs first (may show 401 traces if not authenticated). Default = use storage client.\")\n",
    "    # Jupyter-safe argparse\n",
    "    if \"__file__\" not in globals():\n",
    "        args, _ = parser.parse_known_args(sys.argv)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c70ca7f-8a50-4a04-91d9-05d391c6e6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed arrays from: gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\n",
      "Shapes: X_train=(948281, 39), X_val=(74421, 39), X_test=(495447, 39)\n",
      "Using PredefinedSplit (train+val) for tuning.\n",
      "Saved initial best RF to: gs://final_data_kgc2/rf_results/rf_best_initial_binary_pig.joblib\n",
      "RF best params: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_samples': 0.6, 'max_features': 'sqrt', 'max_depth': 10, 'class_weight': None, 'bootstrap': True}\n",
      "Validation metrics (RF classifier): {'precision': 0.8175742013443094, 'recall': 0.9084081564152051, 'f1': 0.8606010016694491, 'average_precision': 0.9379559607168465, 'roc_auc': 0.971338969380898, 'brier': 0.06557838334845235, 'tn': 45756, 'fp': 4831, 'fn': 2183, 'tp': 21651}\n",
      "Final RF trained in 180.4166374206543 s\n",
      "Saved final RF to: gs://final_data_kgc2/rf_results/final_rf_classifier_binary_pig.joblib\n",
      "Test metrics: {'precision': 0.31722310337014453, 'recall': 0.44880693538495847, 'f1': 0.37171369699348367, 'average_precision': 0.3258480971747378, 'roc_auc': 0.8001907350202164, 'brier': 0.13606460453237926, 'tn': 354391, 'fp': 69308, 'fn': 39547, 'tp': 32201}\n",
      "Saved test predictions to: gs://final_data_kgc2/rf_results/predictions_test_rf_classifier_binary_min_pig.csv\n",
      "Saved summary to: gs://final_data_kgc2/rf_results/training_summary_rf_classifier_binary_pig.json\n",
      "All outputs saved to gs://final_data_kgc2/rf_results/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train RandomForest classifier for binary bloom/no-bloom — GCS-ready final variant\n",
    "with optional tagging of output artifacts to avoid overwriting when running multiple lakes.\n",
    "\n",
    "This patched version fixes a bug that caused a ValueError when constructing the\n",
    "per-pixel grid prediction CSV (arrays of different lengths). It also ensures the\n",
    "\"enriched\" predictions CSV contains date_target, pixel locations (xi, yi, pixel_lon,\n",
    "pixel_lat) and observed/predicted values.\n",
    "\n",
    "Usage is unchanged; outputs are tagged by --tag (default \"nak\").\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, PredefinedSplit, ParameterSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    brier_score_loss, confusion_matrix\n",
    ")\n",
    "from sklearn.base import clone\n",
    "import joblib\n",
    "\n",
    "# joblib parallel backend\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# optional imports\n",
    "try:\n",
    "    import gcsfs\n",
    "except Exception:\n",
    "    gcsfs = None\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "# Try import google-cloud-storage (preferred)\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "except Exception:\n",
    "    storage = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated as an API\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------\n",
    "# Defaults pointing to your bucket\n",
    "# ---------------------------\n",
    "DEFAULT_NPZ = \"gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\"\n",
    "DEFAULT_SAMPLES_DIR = \"gs://final_data_kgc2/Final_data/\"\n",
    "DEFAULT_OUT_DIR = \"gs://final_data_kgc2/rf_results/\"\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# ---------------------------\n",
    "# GCS / local helpers (storage client preferred)\n",
    "# ---------------------------\n",
    "def is_gcs_path(p: str) -> bool:\n",
    "    return isinstance(p, str) and p.startswith(\"gs://\")\n",
    "\n",
    "def gcs_join(prefix: str, *parts: str) -> str:\n",
    "    p = prefix.rstrip('/')\n",
    "    for part in parts:\n",
    "        p = p.rstrip('/') + '/' + str(part).lstrip('/')\n",
    "    return p\n",
    "\n",
    "def upload_file_to_gcs(local_path: str, gs_uri: str, requester_pays_project: str | None = None):\n",
    "    \"\"\"Upload a local file to gs:// using google-cloud-storage (preferred) or gcsfs fallback.\"\"\"\n",
    "    if not is_gcs_path(gs_uri):\n",
    "        raise ValueError(\"gs_uri must be gs:// path\")\n",
    "    # prefer google-cloud-storage\n",
    "    if storage is not None:\n",
    "        client_kwargs = {}\n",
    "        if requester_pays_project:\n",
    "            client_kwargs[\"project\"] = requester_pays_project\n",
    "        client = storage.Client(**client_kwargs)\n",
    "        _, rest = gs_uri.split(\"gs://\", 1)\n",
    "        bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Destination bucket does not exist or is not accessible: gs://{bucket_name}\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(local_path)\n",
    "        return\n",
    "    # fallback to gcsfs\n",
    "    if gcsfs is not None:\n",
    "        fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "        fs.put(local_path, gs_uri)\n",
    "        return\n",
    "    raise RuntimeError(\"No method available to upload to GCS: install google-cloud-storage or gcsfs\")\n",
    "\n",
    "def fetch_gcs_to_local(gcs_path: str, local_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    \"\"\"Download a gs:// object to a local path. Prefer storage client, fallback to gcsfs.\"\"\"\n",
    "    if not is_gcs_path(gcs_path):\n",
    "        raise ValueError(\"gcs path required\")\n",
    "    if storage is not None and not use_gcsfs:\n",
    "        client_kwargs = {}\n",
    "        if requester_pays_project:\n",
    "            client_kwargs[\"project\"] = requester_pays_project\n",
    "        client = storage.Client(**client_kwargs)\n",
    "        _, rest = gcs_path.split(\"gs://\", 1)\n",
    "        bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Bucket not found or access denied: gs://{bucket_name}\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"Blob not found: {gcs_path}\")\n",
    "        blob.download_to_filename(local_path)\n",
    "        return local_path\n",
    "    # fallback to gcsfs\n",
    "    if gcsfs is None:\n",
    "        raise RuntimeError(\"gcsfs not installed and storage client not available\")\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "    with fs.open(gcs_path, \"rb\") as src, open(local_path, \"wb\") as dst:\n",
    "        shutil.copyfileobj(src, dst)\n",
    "    return local_path\n",
    "\n",
    "def load_npz(npz_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> dict:\n",
    "    \"\"\"Load npz from local path or GCS into a dict.\"\"\"\n",
    "    if is_gcs_path(npz_path):\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix=\".npz\", delete=False)\n",
    "        tmp.close()\n",
    "        fetch_gcs_to_local(npz_path, tmp.name, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "        data = np.load(tmp.name, allow_pickle=True)\n",
    "        result = {k: data[k] for k in data.files}\n",
    "        try:\n",
    "            os.unlink(tmp.name)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return result\n",
    "    else:\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        return {k: data[k] for k in data.files}\n",
    "\n",
    "def load_samples_csvs(samples_dir: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    \"\"\"Load samples_train/val/test CSVs (used for metadata & saving preds).\"\"\"\n",
    "    def load_one(path_or_dir, name):\n",
    "        if is_gcs_path(str(path_or_dir)):\n",
    "            # prefer storage client\n",
    "            if storage is not None and not use_gcsfs:\n",
    "                client_kwargs = {}\n",
    "                if requester_pays_project:\n",
    "                    client_kwargs[\"project\"] = requester_pays_project\n",
    "                client = storage.Client(**client_kwargs)\n",
    "                _, rest = str(path_or_dir).split(\"gs://\", 1)\n",
    "                bucket_name, _, prefix = rest.partition(\"/\")\n",
    "                prefix = prefix.rstrip('/') + '/'\n",
    "                blob_path = prefix + name\n",
    "                bucket = client.lookup_bucket(bucket_name)\n",
    "                if bucket is None:\n",
    "                    raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "                blob = bucket.blob(blob_path)\n",
    "                if not blob.exists():\n",
    "                    raise FileNotFoundError(f\"Samples CSV not found: gs://{bucket_name}/{blob_path}\")\n",
    "                data = blob.download_as_bytes()\n",
    "                return pd.read_csv(io.BytesIO(data), parse_dates=['date_t', 'date_target'], low_memory=False)\n",
    "            # fallback gcsfs\n",
    "            if gcsfs is None:\n",
    "                raise RuntimeError(\"gcsfs not installed and storage client not available\")\n",
    "            fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "            target = str(path_or_dir).rstrip('/') + '/' + name\n",
    "            with fs.open(target, \"rb\") as f:\n",
    "                return pd.read_csv(f, parse_dates=['date_t', 'date_target'], low_memory=False)\n",
    "        else:\n",
    "            p = Path(path_or_dir) / name\n",
    "            return pd.read_csv(str(p), parse_dates=['date_t', 'date_target'], low_memory=False)\n",
    "    return load_one(samples_dir, \"samples_train.csv\"), load_one(samples_dir, \"samples_val.csv\"), load_one(samples_dir, \"samples_test.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Serialization helpers\n",
    "# ---------------------------\n",
    "def _save_json_atomic_local(path: str, data):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\") as fh:\n",
    "        json.dump(data, fh, default=lambda o: repr(o), indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _to_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_to_serializable(v) for v in obj]\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return _to_serializable(obj.tolist())\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    return obj\n",
    "\n",
    "# ----------------- helper utilities for grid/date/coords -----------------\n",
    "def create_regular_grid_edges_and_centers(xs, ys, grid_res=400, buffer_frac=0.02):\n",
    "    xs = np.asarray(xs); ys = np.asarray(ys)\n",
    "    xmin, xmax = float(np.nanmin(xs)), float(np.nanmax(xs))\n",
    "    ymin, ymax = float(np.nanmin(ys)), float(np.nanmax(ys))\n",
    "    if xmax == xmin: xmax = xmin + 1e-6\n",
    "    if ymax == ymin: ymax = ymin + 1e-6\n",
    "    xpad = (xmax - xmin) * buffer_frac; ypad = (ymax - ymin) * buffer_frac\n",
    "    xmin -= xpad; xmax += xpad; ymin -= ypad; ymax += ypad\n",
    "    lon_span = xmax - xmin; lat_span = ymax - ymin\n",
    "    longest = max(lon_span, lat_span)\n",
    "    desired_cells = max(2, int(grid_res))\n",
    "    cell_size = float(longest) / float(desired_cells)\n",
    "    nx = max(2, int(np.ceil(lon_span / cell_size))); ny = max(2, int(np.ceil(lat_span / cell_size)))\n",
    "    xmax = xmin + nx * cell_size; ymax = ymin + ny * cell_size\n",
    "    x_edges = np.linspace(xmin, xmax, nx+1); y_edges = np.linspace(ymin, ymax, ny+1)\n",
    "    x_centers = 0.5*(x_edges[:-1] + x_edges[1:]); y_centers = 0.5*(y_edges[:-1] + y_edges[1:])\n",
    "    return x_edges, y_edges, x_centers, y_centers\n",
    "\n",
    "def detect_date_column_simple(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"Pick a likely date column name (basic): prefer date_target, date_t, Date, date.\"\"\"\n",
    "    for c in (\"date_target\",\"date_t\",\"Date\",\"date\"):\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if \"date\" in c.lower() or \"day\" in c.lower():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def find_coord_cols(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    lon_candidates = [\"pixel_lon\",\"lon\",\"longitude\",\"x\",\"X\",\"sample_lon\",\"long\"]\n",
    "    lat_candidates = [\"pixel_lat\",\"lat\",\"latitude\",\"y\",\"Y\",\"sample_lat\"]\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    lon = None; lat = None\n",
    "    for cand in lon_candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            lon = cols_lower[cand.lower()]; break\n",
    "    for cand in lat_candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            lat = cols_lower[cand.lower()]; break\n",
    "    return lon, lat\n",
    "\n",
    "# ----------------- robust scoring callable -----------------\n",
    "def scoring_callable(estimator, X, y):\n",
    "    try:\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            yprob = estimator.predict_proba(X)[:, 1]\n",
    "            return float(average_precision_score(y, yprob))\n",
    "        elif hasattr(estimator, \"decision_function\"):\n",
    "            yprob = estimator.decision_function(X)\n",
    "            return float(average_precision_score(y, yprob))\n",
    "        else:\n",
    "            yhat = estimator.predict(X)\n",
    "            return float(f1_score(y, yhat, zero_division=0))\n",
    "    except Exception:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "# ----------------- checkpointed randomized search -----------------\n",
    "def _save_json_atomic(path, data):\n",
    "    try:\n",
    "        _save_json_atomic_local(path, data)\n",
    "    except Exception:\n",
    "        with open(path, \"w\") as fh:\n",
    "            json.dump(data, fh, default=lambda o: repr(o), indent=2)\n",
    "\n",
    "def checkpointed_random_search(estimator, param_dist, X_train, y_train, cv, n_iter=8, random_state=RANDOM_STATE, checkpoint_path=None, scorer=None, verbose=False, checkpoint_verbose=False):\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = os.path.join(tempfile.gettempdir(), \"rf_search_checkpoint.json\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            with open(checkpoint_path, \"r\") as fh:\n",
    "                history = json.load(fh)\n",
    "            tried = set(h['params_repr'] for h in history if 'params_repr' in h)\n",
    "            if verbose:\n",
    "                print(f\"Loaded checkpoint with {len(history)} completed candidates from {checkpoint_path}\")\n",
    "        except Exception:\n",
    "            history = []; tried = set()\n",
    "    else:\n",
    "        history = []; tried = set()\n",
    "    param_list = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state))\n",
    "    remaining = [p for p in param_list if repr(p) not in tried]\n",
    "    best_score = -np.inf; best_params = None; best_est = None\n",
    "    iterator = enumerate(remaining, start=1)\n",
    "    if tqdm is not None and verbose:\n",
    "        iterator = enumerate(tqdm(remaining, desc=\"RF candidates\", ncols=100), start=1)\n",
    "    for i, params in iterator:\n",
    "        p_repr = repr(params)\n",
    "        if verbose:\n",
    "            print(f\"\\nCandidate {i}/{len(remaining)}: {params}\")\n",
    "        est = clone(estimator); est.set_params(**params)\n",
    "        fold_scores = []; fold_times = []\n",
    "        for fold_idx, (tr_idx, te_idx) in enumerate(cv.split(X_train), start=1):\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                est.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "                try:\n",
    "                    if scorer is not None:\n",
    "                        score = float(scorer(est, X_train[te_idx], y_train[te_idx]))\n",
    "                    else:\n",
    "                        score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "                except TypeError as te:\n",
    "                    if verbose or checkpoint_verbose:\n",
    "                        print(\"Scorer TypeError, falling back. Error:\", te)\n",
    "                    score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "                except Exception as e_other:\n",
    "                    if verbose or checkpoint_verbose:\n",
    "                        print(\"Scorer exception, using fallback:\", e_other)\n",
    "                    score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "            except Exception as e:\n",
    "                score = float(\"-inf\")\n",
    "                if verbose:\n",
    "                    print(\"  fold error during fit/predict/score:\", e)\n",
    "            elapsed = time.time() - t0\n",
    "            fold_scores.append(score); fold_times.append(elapsed)\n",
    "            if verbose:\n",
    "                print(f\"  fold {fold_idx}: score={score if np.isfinite(score) else 'FAILED'}, time={elapsed:.1f}s\")\n",
    "        mean_score = float(np.nanmean([s for s in fold_scores if np.isfinite(s)])) if len(fold_scores) > 0 else float(\"-inf\")\n",
    "        rec = {\"params_repr\": p_repr, \"params\": params, \"fold_scores\": fold_scores, \"fold_times\": fold_times, \"mean_score\": mean_score, \"timestamp\": time.time()}\n",
    "        history.append(rec)\n",
    "        try:\n",
    "            _save_json_atomic(checkpoint_path, history)\n",
    "            if verbose:\n",
    "                print(\"  Saved checkpoint to\", checkpoint_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score; best_params = params\n",
    "            try:\n",
    "                best_est = clone(estimator); best_est.set_params(**params)\n",
    "                best_est.fit(X_train, y_train)\n",
    "                if verbose:\n",
    "                    print(\"  -> New best found and refit on full training set.\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(\"  -> Refit of best failed:\", e)\n",
    "    return best_est, best_params, history\n",
    "\n",
    "def run_random_search(estimator, param_dist, X_train, y_train, cv, n_iter=8, n_jobs=1, scorer=None, random_state=RANDOM_STATE, verbose_search=False, checkpoint_path=None):\n",
    "    if checkpoint_path:\n",
    "        if verbose_search:\n",
    "            print(\"Running checkpointed randomized search (checkpoint_path=%s)\" % checkpoint_path)\n",
    "        best_est, best_params, history = checkpointed_random_search(estimator, param_dist, X_train, y_train, cv, n_iter=n_iter, random_state=random_state, checkpoint_path=checkpoint_path, scorer=scorer, verbose=verbose_search)\n",
    "        return best_est, best_params, history\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    search = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=n_iter,\n",
    "                                scoring=scoring_callable, cv=cv, random_state=random_state,\n",
    "                                n_jobs=n_jobs, verbose=1 if verbose_search else 0)\n",
    "    t0 = time.time()\n",
    "    if n_jobs and n_jobs > 1:\n",
    "        with parallel_backend('threading', n_jobs=n_jobs):\n",
    "            search.fit(X_train, y_train)\n",
    "    else:\n",
    "        search.fit(X_train, y_train)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Search finished in {elapsed:.1f}s; best_score={search.best_score_:.4f}\")\n",
    "    return search.best_estimator_, getattr(search, \"best_params_\", None), search\n",
    "\n",
    "def evaluate_classification(y_true, y_pred_label, y_pred_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred_label = np.asarray(y_pred_label)\n",
    "    y_pred_prob = np.asarray(y_pred_prob)\n",
    "    try:\n",
    "        prec = float(precision_score(y_true, y_pred_label, zero_division=0))\n",
    "        rec = float(recall_score(y_true, y_pred_label, zero_division=0))\n",
    "        f1 = float(f1_score(y_true, y_pred_label, zero_division=0))\n",
    "    except Exception:\n",
    "        prec = rec = f1 = float(\"nan\")\n",
    "    try:\n",
    "        ap = float(average_precision_score(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    try:\n",
    "        roc = float(roc_auc_score(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    try:\n",
    "        brier = float(brier_score_loss(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        brier = float(\"nan\")\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred_label).ravel()\n",
    "    except Exception:\n",
    "        tn = fp = fn = tp = None\n",
    "    return {\n",
    "        \"precision\": prec, \"recall\": rec, \"f1\": f1,\n",
    "        \"average_precision\": ap, \"roc_auc\": roc, \"brier\": brier,\n",
    "        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp\n",
    "    }\n",
    "\n",
    "# ----------------- main -----------------\n",
    "def main(args):\n",
    "    local_tmp_out = tempfile.mkdtemp(prefix=\"rf_out_\")\n",
    "    try:\n",
    "        tag = getattr(args, \"tag\", \"\")\n",
    "        def tag_name(base_filename: str) -> str:\n",
    "            if not tag:\n",
    "                return base_filename\n",
    "            base_noext, ext = os.path.splitext(base_filename)\n",
    "            return f\"{base_noext}_{tag}{ext}\"\n",
    "\n",
    "        checkpoint_path = args.checkpoint_path if args.checkpoint_path else os.path.join(local_tmp_out, tag_name(\"rf_search_checkpoint.json\"))\n",
    "\n",
    "        print(\"Loading processed arrays from:\", args.npz)\n",
    "        data = load_npz(args.npz, requester_pays_project=args.requester_pays_project, use_gcsfs=args.use_gcsfs)\n",
    "        if 'X_train' not in data:\n",
    "            raise RuntimeError(\"processed_dataset must contain X_train/X_val/X_test arrays (and y_*_bin etc).\")\n",
    "\n",
    "        X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "        feature_cols = list(data['feature_cols']) if 'feature_cols' in data else None\n",
    "\n",
    "        if 'y_train_bin' in data and 'y_val_bin' in data and 'y_test_bin' in data:\n",
    "            y_train_bin = np.asarray(data['y_train_bin'])\n",
    "            y_val_bin = np.asarray(data['y_val_bin'])\n",
    "            y_test_bin = np.asarray(data['y_test_bin'])\n",
    "        else:\n",
    "            print(\"Binary targets not found in NPZ — attempting to load from samples CSVs.\")\n",
    "            train_df, val_df, test_df = load_samples_csvs(args.samples_dir, requester_pays_project=args.requester_pays_project, use_gcsfs=args.use_gcsfs)\n",
    "            if 'y_bin' not in train_df.columns:\n",
    "                raise RuntimeError(\"y_bin column not found in samples CSVs nor NPZ. Please run data prep to save y_bin.\")\n",
    "            y_train_bin = train_df['y_bin'].astype(int).values\n",
    "            y_val_bin = val_df['y_bin'].astype(int).values\n",
    "            y_test_bin = test_df['y_bin'].astype(int).values\n",
    "\n",
    "        print(f\"Shapes: X_train={X_train.shape}, X_val={X_val.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "        # load sample csvs for metadata (non-fatal)\n",
    "        try:\n",
    "            train_df, val_df, test_df = load_samples_csvs(args.samples_dir, requester_pays_project=args.requester_pays_project, use_gcsfs=args.use_gcsfs)\n",
    "        except Exception:\n",
    "            train_df = val_df = test_df = None\n",
    "\n",
    "        # subsample for tuning if requested\n",
    "        if args.max_train_samples:\n",
    "            X_train_sub = X_train[:args.max_train_samples]; y_train_sub = y_train_bin[:args.max_train_samples]\n",
    "            print(\"Using subsample for tuning:\", X_train_sub.shape)\n",
    "        else:\n",
    "            X_train_sub, y_train_sub = X_train, y_train_bin\n",
    "\n",
    "        # build CV object\n",
    "        if args.use_predefined_split:\n",
    "            X_comb = np.vstack([X_train_sub, X_val]); y_comb = np.concatenate([y_train_sub, y_val_bin])\n",
    "            test_fold = np.concatenate([np.full(X_train_sub.shape[0], -1), np.zeros(X_val.shape[0], dtype=int)])\n",
    "            cv_obj = PredefinedSplit(test_fold)\n",
    "            X_tune, y_tune = X_comb, y_comb\n",
    "            print(\"Using PredefinedSplit (train+val) for tuning.\")\n",
    "        else:\n",
    "            cv_obj = TimeSeriesSplit(n_splits=args.ts_splits)\n",
    "            X_tune, y_tune = X_train_sub, y_train_sub\n",
    "            print(f\"Using TimeSeriesSplit(n_splits={args.ts_splits})\")\n",
    "\n",
    "        # parameter distributions\n",
    "        if args.fast:\n",
    "            param_dist = {\n",
    "                \"n_estimators\": [100, 200],\n",
    "                \"max_depth\": [5, 8],\n",
    "                \"min_samples_leaf\": [5, 10],\n",
    "                \"min_samples_split\": [5, 10],\n",
    "                \"max_features\": [\"sqrt\", 0.5],\n",
    "                \"class_weight\": [\"balanced\"],\n",
    "                \"bootstrap\": [True],\n",
    "                \"max_samples\": [0.6, 0.8]\n",
    "            }\n",
    "        else:\n",
    "            param_dist = {\n",
    "                \"n_estimators\": [200, 500],\n",
    "                \"max_depth\": [5, 8, 10],\n",
    "                \"min_samples_leaf\": [3, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 8],\n",
    "                \"max_features\": [\"sqrt\", 0.5, 0.7],\n",
    "                \"class_weight\": [\"balanced\", None],\n",
    "                \"bootstrap\": [True],\n",
    "                \"max_samples\": [0.5, 0.6, 0.8]\n",
    "            }\n",
    "\n",
    "        rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=1)\n",
    "\n",
    "        best_rf, rf_params, rf_search = run_random_search(\n",
    "            rf, param_dist, X_tune, y_tune, cv_obj,\n",
    "            n_iter=args.n_iter, n_jobs=min(args.n_jobs, 8),\n",
    "            scorer=None,\n",
    "            random_state=RANDOM_STATE, verbose_search=args.verbose, checkpoint_path=checkpoint_path\n",
    "        )\n",
    "        if best_rf is None:\n",
    "            print(\"No RF candidate found (search returned None). Exiting.\")\n",
    "            return\n",
    "\n",
    "        best_initial_local = os.path.join(local_tmp_out, tag_name(\"rf_best_initial_binary.joblib\"))\n",
    "        joblib.dump(best_rf, best_initial_local)\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_best_initial = gcs_join(args.out_dir, os.path.basename(best_initial_local))\n",
    "            upload_file_to_gcs(best_initial_local, dest_best_initial, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved initial best RF to:\", dest_best_initial)\n",
    "        else:\n",
    "            out_best = os.path.join(args.out_dir, os.path.basename(best_initial_local))\n",
    "            shutil.move(best_initial_local, out_best)\n",
    "            print(\"Saved initial best RF to:\", out_best)\n",
    "\n",
    "        print(\"RF best params:\", rf_params)\n",
    "\n",
    "        # Validate on val\n",
    "        try:\n",
    "            y_val_prob = best_rf.predict_proba(X_val)[:, 1]\n",
    "            y_val_label = (y_val_prob >= args.prob_threshold).astype(int)\n",
    "            val_metrics = evaluate_classification(y_val_bin, y_val_label, y_val_prob)\n",
    "            print(\"Validation metrics (RF classifier):\", val_metrics)\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed:\", e)\n",
    "            val_metrics = None\n",
    "\n",
    "        # Retrain on train+val and save final\n",
    "        X_train_val = np.vstack([X_train, X_val]); y_train_val = np.concatenate([y_train_bin, y_val_bin])\n",
    "        final = RandomForestClassifier(**best_rf.get_params())\n",
    "        final.set_params(n_jobs=args.n_jobs, random_state=RANDOM_STATE)\n",
    "        t0 = time.time(); final.fit(X_train_val, y_train_val); print(\"Final RF trained in\", time.time() - t0, \"s\")\n",
    "\n",
    "        final_local = os.path.join(local_tmp_out, tag_name(\"final_rf_classifier_binary.joblib\"))\n",
    "        joblib.dump(final, final_local)\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_final = gcs_join(args.out_dir, os.path.basename(final_local))\n",
    "            upload_file_to_gcs(final_local, dest_final, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved final RF to:\", dest_final)\n",
    "        else:\n",
    "            out_final = os.path.join(args.out_dir, os.path.basename(final_local))\n",
    "            shutil.move(final_local, out_final)\n",
    "            print(\"Saved final RF to:\", out_final)\n",
    "\n",
    "        # Test eval and save predictions\n",
    "        y_test_prob = final.predict_proba(X_test)[:, 1]\n",
    "        y_test_label = (y_test_prob >= args.prob_threshold).astype(int)\n",
    "        test_metrics = evaluate_classification(y_test_bin, y_test_label, y_test_prob)\n",
    "        print(\"Test metrics:\", test_metrics)\n",
    "\n",
    "        # Build enriched predictions table with dates and pixel locations\n",
    "        # If samples_test.csv is available (test_df), prefer it and augment; otherwise build from X_test grid.\n",
    "        try:\n",
    "            # reconstruct canonical grid from X_test for pixel centers/indices\n",
    "            if feature_cols:\n",
    "                lon_idx = None; lat_idx = None\n",
    "                for j,c in enumerate(feature_cols):\n",
    "                    if isinstance(c, str):\n",
    "                        cl = c.lower()\n",
    "                        if lon_idx is None and any(tok in cl for tok in (\"lon\",\"long\",\"longitude\",\"x\")):\n",
    "                            lon_idx = j\n",
    "                        if lat_idx is None and any(tok in cl for tok in (\"lat\",\"latitude\",\"y\")):\n",
    "                            lat_idx = j\n",
    "                if lon_idx is None or lat_idx is None:\n",
    "                    lon_idx = 0 if lon_idx is None else lon_idx\n",
    "                    lat_idx = 1 if lat_idx is None else lat_idx\n",
    "            else:\n",
    "                lon_idx, lat_idx = 0, 1\n",
    "            xs = X_test[:, lon_idx].astype(float)\n",
    "            ys = X_test[:, lat_idx].astype(float)\n",
    "            x_edges, y_edges, x_centers, y_centers = create_regular_grid_edges_and_centers(xs, ys, grid_res=400, buffer_frac=0.02)\n",
    "        except Exception:\n",
    "            x_edges = y_edges = x_centers = y_centers = None\n",
    "\n",
    "        if test_df is not None:\n",
    "            pred_table = test_df.copy()\n",
    "            # ensure date column parsed\n",
    "            date_col = detect_date_column_simple(pred_table)\n",
    "            if date_col is not None:\n",
    "                pred_table[date_col] = pd.to_datetime(pred_table[date_col], errors='coerce')\n",
    "                pred_table['date_target'] = pred_table[date_col]\n",
    "            else:\n",
    "                pred_table['date_target'] = pd.NaT\n",
    "            # observed label\n",
    "            if 'y_raw' in pred_table.columns:\n",
    "                pred_table['y_true_bin'] = pred_table['y_raw'].apply(lambda v: int(v >= args.threshold) if not pd.isna(v) else np.nan)\n",
    "            elif 'y_bin' in pred_table.columns:\n",
    "                pred_table['y_true_bin'] = pred_table['y_bin'].astype(int)\n",
    "            else:\n",
    "                pred_table['y_true_bin'] = y_test_bin if len(y_test_bin)==len(pred_table) else np.nan\n",
    "            # attach predictions\n",
    "            # If lengths match, attach by order; otherwise broadcast if single value\n",
    "            if len(pred_table) == len(y_test_prob):\n",
    "                pred_table['y_pred_prob'] = y_test_prob\n",
    "                pred_table['y_pred_label'] = y_test_label\n",
    "            else:\n",
    "                # lengths differ; attach elementwise where possible, else NaN\n",
    "                pred_table['y_pred_prob'] = np.nan\n",
    "                pred_table['y_pred_label'] = np.nan\n",
    "                minlen = min(len(pred_table), len(y_test_prob))\n",
    "                pred_table.loc[:minlen-1, 'y_pred_prob'] = y_test_prob[:minlen]\n",
    "                pred_table.loc[:minlen-1, 'y_pred_label'] = y_test_label[:minlen]\n",
    "\n",
    "            # detect coordinate columns and compute pixel indices if possible\n",
    "            lon_col, lat_col = find_coord_cols(pred_table)\n",
    "            if lon_col is not None and lat_col is not None and x_edges is not None:\n",
    "                pred_table['_mx'] = pd.to_numeric(pred_table[lon_col], errors='coerce')\n",
    "                pred_table['_my'] = pd.to_numeric(pred_table[lat_col], errors='coerce')\n",
    "                pred_table['xi'] = np.clip(np.searchsorted(x_edges, pred_table['_mx'].values) - 1, 0, len(x_centers)-1).astype(int)\n",
    "                pred_table['yi'] = np.clip(np.searchsorted(y_edges, pred_table['_my'].values) - 1, 0, len(y_centers)-1).astype(int)\n",
    "                pred_table['pixel_lon'] = x_centers[pred_table['xi'].astype(int).values]\n",
    "                pred_table['pixel_lat'] = y_centers[pred_table['yi'].astype(int).values]\n",
    "                pred_out_cols = ['date_target','xi','yi','pixel_lon','pixel_lat', lon_col, lat_col, 'y_true_bin','y_pred_prob','y_pred_label']\n",
    "            else:\n",
    "                pred_out_cols = ['date_target','y_true_bin','y_pred_prob','y_pred_label']\n",
    "            preds_local = os.path.join(local_tmp_out, tag_name(\"predictions_test_rf_classifier_binary_enriched.csv\"))\n",
    "            pred_table.to_csv(preds_local, index=False)\n",
    "        else:\n",
    "            # build per-pixel predictions from X_test grid (use X_test rows — lengths must match y_test_prob)\n",
    "            if x_edges is None or x_centers is None:\n",
    "                # fallback minimal\n",
    "                df_min = pd.DataFrame({\"y_true_bin\": y_test_bin if len(y_test_bin)==len(y_test_prob) else np.nan, \"y_pred_prob\": y_test_prob, \"y_pred_label\": y_test_label})\n",
    "                preds_local = os.path.join(local_tmp_out, tag_name(\"predictions_test_rf_classifier_binary_min.csv\"))\n",
    "                df_min.to_csv(preds_local, index=False)\n",
    "            else:\n",
    "                # Use X_test rows directly to compute xi/yi and pixel centers per row (ensures matching lengths)\n",
    "                xi_per_row = np.clip(np.searchsorted(x_edges, xs) - 1, 0, len(x_centers)-1).astype(int)\n",
    "                yi_per_row = np.clip(np.searchsorted(y_edges, ys) - 1, 0, len(y_centers)-1).astype(int)\n",
    "                pixel_lon_per_row = x_centers[xi_per_row]\n",
    "                pixel_lat_per_row = y_centers[yi_per_row]\n",
    "                df_grid = pd.DataFrame({\n",
    "                    \"xi\": xi_per_row,\n",
    "                    \"yi\": yi_per_row,\n",
    "                    \"pixel_lon\": pixel_lon_per_row,\n",
    "                    \"pixel_lat\": pixel_lat_per_row,\n",
    "                    \"y_pred_prob\": y_test_prob,\n",
    "                    \"y_pred_label\": y_test_label\n",
    "                })\n",
    "                # If y_test_bin aligns by X_test order, attach\n",
    "                if len(y_test_bin) == len(df_grid):\n",
    "                    df_grid['y_true_bin'] = y_test_bin\n",
    "                else:\n",
    "                    df_grid['y_true_bin'] = np.nan\n",
    "                preds_local = os.path.join(local_tmp_out, tag_name(\"predictions_test_rf_classifier_binary_grid.csv\"))\n",
    "                df_grid.to_csv(preds_local, index=False)\n",
    "\n",
    "        # upload/move predictions file\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_preds = gcs_join(args.out_dir, os.path.basename(preds_local))\n",
    "            upload_file_to_gcs(preds_local, dest_preds, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved test predictions to:\", dest_preds)\n",
    "        else:\n",
    "            out_preds = os.path.join(args.out_dir, os.path.basename(preds_local))\n",
    "            shutil.move(preds_local, out_preds)\n",
    "            print(\"Saved test predictions to:\", out_preds)\n",
    "\n",
    "        # Save summary JSON (tagged)\n",
    "        summary = {\n",
    "            \"model\": \"rf_classifier_binary\",\n",
    "            \"val_metrics\": val_metrics,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"best_params\": rf_params,\n",
    "            \"n_iter\": int(args.n_iter),\n",
    "            \"checkpoint_path\": checkpoint_path\n",
    "        }\n",
    "        summary_safe = _to_serializable(summary)\n",
    "        summary_local = os.path.join(local_tmp_out, tag_name(\"training_summary_rf_classifier_binary.json\"))\n",
    "        with open(summary_local, \"w\") as fh:\n",
    "            json.dump(summary_safe, fh, indent=2)\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_summary = gcs_join(args.out_dir, os.path.basename(summary_local))\n",
    "            upload_file_to_gcs(summary_local, dest_summary, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved summary to:\", dest_summary)\n",
    "        else:\n",
    "            out_summary = os.path.join(args.out_dir, os.path.basename(summary_local))\n",
    "            shutil.move(summary_local, out_summary)\n",
    "            print(\"Saved summary to:\", out_summary)\n",
    "\n",
    "        print(\"All outputs saved to\", args.out_dir)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            shutil.rmtree(local_tmp_out)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--npz\", type=str, default=DEFAULT_NPZ, help=\"Processed dataset .npz (must contain X_train/X_val/X_test and y_*_bin)\")\n",
    "    parser.add_argument(\"--samples_dir\", type=str, default=DEFAULT_SAMPLES_DIR, help=\"Folder with samples_train/val/test CSVs (used for metadata & fallback)\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=DEFAULT_OUT_DIR, help=\"Output folder (local path or gs://bucket/path/)\")\n",
    "    parser.add_argument(\"--n_iter\", type=int, default=8)\n",
    "    parser.add_argument(\"--n_jobs\", type=int, default=4)\n",
    "    parser.add_argument(\"--ts_splits\", type=int, default=5)\n",
    "    parser.add_argument(\"--no_predefined_split\", action=\"store_false\", dest=\"use_predefined_split\", help=\"Disable using PredefinedSplit and use TimeSeriesSplit instead.\")\n",
    "    parser.set_defaults(use_predefined_split=True)\n",
    "    parser.add_argument(\"--max_train_samples\", type=int, default=None)\n",
    "    parser.add_argument(\"--fast\", action=\"store_true\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "    parser.add_argument(\"--checkpoint_path\", type=str, default=None, help=\"Path to JSON checkpoint file to save/resume search (local recommended)\")\n",
    "    parser.add_argument(\"--prob_threshold\", type=float, default=0.5, help=\"Probability threshold to convert probs into labels for evaluation\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=20.0, help=\"Chl a threshold used to create y_bin (informational)\")\n",
    "    parser.add_argument(\"--use_gcsfs\", action=\"store_true\", help=\"Attempt gcsfs for reads if storage client fails (default: prefer storage client)\")\n",
    "    parser.add_argument(\"--requester_pays_project\", type=str, default=None, help=\"GCP project id to use when accessing requester-pays buckets\")\n",
    "    parser.add_argument(\"--tag\", type=str, default=\"nak\", help=\"Optional tag appended to artifact filenames to avoid overwriting (default: nak). Use empty string to disable tagging.\")\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7aec1c-8cba-4f56-821a-e6c231f24ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed arrays from: gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\n",
      "Shapes: X_train=(948281, 39), X_val=(74421, 39), X_test=(495447, 39)\n",
      "Using PredefinedSplit (train+val) for tuning.\n",
      "Saved initial best XGB to: gs://final_data_kgc2/rf_results/xgb_best_initial_binary_pig.joblib\n",
      "XGB best params: {'verbosity': 0, 'use_label_encoder': False, 'subsample': 0.5, 'scale_pos_weight': 1.3642127360395713, 'reg_lambda': 1.0, 'reg_alpha': 1.0, 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 2.0, 'colsample_bytree': 0.5}\n",
      "Validation metrics (XGB): {'precision': 0.786209184049532, 'recall': 0.8950658722832928, 'f1': 0.8371134829697064, 'average_precision': 0.8890391218396061, 'roc_auc': 0.9517202854299869, 'brier': 0.08197013808898203, 'tn': 44786, 'fp': 5801, 'fn': 2501, 'tp': 21333}\n",
      "Final XGB trained in 12.07669472694397 s\n",
      "Saved final XGB to: gs://final_data_kgc2/rf_results/final_xgb_classifier_binary_pig.joblib\n",
      "Test metrics (XGB): {'precision': 0.29216732449187105, 'recall': 0.7448988125104532, 'f1': 0.41971304491231926, 'average_precision': 0.3006170624416304, 'roc_auc': 0.7853725108101951, 'brier': 0.21093398257648327, 'tn': 294218, 'fp': 129481, 'fn': 18303, 'tp': 53445}\n",
      "Saved test predictions to: gs://final_data_kgc2/rf_results/predictions_test_xgb_binary_min_pig.csv\n",
      "Saved summary to: gs://final_data_kgc2/rf_results/training_summary_xgb_classifier_binary_pig.json\n",
      "All outputs saved to gs://final_data_kgc2/rf_results/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train XGBoost classifier for binary bloom/no-bloom — GCS-ready (Pigeon Lake / pig tag)\n",
    "\n",
    "This is the original script with two small, safe changes for Pigeon Lake:\n",
    "- Default NPZ points to the Pigeon dataset.\n",
    "- Default --tag is now \"pig\" (was \"nak\") and the tag is appended to all output filenames\n",
    "  so runs for different lakes won't overwrite each other's artifacts.\n",
    "\n",
    "Usage example:\n",
    "  python3 train_xgb_binary_gcs_tagged_pig.py --tag pig --out_dir gs://final_data_kgc2/rf_results/ ...\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, PredefinedSplit, ParameterSampler\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    brier_score_loss, confusion_matrix\n",
    ")\n",
    "from sklearn.base import clone\n",
    "import joblib\n",
    "\n",
    "# joblib parallel backend\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# optional imports (GCS + progress)\n",
    "try:\n",
    "    import gcsfs\n",
    "except Exception:\n",
    "    gcsfs = None\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "# xgboost import\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception:\n",
    "    XGBClassifier = None\n",
    "\n",
    "# preferred storage client\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "except Exception:\n",
    "    storage = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated as an API\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------\n",
    "# Defaults pointing to your bucket (Pigeon Lake)\n",
    "# ---------------------------\n",
    "DEFAULT_NPZ = \"gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\"\n",
    "DEFAULT_SAMPLES_DIR = \"gs://final_data_kgc2/Final_data/\"\n",
    "DEFAULT_OUT_DIR = \"gs://final_data_kgc2/rf_results/\"\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers: GCS/local I/O\n",
    "# ---------------------------\n",
    "def is_gcs_path(p: str) -> bool:\n",
    "    return isinstance(p, str) and p.startswith(\"gs://\")\n",
    "\n",
    "def gcs_join(prefix: str, *parts: str) -> str:\n",
    "    p = prefix.rstrip('/')\n",
    "    for part in parts:\n",
    "        p = p.rstrip('/') + '/' + str(part).lstrip('/')\n",
    "    return p\n",
    "\n",
    "def upload_file_to_gcs(local_path: str, gs_uri: str, requester_pays_project: str | None = None):\n",
    "    \"\"\"Upload a local file to gs:// using google-cloud-storage (preferred) or gcsfs fallback.\"\"\"\n",
    "    if not is_gcs_path(gs_uri):\n",
    "        raise ValueError(\"gs_uri must be gs:// path\")\n",
    "    # prefer storage client\n",
    "    if storage is not None:\n",
    "        client_kwargs = {}\n",
    "        if requester_pays_project:\n",
    "            client_kwargs[\"project\"] = requester_pays_project\n",
    "        client = storage.Client(**client_kwargs)\n",
    "        _, rest = gs_uri.split(\"gs://\", 1)\n",
    "        bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Destination bucket does not exist or is not accessible: gs://{bucket_name}\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(local_path)\n",
    "        return\n",
    "    # fallback to gcsfs if available\n",
    "    if gcsfs is not None:\n",
    "        fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "        fs.put(local_path, gs_uri)\n",
    "        return\n",
    "    raise RuntimeError(\"No method available to upload to GCS: install google-cloud-storage or gcsfs\")\n",
    "\n",
    "def fetch_gcs_to_local(gcs_path: str, local_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    \"\"\"Download gs:// object to local path. Prefer storage client, fallback to gcsfs.\"\"\"\n",
    "    if not is_gcs_path(gcs_path):\n",
    "        raise ValueError(\"gcs path required\")\n",
    "    # prefer storage client\n",
    "    if storage is not None and not use_gcsfs:\n",
    "        client_kwargs = {}\n",
    "        if requester_pays_project:\n",
    "            client_kwargs[\"project\"] = requester_pays_project\n",
    "        client = storage.Client(**client_kwargs)\n",
    "        _, rest = gcs_path.split(\"gs://\", 1)\n",
    "        bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"Blob not found: {gcs_path}\")\n",
    "        blob.download_to_filename(local_path)\n",
    "        return local_path\n",
    "    # fallback to gcsfs\n",
    "    if gcsfs is None:\n",
    "        raise RuntimeError(\"gcsfs not installed and storage client not available\")\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "    with fs.open(gcs_path, \"rb\") as src, open(local_path, \"wb\") as dst:\n",
    "        shutil.copyfileobj(src, dst)\n",
    "    return local_path\n",
    "\n",
    "def load_npz(npz_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> dict:\n",
    "    \"\"\"Load npz from local path or GCS into a dict.\"\"\"\n",
    "    if is_gcs_path(npz_path):\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix=\".npz\", delete=False)\n",
    "        tmp.close()\n",
    "        fetch_gcs_to_local(npz_path, tmp.name, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "        data = np.load(tmp.name, allow_pickle=True)\n",
    "        result = {k: data[k] for k in data.files}\n",
    "        try:\n",
    "            os.unlink(tmp.name)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return result\n",
    "    else:\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        return {k: data[k] for k in data.files}\n",
    "\n",
    "def load_samples_csvs(samples_dir: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    \"\"\"Load samples_train/val/test CSVs (used for saving predictions and metadata).\"\"\"\n",
    "    def load_one(path_or_dir, name):\n",
    "        if is_gcs_path(str(path_or_dir)):\n",
    "            # prefer storage client\n",
    "            if storage is not None and not use_gcsfs:\n",
    "                client_kwargs = {}\n",
    "                if requester_pays_project:\n",
    "                    client_kwargs[\"project\"] = requester_pays_project\n",
    "                client = storage.Client(**client_kwargs)\n",
    "                _, rest = str(path_or_dir).split(\"gs://\", 1)\n",
    "                bucket_name, _, prefix = rest.partition(\"/\")\n",
    "                prefix = prefix.rstrip('/') + '/'\n",
    "                blob_path = prefix + name\n",
    "                bucket = client.lookup_bucket(bucket_name)\n",
    "                if bucket is None:\n",
    "                    raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "                blob = bucket.blob(blob_path)\n",
    "                if not blob.exists():\n",
    "                    raise FileNotFoundError(f\"Samples CSV not found: gs://{bucket_name}/{blob_path}\")\n",
    "                data = blob.download_as_bytes()\n",
    "                return pd.read_csv(io.BytesIO(data), parse_dates=['date_t', 'date_target'])\n",
    "            # fallback gcsfs\n",
    "            if gcsfs is None:\n",
    "                raise RuntimeError(\"gcsfs not installed and storage client not available\")\n",
    "            fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "            target = str(path_or_dir).rstrip('/') + '/' + name\n",
    "            with fs.open(target, \"rb\") as f:\n",
    "                return pd.read_csv(f, parse_dates=['date_t', 'date_target'])\n",
    "        else:\n",
    "            p = Path(path_or_dir) / name\n",
    "            return pd.read_csv(str(p), parse_dates=['date_t', 'date_target'])\n",
    "    return load_one(samples_dir, \"samples_train.csv\"), load_one(samples_dir, \"samples_val.csv\"), load_one(samples_dir, \"samples_test.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Serialization helpers\n",
    "# ---------------------------\n",
    "def _save_json_atomic_local(path: str, data):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\") as fh:\n",
    "        json.dump(data, fh, default=lambda o: repr(o), indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _to_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_to_serializable(v) for v in obj]\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return _to_serializable(obj.tolist())\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    return obj\n",
    "\n",
    "# ----------------- robust scoring callable -----------------\n",
    "def scoring_callable(estimator, X, y):\n",
    "    try:\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            yprob = estimator.predict_proba(X)[:, 1]\n",
    "            return float(average_precision_score(y, yprob))\n",
    "        elif hasattr(estimator, \"decision_function\"):\n",
    "            yprob = estimator.decision_function(X)\n",
    "            return float(average_precision_score(y, yprob))\n",
    "        else:\n",
    "            yhat = estimator.predict(X)\n",
    "            return float(f1_score(y, yhat, zero_division=0))\n",
    "    except Exception:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "# ----------------- checkpointed randomized search -----------------\n",
    "def _save_json_atomic(path, data):\n",
    "    try:\n",
    "        _save_json_atomic_local(path, data)\n",
    "    except Exception:\n",
    "        with open(path, \"w\") as fh:\n",
    "            json.dump(data, fh, default=lambda o: repr(o), indent=2)\n",
    "\n",
    "def checkpointed_random_search(estimator, param_dist, X_train, y_train, cv, n_iter=8, random_state=RANDOM_STATE, checkpoint_path=None, scorer=None, verbose=False, checkpoint_verbose=False):\n",
    "    if checkpoint_path is None:\n",
    "        checkpoint_path = os.path.join(tempfile.gettempdir(), \"xgb_search_checkpoint.json\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            with open(checkpoint_path, \"r\") as fh:\n",
    "                history = json.load(fh)\n",
    "            tried = set(h['params_repr'] for h in history if 'params_repr' in h)\n",
    "            if verbose:\n",
    "                print(f\"Loaded checkpoint with {len(history)} completed candidates from {checkpoint_path}\")\n",
    "        except Exception:\n",
    "            history = []; tried = set()\n",
    "    else:\n",
    "        history = []; tried = set()\n",
    "    param_list = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state))\n",
    "    remaining = [p for p in param_list if repr(p) not in tried]\n",
    "    best_score = -np.inf; best_params = None; best_est = None\n",
    "    iterator = enumerate(remaining, start=1)\n",
    "    if tqdm is not None and verbose:\n",
    "        iterator = enumerate(tqdm(remaining, desc=\"XGB candidates\", ncols=100), start=1)\n",
    "    for i, params in iterator:\n",
    "        p_repr = repr(params)\n",
    "        if verbose:\n",
    "            print(f\"\\nCandidate {i}/{len(remaining)}: {params}\")\n",
    "        est = clone(estimator); est.set_params(**params)\n",
    "        fold_scores = []; fold_times = []\n",
    "        for fold_idx, (tr_idx, te_idx) in enumerate(cv.split(X_train), start=1):\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                est.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "                try:\n",
    "                    if scorer is not None:\n",
    "                        score = float(scorer(est, X_train[te_idx], y_train[te_idx]))\n",
    "                    else:\n",
    "                        score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "                except TypeError as te:\n",
    "                    if verbose or checkpoint_verbose:\n",
    "                        print(\"Scorer TypeError, falling back. Error:\", te)\n",
    "                    score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "                except Exception as e_other:\n",
    "                    if verbose or checkpoint_verbose:\n",
    "                        print(\"Scorer exception, using fallback:\", e_other)\n",
    "                    score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "            except Exception as e:\n",
    "                score = float(\"-inf\")\n",
    "                if verbose:\n",
    "                    print(\"  fold error during fit/predict/score:\", e)\n",
    "            elapsed = time.time() - t0\n",
    "            fold_scores.append(score); fold_times.append(elapsed)\n",
    "            if verbose:\n",
    "                print(f\"  fold {fold_idx}: score={score if np.isfinite(score) else 'FAILED'}, time={elapsed:.1f}s\")\n",
    "        mean_score = float(np.nanmean([s for s in fold_scores if np.isfinite(s)])) if len(fold_scores) > 0 else float(\"-inf\")\n",
    "        rec = {\"params_repr\": p_repr, \"params\": params, \"fold_scores\": fold_scores, \"fold_times\": fold_times, \"mean_score\": mean_score, \"timestamp\": time.time()}\n",
    "        history.append(rec)\n",
    "        try:\n",
    "            _save_json_atomic(checkpoint_path, history)\n",
    "            if verbose:\n",
    "                print(\"  Saved checkpoint to\", checkpoint_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score; best_params = params\n",
    "            try:\n",
    "                best_est = clone(estimator); best_est.set_params(**params)\n",
    "                best_est.fit(X_train, y_train)\n",
    "                if verbose:\n",
    "                    print(\"  -> New best found and refit on full training set.\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(\"  -> Refit of best failed:\", e)\n",
    "    return best_est, best_params, history\n",
    "\n",
    "def run_random_search(estimator, param_dist, X_train, y_train, cv, n_iter=8, n_jobs=1, scorer=None, random_state=RANDOM_STATE, verbose_search=False, checkpoint_path=None):\n",
    "    if checkpoint_path:\n",
    "        if verbose_search:\n",
    "            print(\"Running checkpointed randomized search (checkpoint_path=%s)\" % checkpoint_path)\n",
    "        best_est, best_params, history = checkpointed_random_search(estimator, param_dist, X_train, y_train, cv, n_iter=n_iter, random_state=random_state, checkpoint_path=checkpoint_path, scorer=scorer, verbose=verbose_search)\n",
    "        return best_est, best_params, history\n",
    "    search = RandomizedSearchCV(estimator, param_distributions=param_dist, n_iter=n_iter,\n",
    "                                scoring=scoring_callable, cv=cv, random_state=random_state,\n",
    "                                n_jobs=n_jobs, verbose=1 if verbose_search else 0)\n",
    "    t0 = time.time()\n",
    "    if n_jobs and n_jobs > 1:\n",
    "        with parallel_backend('threading', n_jobs=n_jobs):\n",
    "            search.fit(X_train, y_train)\n",
    "    else:\n",
    "        search.fit(X_train, y_train)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Search finished in {elapsed:.1f}s; best_score={search.best_score_:.4f}\")\n",
    "    return search.best_estimator_, getattr(search, \"best_params_\", None), search\n",
    "\n",
    "def evaluate_classification(y_true, y_pred_label, y_pred_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred_label = np.asarray(y_pred_label)\n",
    "    y_pred_prob = np.asarray(y_pred_prob)\n",
    "    try:\n",
    "        prec = float(precision_score(y_true, y_pred_label, zero_division=0))\n",
    "        rec = float(recall_score(y_true, y_pred_label, zero_division=0))\n",
    "        f1 = float(f1_score(y_true, y_pred_label, zero_division=0))\n",
    "    except Exception:\n",
    "        prec = rec = f1 = float(\"nan\")\n",
    "    try:\n",
    "        ap = float(average_precision_score(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    try:\n",
    "        roc = float(roc_auc_score(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    try:\n",
    "        brier = float(brier_score_loss(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        brier = float(\"nan\")\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred_label).ravel()\n",
    "    except Exception:\n",
    "        tn = fp = fn = tp = None\n",
    "    return {\n",
    "        \"precision\": prec, \"recall\": rec, \"f1\": f1,\n",
    "        \"average_precision\": ap, \"roc_auc\": roc, \"brier\": brier,\n",
    "        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp\n",
    "    }\n",
    "\n",
    "# ----------------- main -----------------\n",
    "def main(args):\n",
    "    if XGBClassifier is None:\n",
    "        raise RuntimeError(\"xgboost.XGBClassifier not available. Please install xgboost (pip install xgboost).\")\n",
    "\n",
    "    # Tag helper to avoid overwriting outputs across lakes/runs\n",
    "    tag = getattr(args, \"tag\", \"\")\n",
    "    def tag_name(base_filename: str) -> str:\n",
    "        if not tag:\n",
    "            return base_filename\n",
    "        base_noext, ext = os.path.splitext(base_filename)\n",
    "        return f\"{base_noext}_{tag}{ext}\"\n",
    "\n",
    "    # If checkpoint_path not provided, create a tagged one in tempdir so different tags don't clash\n",
    "    checkpoint_path = args.checkpoint_path if args.checkpoint_path else os.path.join(tempfile.gettempdir(), tag_name(\"xgb_search_checkpoint.json\"))\n",
    "\n",
    "    # ensure local outdir exists (for local intermediate files)\n",
    "    if not is_gcs_path(args.out_dir):\n",
    "        os.makedirs(args.out_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Loading processed arrays from:\", args.npz)\n",
    "    data = load_npz(args.npz, requester_pays_project=args.requester_pays_project, use_gcsfs=args.use_gcsfs)\n",
    "\n",
    "    if 'X_train' not in data:\n",
    "        raise RuntimeError(\"processed_dataset must contain X_train/X_val/X_test arrays (and y_train_bin etc).\")\n",
    "\n",
    "    X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "    feature_cols = list(data['feature_cols']) if 'feature_cols' in data else None\n",
    "\n",
    "    if 'y_train_bin' in data and 'y_val_bin' in data and 'y_test_bin' in data:\n",
    "        y_train_bin = np.asarray(data['y_train_bin'])\n",
    "        y_val_bin = np.asarray(data['y_val_bin'])\n",
    "        y_test_bin = np.asarray(data['y_test_bin'])\n",
    "    else:\n",
    "        print(\"Binary targets not found in NPZ — attempting to load from samples CSVs.\")\n",
    "        train_df, val_df, test_df = load_samples_csvs(args.samples_dir, requester_pays_project=args.requester_pays_project, use_gcsfs=args.use_gcsfs)\n",
    "        if 'y_bin' not in train_df.columns:\n",
    "            raise RuntimeError(\"y_bin column not found in samples CSVs nor NPZ. Please run data prep to save y_bin.\")\n",
    "        y_train_bin = train_df['y_bin'].astype(int).values\n",
    "        y_val_bin = val_df['y_bin'].astype(int).values\n",
    "        y_test_bin = test_df['y_bin'].astype(int).values\n",
    "\n",
    "    print(f\"Shapes: X_train={X_train.shape}, X_val={X_val.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "    # load sample CSVs for metadata/predictions (non-fatal)\n",
    "    try:\n",
    "        train_df, val_df, test_df = load_samples_csvs(args.samples_dir, requester_pays_project=args.requester_pays_project, use_gcsfs=args.use_gcsfs)\n",
    "    except Exception:\n",
    "        train_df = val_df = test_df = None\n",
    "\n",
    "    # respect max_train_samples\n",
    "    if args.max_train_samples:\n",
    "        X_train_sub = X_train[:args.max_train_samples]; y_train_sub = y_train_bin[:args.max_train_samples]\n",
    "        print(\"Using subsample for tuning:\", X_train_sub.shape)\n",
    "    else:\n",
    "        X_train_sub, y_train_sub = X_train, y_train_bin\n",
    "\n",
    "    # build CV\n",
    "    if args.use_predefined_split:\n",
    "        X_comb = np.vstack([X_train_sub, X_val]); y_comb = np.concatenate([y_train_sub, y_val_bin])\n",
    "        test_fold = np.concatenate([np.full(X_train_sub.shape[0], -1), np.zeros(X_val.shape[0], dtype=int)])\n",
    "        cv_obj = PredefinedSplit(test_fold)\n",
    "        X_tune, y_tune = X_comb, y_comb\n",
    "        print(\"Using PredefinedSplit (train+val) for tuning.\")\n",
    "    else:\n",
    "        cv_obj = TimeSeriesSplit(n_splits=args.ts_splits)\n",
    "        X_tune, y_tune = X_train_sub, y_train_sub\n",
    "        print(f\"Using TimeSeriesSplit(n_splits={args.ts_splits})\")\n",
    "\n",
    "    # compute imbalance ratio\n",
    "    try:\n",
    "        pos = int(np.sum(y_train_sub == 1))\n",
    "        neg = int(np.sum(y_train_sub == 0))\n",
    "        imbalance_ratio = float(neg / pos) if pos > 0 else 1.0\n",
    "    except Exception:\n",
    "        imbalance_ratio = 1.0\n",
    "\n",
    "    # build param_dist\n",
    "    if args.fast:\n",
    "        param_dist = {\n",
    "            \"n_estimators\": [100, 200],\n",
    "            \"max_depth\": [3, 5],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"subsample\": [0.6, 0.8],\n",
    "            \"colsample_bytree\": [0.6, 0.8],\n",
    "            \"gamma\": [0, 1],\n",
    "            \"reg_lambda\": [1, 5],\n",
    "            \"reg_alpha\": [0, 1],\n",
    "            \"scale_pos_weight\": [1, imbalance_ratio],\n",
    "            \"use_label_encoder\": [False],\n",
    "            \"verbosity\": [0]\n",
    "        }\n",
    "    else:\n",
    "        param_dist = {\n",
    "            \"n_estimators\": [200, 300, 500],\n",
    "            \"max_depth\": [3, 5, 8, 10],\n",
    "            \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "            \"subsample\": [0.5, 0.7, 0.9],\n",
    "            \"colsample_bytree\": [0.5, 0.7, 0.9],\n",
    "            \"gamma\": [0, 0.5, 1.0, 2.0],\n",
    "            \"reg_lambda\": [0.5, 1.0, 5.0],\n",
    "            \"reg_alpha\": [0.0, 0.5, 1.0],\n",
    "            \"scale_pos_weight\": [1, imbalance_ratio],\n",
    "            \"use_label_encoder\": [False],\n",
    "            \"verbosity\": [0]\n",
    "        }\n",
    "\n",
    "    # instantiate XGB\n",
    "    xgb = XGBClassifier(random_state=RANDOM_STATE, n_jobs=1, objective=\"binary:logistic\", eval_metric=\"logloss\")\n",
    "\n",
    "    best_xgb, xgb_params, xgb_search = run_random_search(\n",
    "        xgb, param_dist, X_tune, y_tune, cv_obj,\n",
    "        n_iter=args.n_iter, n_jobs=min(args.n_jobs, 8),\n",
    "        scorer=None, random_state=RANDOM_STATE, verbose_search=args.verbose, checkpoint_path=checkpoint_path\n",
    "    )\n",
    "    if best_xgb is None:\n",
    "        print(\"No XGB candidate found (search returned None). Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Save locally then upload if needed (tagged filenames)\n",
    "    local_tmp = tempfile.mkdtemp(prefix=\"xgb_out_\")\n",
    "    try:\n",
    "        best_initial_local = os.path.join(local_tmp, tag_name(\"xgb_best_initial_binary.joblib\"))\n",
    "        joblib.dump(best_xgb, best_initial_local)\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_best = gcs_join(args.out_dir, os.path.basename(best_initial_local))\n",
    "            upload_file_to_gcs(best_initial_local, dest_best, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved initial best XGB to:\", dest_best)\n",
    "        else:\n",
    "            out_best = os.path.join(args.out_dir, os.path.basename(best_initial_local))\n",
    "            shutil.move(best_initial_local, out_best)\n",
    "            print(\"Saved initial best XGB to:\", out_best)\n",
    "\n",
    "        print(\"XGB best params:\", xgb_params)\n",
    "\n",
    "        # Validate on val\n",
    "        try:\n",
    "            y_val_prob = best_xgb.predict_proba(X_val)[:, 1]\n",
    "            y_val_label = (y_val_prob >= args.prob_threshold).astype(int)\n",
    "            val_metrics = evaluate_classification(y_val_bin, y_val_label, y_val_prob)\n",
    "            print(\"Validation metrics (XGB):\", val_metrics)\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed:\", e)\n",
    "            val_metrics = None\n",
    "\n",
    "        # Retrain on train+val\n",
    "        X_train_val = np.vstack([X_train, X_val]); y_train_val = np.concatenate([y_train_bin, y_val_bin])\n",
    "        final = XGBClassifier(**best_xgb.get_params())\n",
    "        final.set_params(n_jobs=args.n_jobs, random_state=RANDOM_STATE, use_label_encoder=False, verbosity=0)\n",
    "        t0 = time.time(); final.fit(X_train_val, y_train_val); print(\"Final XGB trained in\", time.time() - t0, \"s\")\n",
    "\n",
    "        final_local = os.path.join(local_tmp, tag_name(\"final_xgb_classifier_binary.joblib\"))\n",
    "        joblib.dump(final, final_local)\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_final = gcs_join(args.out_dir, os.path.basename(final_local))\n",
    "            upload_file_to_gcs(final_local, dest_final, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved final XGB to:\", dest_final)\n",
    "        else:\n",
    "            out_final = os.path.join(args.out_dir, os.path.basename(final_local))\n",
    "            shutil.move(final_local, out_final)\n",
    "            print(\"Saved final XGB to:\", out_final)\n",
    "\n",
    "        # Test evaluation & predictions\n",
    "        y_test_prob = final.predict_proba(X_test)[:, 1]\n",
    "        y_test_label = (y_test_prob >= args.prob_threshold).astype(int)\n",
    "        test_metrics = evaluate_classification(y_test_bin, y_test_label, y_test_prob)\n",
    "        print(\"Test metrics (XGB):\", test_metrics)\n",
    "\n",
    "        if test_df is not None:\n",
    "            test_out = test_df.copy()\n",
    "            if 'y_raw' in test_out.columns:\n",
    "                test_out['y_true_bin'] = test_out['y_raw'].apply(lambda v: int(v >= args.threshold) if not pd.isna(v) else np.nan)\n",
    "            else:\n",
    "                test_out['y_true_bin'] = y_test_bin\n",
    "            test_out['y_pred_prob'] = y_test_prob\n",
    "            test_out['y_pred_label'] = y_test_label\n",
    "            preds_local = os.path.join(local_tmp, tag_name(\"predictions_test_xgb_binary.csv\"))\n",
    "            test_out.to_csv(preds_local, index=False)\n",
    "        else:\n",
    "            df_min = pd.DataFrame({\"y_true_bin\": y_test_bin, \"y_pred_prob\": y_test_prob, \"y_pred_label\": y_test_label})\n",
    "            preds_local = os.path.join(local_tmp, tag_name(\"predictions_test_xgb_binary_min.csv\"))\n",
    "            df_min.to_csv(preds_local, index=False)\n",
    "\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_preds = gcs_join(args.out_dir, os.path.basename(preds_local))\n",
    "            upload_file_to_gcs(preds_local, dest_preds, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved test predictions to:\", dest_preds)\n",
    "        else:\n",
    "            out_preds = os.path.join(args.out_dir, os.path.basename(preds_local))\n",
    "            shutil.move(preds_local, out_preds)\n",
    "            print(\"Saved test predictions to:\", out_preds)\n",
    "\n",
    "        # Save summary (tagged)\n",
    "        summary = {\n",
    "            \"model\": \"xgb_classifier_binary\",\n",
    "            \"val_metrics\": val_metrics,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"best_params\": xgb_params,\n",
    "            \"n_iter\": int(args.n_iter),\n",
    "            \"checkpoint_path\": checkpoint_path\n",
    "        }\n",
    "        summary_safe = _to_serializable(summary)\n",
    "        summary_local = os.path.join(local_tmp, tag_name(\"training_summary_xgb_classifier_binary.json\"))\n",
    "        with open(summary_local, \"w\") as fh:\n",
    "            json.dump(summary_safe, fh, indent=2)\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            dest_summary = gcs_join(args.out_dir, os.path.basename(summary_local))\n",
    "            upload_file_to_gcs(summary_local, dest_summary, requester_pays_project=args.requester_pays_project)\n",
    "            print(\"Saved summary to:\", dest_summary)\n",
    "        else:\n",
    "            out_summary = os.path.join(args.out_dir, os.path.basename(summary_local))\n",
    "            shutil.move(summary_local, out_summary)\n",
    "            print(\"Saved summary to:\", out_summary)\n",
    "\n",
    "        print(\"All outputs saved to\", args.out_dir)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            shutil.rmtree(local_tmp)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--npz\", type=str, default=DEFAULT_NPZ, help=\"Processed dataset .npz (must contain X_train/X_val/X_test and y_*_bin)\")\n",
    "    parser.add_argument(\"--samples_dir\", type=str, default=DEFAULT_SAMPLES_DIR, help=\"Folder with samples_train/val/test CSVs (used for metadata & fallback)\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=DEFAULT_OUT_DIR, help=\"Output folder (local path or gs://bucket/path/)\")\n",
    "    parser.add_argument(\"--n_iter\", type=int, default=16)\n",
    "    parser.add_argument(\"--n_jobs\", type=int, default=4)\n",
    "    parser.add_argument(\"--ts_splits\", type=int, default=5)\n",
    "    parser.add_argument(\"--no_predefined_split\", action=\"store_false\", dest=\"use_predefined_split\", help=\"Disable using PredefinedSplit and use TimeSeriesSplit instead.\")\n",
    "    parser.set_defaults(use_predefined_split=True)\n",
    "    parser.add_argument(\"--max_train_samples\", type=int, default=None)\n",
    "    parser.add_argument(\"--fast\", action=\"store_true\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\")\n",
    "    parser.add_argument(\"--checkpoint_path\", type=str, default=None, help=\"Path to JSON checkpoint file to save/resume search (local recommended)\")\n",
    "    parser.add_argument(\"--prob_threshold\", type=float, default=0.5, help=\"Probability threshold to convert probs into labels for evaluation\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=20.0, help=\"Chl a threshold used to create y_bin (informational)\")\n",
    "    parser.add_argument(\"--use_gcsfs\", action=\"store_true\", help=\"Attempt gcsfs for reads if storage client fails (default: prefer storage client)\")\n",
    "    parser.add_argument(\"--requester_pays_project\", type=str, default=None, help=\"GCP project id to use when accessing requester-pays buckets\")\n",
    "    parser.add_argument(\"--tag\", type=str, default=\"pig\", help=\"Optional tag appended to artifact filenames to avoid overwriting (default: pig). Use empty string to disable tagging.\")\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334bf60-0f08-49ec-8575-d629ab3ed75e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMC3 not available: falling back to sklearn LogisticRegression.\n",
      "Loading processed arrays from: gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\n",
      "Shapes: X_train=(948281, 39), X_val=(74421, 39), X_test=(495447, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=8. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Results and model saved with tag: pig\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train a Bayesian logistic classifier for binary bloom/no-bloom — GCS-ready (tagged, non-overwriting)\n",
    "\n",
    "This is an updated variant of your script that:\n",
    "- Adds a --tag CLI argument (default \"pig\").\n",
    "- Includes the tag in saved filenames (scaler, model, results, checkpoints).\n",
    "- Writes the tag into checkpoint/history entries and the final results JSON.\n",
    "- Avoids overwriting previous outputs by embedding a timestamp (and tag) in saved filenames.\n",
    "- Adds a small atomic JSON saver used by the checkpoint procedure.\n",
    "\n",
    "Usage (example):\n",
    "python3 train_bayes_binary_gcs_tagged_pig.py \\\n",
    "  --npz \"gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\" \\\n",
    "  --samples_dir \"gs://final_data_kgc2/Final_data/\" \\\n",
    "  --out_dir \"gs://final_data_kgc2/bayes_results/\" \\\n",
    "  --n_iter 12 --n_jobs 2 --max_iter 5000 --solver saga --scale_features \\\n",
    "  --tag pig --verbose\n",
    "\n",
    "Notes:\n",
    "- When running inside Jupyter / IPython the script now uses parse_known_args() to ignore\n",
    "  Jupyter kernel arguments (e.g. -f /.../kernel-*.json) so it won't raise an argparse error.\n",
    "- This preserves your original logic but ensures outputs carry the \"tag\" metadata\n",
    "  and are written with timestamped filenames to avoid overwrites.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, PredefinedSplit, ParameterSampler\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    brier_score_loss, confusion_matrix\n",
    ")\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optional PyMC3 imports\n",
    "try:\n",
    "    import pymc3 as pm\n",
    "    import arviz as az\n",
    "    PM_AVAILABLE = True\n",
    "except Exception:\n",
    "    pm = None\n",
    "    az = None\n",
    "    PM_AVAILABLE = False\n",
    "\n",
    "# joblib parallel backend\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# optional imports (gcsfs + progress)\n",
    "try:\n",
    "    import gcsfs\n",
    "except Exception:\n",
    "    gcsfs = None\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "# preferred storage client\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "except Exception:\n",
    "    storage = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated as an API\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------\n",
    "# Defaults pointing to your bucket (Pigeon)\n",
    "# ---------------------------\n",
    "DEFAULT_NPZ = \"gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\"\n",
    "DEFAULT_SAMPLES_DIR = \"gs://final_data_kgc2/Final_data/\"\n",
    "DEFAULT_OUT_DIR = \"gs://final_data_kgc2/bayes_results/\"\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers: GCS / local I/O\n",
    "# ---------------------------\n",
    "def is_gcs_path(p: str) -> bool:\n",
    "    return isinstance(p, str) and p.startswith(\"gs://\")\n",
    "\n",
    "def gcs_join(prefix: str, *parts: str) -> str:\n",
    "    p = prefix.rstrip('/')\n",
    "    for part in parts:\n",
    "        p = p.rstrip('/') + '/' + str(part).lstrip('/')\n",
    "    return p\n",
    "\n",
    "def upload_file_to_gcs(local_path: str, gs_uri: str, requester_pays_project: str | None = None):\n",
    "    if not is_gcs_path(gs_uri):\n",
    "        raise ValueError(\"gs_uri must be a gs:// path\")\n",
    "    if storage is not None:\n",
    "        client_kwargs = {}\n",
    "        if requester_pays_project:\n",
    "            client_kwargs[\"project\"] = requester_pays_project\n",
    "        client = storage.Client(**client_kwargs)\n",
    "        _, rest = gs_uri.split(\"gs://\", 1)\n",
    "        bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Destination bucket does not exist or is not accessible: gs://{bucket_name}\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        try:\n",
    "            blob.upload_from_filename(local_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Upload failed for {local_path} -> {gs_uri}: {e}\") from e\n",
    "        return\n",
    "    if gcsfs is not None:\n",
    "        fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "        fs.put(local_path, gs_uri)\n",
    "        return\n",
    "    raise RuntimeError(\"No method available to upload to GCS: install google-cloud-storage or gcsfs\")\n",
    "\n",
    "def fetch_gcs_to_local(gcs_path: str, local_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    if not is_gcs_path(gcs_path):\n",
    "        raise ValueError(\"gcs_path must be gs://...\")\n",
    "    if storage is not None and not use_gcsfs:\n",
    "        client_kwargs = {}\n",
    "        if requester_pays_project:\n",
    "            client_kwargs[\"project\"] = requester_pays_project\n",
    "        client = storage.Client(**client_kwargs)\n",
    "        _, rest = gcs_path.split(\"gs://\", 1)\n",
    "        bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"Blob not found: {gcs_path}\")\n",
    "        blob.download_to_filename(local_path)\n",
    "        return local_path\n",
    "    if gcsfs is None:\n",
    "        raise RuntimeError(\"gcsfs not installed and storage client not available\")\n",
    "    fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "    with fs.open(gcs_path, \"rb\") as src, open(local_path, \"wb\") as dst:\n",
    "        shutil.copyfileobj(src, dst)\n",
    "    return local_path\n",
    "\n",
    "def save_local_or_gcs(local_src_path: str, out_dir: str, dest_basename: str | None = None, requester_pays_project: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Save a local file to out_dir. If out_dir is a GCS path it will upload;\n",
    "    otherwise it will move to a local directory. This function does not attempt\n",
    "    to detect collisions — callers should provide distinct dest_basename values\n",
    "    (we use tag+timestamp in callers).\n",
    "    \"\"\"\n",
    "    if dest_basename is None:\n",
    "        dest_basename = os.path.basename(local_src_path)\n",
    "    if is_gcs_path(out_dir):\n",
    "        gs_dest = gcs_join(out_dir, dest_basename)\n",
    "        upload_file_to_gcs(local_src_path, gs_dest, requester_pays_project=requester_pays_project)\n",
    "        try:\n",
    "            os.remove(local_src_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return gs_dest\n",
    "    else:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        dst = os.path.join(out_dir, dest_basename)\n",
    "        shutil.move(local_src_path, dst)\n",
    "        return dst\n",
    "\n",
    "def load_samples_csvs(samples_dir: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    def load_one(path_or_dir, name):\n",
    "        if is_gcs_path(str(path_or_dir)):\n",
    "            if storage is not None and not use_gcsfs:\n",
    "                client_kwargs = {}\n",
    "                if requester_pays_project:\n",
    "                    client_kwargs[\"project\"] = requester_pays_project\n",
    "                client = storage.Client(**client_kwargs)\n",
    "                _, rest = str(path_or_dir).split(\"gs://\", 1)\n",
    "                bucket_name, _, prefix = rest.partition(\"/\")\n",
    "                prefix = prefix.rstrip('/') + '/'\n",
    "                blob_path = prefix + name\n",
    "                bucket = client.lookup_bucket(bucket_name)\n",
    "                if bucket is None:\n",
    "                    raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "                blob = bucket.blob(blob_path)\n",
    "                if not blob.exists():\n",
    "                    raise FileNotFoundError(f\"Samples CSV not found: gs://{bucket_name}/{blob_path}\")\n",
    "                data = blob.download_as_bytes()\n",
    "                return pd.read_csv(io.BytesIO(data), parse_dates=['date_t', 'date_target'])\n",
    "            if gcsfs is None:\n",
    "                raise RuntimeError(\"gcsfs not installed and storage client not available\")\n",
    "            fs = gcsfs.GCSFileSystem(token=\"google\")\n",
    "            target = str(path_or_dir).rstrip('/') + '/' + name\n",
    "            with fs.open(target, \"rb\") as f:\n",
    "                return pd.read_csv(f, parse_dates=['date_t', 'date_target'])\n",
    "        else:\n",
    "            p = Path(path_or_dir) / name\n",
    "            return pd.read_csv(str(p), parse_dates=['date_t', 'date_target'])\n",
    "    return load_one(samples_dir, \"samples_train.csv\"), load_one(samples_dir, \"samples_val.csv\"), load_one(samples_dir, \"samples_test.csv\")\n",
    "\n",
    "def load_npz(npz_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> dict:\n",
    "    if is_gcs_path(npz_path):\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix=\".npz\", delete=False)\n",
    "        tmp.close()\n",
    "        fetch_gcs_to_local(npz_path, tmp.name, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "        data = np.load(tmp.name, allow_pickle=True)\n",
    "        result = {k: data[k] for k in data.files}\n",
    "        try:\n",
    "            os.unlink(tmp.name)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return result\n",
    "    else:\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        return {k: data[k] for k in data.files}\n",
    "\n",
    "# ---------------------------\n",
    "# Utility: JSON-safe conversion\n",
    "# ---------------------------\n",
    "def _to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: _to_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [_to_serializable(v) for v in obj]\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return _to_serializable(obj.tolist())\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    return obj\n",
    "\n",
    "def timestamp_str():\n",
    "    return time.strftime(\"%Y%m%dT%H%M%S\", time.gmtime())\n",
    "\n",
    "def _save_json_atomic(path: str, data):\n",
    "    \"\"\"Write JSON to path atomically (write temp then replace).\"\"\"\n",
    "    tmp = None\n",
    "    try:\n",
    "        dirn = os.path.dirname(path) or \".\"\n",
    "        fd, tmp = tempfile.mkstemp(prefix=\"tmp_json_\", dir=dirn, text=True)\n",
    "        with os.fdopen(fd, \"w\") as fh:\n",
    "            json.dump(_to_serializable(data), fh, indent=2)\n",
    "            fh.flush()\n",
    "            os.fsync(fh.fileno())\n",
    "        os.replace(tmp, path)\n",
    "    finally:\n",
    "        if tmp and os.path.exists(tmp):\n",
    "            try:\n",
    "                os.remove(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ----------------- scoring callable (same) -----------------\n",
    "def scoring_callable(estimator, X, y):\n",
    "    try:\n",
    "        if hasattr(estimator, \"predict_proba\"):\n",
    "            yprob = estimator.predict_proba(X)[:, 1]\n",
    "            return float(average_precision_score(y, yprob))\n",
    "        elif hasattr(estimator, \"decision_function\"):\n",
    "            yprob = estimator.decision_function(X)\n",
    "            return float(average_precision_score(y, yprob))\n",
    "        else:\n",
    "            yhat = estimator.predict(X)\n",
    "            return float(f1_score(y, yhat, zero_division=0))\n",
    "    except Exception:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "# ----------------- checkpointed randomized search (estimator-safe) -----------------\n",
    "def checkpointed_random_search(estimator, param_dist, X_train, y_train, cv,\n",
    "                               n_iter=8, random_state=RANDOM_STATE,\n",
    "                               checkpoint_path: str | None = None,\n",
    "                               scorer=None, verbose=False, checkpoint_verbose=False,\n",
    "                               run_tag: str | None = None, out_dir: str | None = None, requester_pays_project: str | None = None):\n",
    "    \"\"\"\n",
    "    Performs randomized search but checkpoints history to checkpoint_path (local).\n",
    "    If out_dir is provided, a copy of the checkpoint will be uploaded to out_dir\n",
    "    after each local save (dest named to include tag+timestamp).\n",
    "    Each history record will include 'tag': run_tag (if provided).\n",
    "    \"\"\"\n",
    "    if checkpoint_path is None:\n",
    "        # default local checkpoint in tempdir, include tag+timestamp to avoid collisions\n",
    "        fname = f\"bayes_search_checkpoint_{run_tag or 'run'}_{timestamp_str()}.json\"\n",
    "        checkpoint_path = os.path.join(tempfile.gettempdir(), fname)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        try:\n",
    "            with open(checkpoint_path, \"r\") as fh:\n",
    "                history = json.load(fh)\n",
    "            tried = set(h.get('params_repr') for h in history if 'params_repr' in h)\n",
    "            if verbose:\n",
    "                print(f\"Loaded checkpoint with {len(history)} completed candidates from {checkpoint_path}\")\n",
    "        except Exception:\n",
    "            history = []; tried = set()\n",
    "    else:\n",
    "        history = []; tried = set()\n",
    "    param_list = list(ParameterSampler(param_dist, n_iter=n_iter, random_state=random_state))\n",
    "    remaining = [p for p in param_list if repr(p) not in tried]\n",
    "    best_score = -np.inf; best_params = None; best_est = None\n",
    "    iterator = enumerate(remaining, start=1)\n",
    "    if tqdm is not None and verbose:\n",
    "        iterator = enumerate(tqdm(remaining, desc=\"Bayes candidates\", ncols=100), start=1)\n",
    "    for i, params in iterator:\n",
    "        p_repr = repr(params)\n",
    "        if verbose:\n",
    "            print(f\"\\nCandidate {i}/{len(remaining)}: {params}\")\n",
    "        est = clone(estimator)\n",
    "        try:\n",
    "            valid_keys = set(est.get_params().keys())\n",
    "        except Exception:\n",
    "            valid_keys = set()\n",
    "        set_params = {k: v for k, v in params.items() if k in valid_keys}\n",
    "        fit_kwargs = {k: v for k, v in params.items() if k not in valid_keys}\n",
    "        if set_params:\n",
    "            try:\n",
    "                est.set_params(**set_params)\n",
    "            except Exception:\n",
    "                if verbose:\n",
    "                    print(\"Warning: set_params failed for some params; continuing.\")\n",
    "        fold_scores = []; fold_times = []\n",
    "        for fold_idx, (tr_idx, te_idx) in enumerate(cv.split(X_train), start=1):\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                if type(est).__name__ == \"BayesLogisticClassifier\":\n",
    "                    try:\n",
    "                        est.fit(X_train[tr_idx], y_train[tr_idx], **fit_kwargs)\n",
    "                    except TypeError:\n",
    "                        est.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "                else:\n",
    "                    est.fit(X_train[tr_idx], y_train[tr_idx])\n",
    "                try:\n",
    "                    if scorer is not None:\n",
    "                        score = float(scorer(est, X_train[te_idx], y_train[te_idx]))\n",
    "                    else:\n",
    "                        score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "                except Exception:\n",
    "                    score = float(scoring_callable(est, X_train[te_idx], y_train[te_idx]))\n",
    "            except Exception as e:\n",
    "                score = float(\"-inf\")\n",
    "                if verbose:\n",
    "                    print(\"  fold error during fit/predict/score:\", e)\n",
    "            elapsed = time.time() - t0\n",
    "            fold_scores.append(score); fold_times.append(elapsed)\n",
    "            if verbose:\n",
    "                print(f\"  fold {fold_idx}: score={score if np.isfinite(score) else 'FAILED'}, time={elapsed:.1f}s\")\n",
    "        mean_score = float(np.nanmean([s for s in fold_scores if np.isfinite(s)])) if len(fold_scores) > 0 else float(\"-inf\")\n",
    "        rec = {\"params_repr\": p_repr, \"params\": params, \"fold_scores\": fold_scores, \"fold_times\": fold_times,\n",
    "               \"mean_score\": mean_score, \"timestamp\": time.time()}\n",
    "        if run_tag is not None:\n",
    "            rec[\"tag\"] = run_tag\n",
    "        history.append(rec)\n",
    "        try:\n",
    "            _save_json_atomic(path=checkpoint_path, data=history)\n",
    "            if verbose:\n",
    "                print(\"  Saved checkpoint to\", checkpoint_path)\n",
    "            # mirror checkpoint to out_dir if provided (non-overwriting via timestamped dest)\n",
    "            if out_dir is not None:\n",
    "                try:\n",
    "                    dest_name = f\"bayes_search_checkpoint_{run_tag or 'run'}_{timestamp_str()}.json\"\n",
    "                    # create a local copy path (it already exists as checkpoint_path), so just upload\n",
    "                    save_local_or_gcs(checkpoint_path, out_dir, dest_basename=dest_name, requester_pays_project=requester_pays_project)\n",
    "                except Exception as e:\n",
    "                    if checkpoint_verbose:\n",
    "                        print(\"  Warning: failed to mirror checkpoint to out_dir:\", e)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score; best_params = params\n",
    "            try:\n",
    "                best_clone = clone(estimator)\n",
    "                if set_params:\n",
    "                    try:\n",
    "                        best_clone.set_params(**set_params)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if type(best_clone).__name__ == \"BayesLogisticClassifier\":\n",
    "                    try:\n",
    "                        best_clone.fit(X_train, y_train, **fit_kwargs)\n",
    "                    except TypeError:\n",
    "                        best_clone.fit(X_train, y_train)\n",
    "                else:\n",
    "                    best_clone.fit(X_train, y_train)\n",
    "                best_est = best_clone\n",
    "                if verbose:\n",
    "                    print(\"  -> New best found and refit on full tuning set.\")\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(\"  -> Refit of best failed:\", e)\n",
    "    return best_est, best_params, history\n",
    "\n",
    "# ----------------- evaluation helpers (same) -----------------\n",
    "def evaluate_classification(y_true, y_pred_label, y_pred_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred_label = np.asarray(y_pred_label)\n",
    "    y_pred_prob = np.asarray(y_pred_prob)\n",
    "    try:\n",
    "        prec = float(precision_score(y_true, y_pred_label, zero_division=0))\n",
    "        rec = float(recall_score(y_true, y_pred_label, zero_division=0))\n",
    "        f1 = float(f1_score(y_true, y_pred_label, zero_division=0))\n",
    "    except Exception:\n",
    "        prec = rec = f1 = float(\"nan\")\n",
    "    try:\n",
    "        ap = float(average_precision_score(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    try:\n",
    "        roc = float(roc_auc_score(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    try:\n",
    "        brier = float(brier_score_loss(y_true, y_pred_prob))\n",
    "    except Exception:\n",
    "        brier = float(\"nan\")\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred_label).ravel()\n",
    "        tn, fp, fn, tp = int(tn), int(fp), int(fn), int(tp)\n",
    "    except Exception:\n",
    "        tn = fp = fn = tp = None\n",
    "    return {\n",
    "        \"precision\": prec, \"recall\": rec, \"f1\": f1,\n",
    "        \"average_precision\": ap, \"roc_auc\": roc, \"brier\": brier,\n",
    "        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp\n",
    "    }\n",
    "\n",
    "# ----------------- Bayes wrapper (sketch, similar to earlier) -----------------\n",
    "class BayesLogisticClassifier:\n",
    "    def __init__(self, prior_scale: float = 1.0, advi_iters: int = 10000, n_draws: int = 400, seed: int = RANDOM_STATE):\n",
    "        self.prior_scale = float(prior_scale)\n",
    "        self.advi_iters = int(advi_iters)\n",
    "        self.n_draws = int(n_draws)\n",
    "        self.seed = int(seed)\n",
    "        self.posterior_coef_ = None\n",
    "        self.posterior_intercept_ = None\n",
    "        self.feature_names_in_ = None\n",
    "        self.is_fitted_ = False\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"prior_scale\": self.prior_scale, \"advi_iters\": self.advi_iters, \"n_draws\": self.n_draws, \"seed\": self.seed}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            if hasattr(self, k):\n",
    "                setattr(self, k, v)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y, feature_names=None, verbose=False):\n",
    "        if not PM_AVAILABLE:\n",
    "            raise RuntimeError(\"PyMC3 not available; cannot fit Bayesian model.\")\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=int).ravel()\n",
    "        n_samples, n_features = X.shape\n",
    "        if feature_names is None:\n",
    "            feature_names = [f\"f{i}\" for i in range(n_features)]\n",
    "        self.feature_names_in_ = list(feature_names)\n",
    "        # standardize\n",
    "        self._x_mean = X.mean(axis=0)\n",
    "        self._x_std = X.std(axis=0)\n",
    "        self._x_std[self._x_std == 0.0] = 1.0\n",
    "        Xs = (X - self._x_mean) / self._x_std\n",
    "        with pm.Model() as model:\n",
    "            sigma = pm.HalfNormal(\"sigma\", sigma=self.prior_scale)\n",
    "            intercept = pm.Normal(\"intercept\", mu=0.0, sigma=self.prior_scale)\n",
    "            coeffs = pm.Normal(\"coeffs\", mu=0.0, sigma=self.prior_scale, shape=n_features)\n",
    "            logits = intercept + pm.math.dot(Xs, coeffs)\n",
    "            y_obs = pm.Bernoulli(\"y_obs\", logit_p=logits, observed=y)\n",
    "            approx = pm.fit(n=self.advi_iters, method=\"advi\", random_seed=self.seed, progressbar=verbose)\n",
    "            post_samples = approx.sample(draws=self.n_draws)\n",
    "        try:\n",
    "            if hasattr(post_samples, \"posterior\"):\n",
    "                ds = post_samples.posterior\n",
    "                coef_arr = np.asarray(ds[\"coeffs\"].stack(draws=(\"chain\", \"draw\")).values).reshape(-1, n_features)\n",
    "                intercept_arr = np.asarray(ds[\"intercept\"].stack(draws=(\"chain\", \"draw\")).values).reshape(-1)\n",
    "            elif isinstance(post_samples, dict):\n",
    "                coef_arr = np.asarray(post_samples[\"coeffs\"])\n",
    "                intercept_arr = np.asarray(post_samples[\"intercept\"]).reshape(-1)\n",
    "            else:\n",
    "                idata = az.from_pymc3(trace=post_samples)\n",
    "                ds = idata.posterior\n",
    "                coef_arr = np.asarray(ds[\"coeffs\"].stack(draws=(\"chain\", \"draw\")).values).reshape(-1, n_features)\n",
    "                intercept_arr = np.asarray(ds[\"intercept\"].stack(draws=(\"chain\", \"draw\")).values).reshape(-1)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Unable to extract posterior samples: \" + str(e))\n",
    "        self.posterior_coef_ = coef_arr\n",
    "        self.posterior_intercept_ = intercept_arr\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not getattr(self, \"is_fitted_\", False):\n",
    "            raise RuntimeError(\"Estimator not fitted.\")\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        Xs = (X - self._x_mean) / self._x_std\n",
    "        logits = Xs.dot(self.posterior_coef_.T) + self.posterior_intercept_.reshape(1, -1)\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "        p_mean = probs.mean(axis=1)\n",
    "        return np.vstack([1.0 - p_mean, p_mean]).T\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probs = self.predict_proba(X)[:, 1]\n",
    "        return (probs >= threshold).astype(int)\n",
    "\n",
    "# ----------------- main -----------------\n",
    "def main(args):\n",
    "    local_tmp = tempfile.mkdtemp(prefix=\"bayes_out_\")\n",
    "    try:\n",
    "        if PM_AVAILABLE:\n",
    "            print(\"PyMC3 available: Bayesian path enabled.\")\n",
    "        else:\n",
    "            print(\"PyMC3 not available: falling back to sklearn LogisticRegression.\")\n",
    "\n",
    "        print(\"Loading processed arrays from:\", args.npz)\n",
    "        data = load_npz(args.npz, requester_pays_project=getattr(args, \"requester_pays_project\", None), use_gcsfs=getattr(args, \"use_gcsfs\", False))\n",
    "\n",
    "        if 'X_train' not in data:\n",
    "            raise RuntimeError(\"processed_dataset must contain X_train/X_val/X_test arrays.\")\n",
    "\n",
    "        X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "        feature_cols = list(data['feature_cols']) if 'feature_cols' in data else None\n",
    "\n",
    "        # Load binary targets\n",
    "        if 'y_train_bin' in data and 'y_val_bin' in data and 'y_test_bin' in data:\n",
    "            y_train_bin = np.asarray(data['y_train_bin'])\n",
    "            y_val_bin = np.asarray(data['y_val_bin'])\n",
    "            y_test_bin = np.asarray(data['y_test_bin'])\n",
    "        else:\n",
    "            print(\"Binary targets not found in NPZ — attempting to load from samples CSVs.\")\n",
    "            train_df, val_df, test_df = load_samples_csvs(args.samples_dir, requester_pays_project=getattr(args, \"requester_pays_project\", None), use_gcsfs=getattr(args, \"use_gcsfs\", False))\n",
    "            if 'y_bin' not in train_df.columns:\n",
    "                raise RuntimeError(\"y_bin column not found in samples CSVs nor NPZ.\")\n",
    "            y_train_bin = train_df['y_bin'].astype(int).values\n",
    "            y_val_bin = val_df['y_bin'].astype(int).values\n",
    "            y_test_bin = test_df['y_bin'].astype(int).values\n",
    "\n",
    "        print(f\"Shapes: X_train={X_train.shape}, X_val={X_val.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "        # try to load sample CSVs for metadata\n",
    "        try:\n",
    "            train_df, val_df, test_df = load_samples_csvs(args.samples_dir, requester_pays_project=getattr(args, \"requester_pays_project\", None), use_gcsfs=getattr(args, \"use_gcsfs\", False))\n",
    "        except Exception:\n",
    "            train_df = val_df = test_df = None\n",
    "\n",
    "        # optionally scale features when using sklearn fallback (recommended)\n",
    "        scaler = None\n",
    "        if not PM_AVAILABLE and args.scale_features:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)\n",
    "            X_train = scaler.transform(X_train)\n",
    "            X_val = scaler.transform(X_val)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            # Save scaler locally then upload to out_dir\n",
    "            tmp_scaler = tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\")\n",
    "            tmp_scaler.close()\n",
    "            joblib.dump(scaler, tmp_scaler.name)\n",
    "            dest_name = f\"standard_scaler_bayes_fallback_{args.tag}_{timestamp_str()}.joblib\"\n",
    "            save_local_or_gcs(tmp_scaler.name, args.out_dir, dest_basename=dest_name, requester_pays_project=getattr(args, \"requester_pays_project\", None))\n",
    "\n",
    "        if args.max_train_samples:\n",
    "            X_train_sub = X_train[:args.max_train_samples]; y_train_sub = y_train_bin[:args.max_train_samples]\n",
    "            print(\"Using subsample for tuning:\", X_train_sub.shape)\n",
    "        else:\n",
    "            X_train_sub, y_train_sub = X_train, y_train_bin\n",
    "\n",
    "        # CV\n",
    "        if args.use_predefined_split:\n",
    "            X_comb = np.vstack([X_train_sub, X_val]); y_comb = np.concatenate([y_train_sub, y_val_bin[:len(X_val)]])\n",
    "            test_fold = np.array([ -1 ] * len(X_comb))\n",
    "            # training portion indices 0..len(X_train_sub)-1 are train, rest are validation\n",
    "            test_fold[:len(X_train_sub)] = -1\n",
    "            test_fold[len(X_train_sub):] = 0\n",
    "            ps = PredefinedSplit(test_fold)\n",
    "            cv = ps\n",
    "        else:\n",
    "            cv = TimeSeriesSplit(n_splits=args.n_splits)\n",
    "\n",
    "        # Build estimator and parameter distribution\n",
    "        if PM_AVAILABLE and args.use_bayesian:\n",
    "            base_est = BayesLogisticClassifier(prior_scale=args.prior_scale, advi_iters=args.advi_iters, n_draws=args.n_draws, seed=RANDOM_STATE)\n",
    "            param_dist = {\n",
    "                \"prior_scale\": [0.5, 1.0, 2.0]\n",
    "            }\n",
    "        else:\n",
    "            # sklearn fallback\n",
    "            base_est = LogisticRegression(solver=args.solver, max_iter=args.max_iter, tol=args.tol, class_weight=(None if args.class_weight == \"none\" else args.class_weight), random_state=RANDOM_STATE)\n",
    "            param_dist = {\n",
    "                \"C\": [1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0],\n",
    "                \"penalty\": [\"l2\"] if args.solver in (\"saga\", \"lbfgs\", \"newton-cg\") else [\"l1\", \"l2\"]\n",
    "            }\n",
    "\n",
    "        # Run randomized search with checkpointing; checkpoint saved locally and mirrored to out_dir\n",
    "        checkpoint_name = f\"bayes_search_checkpoint_{args.tag}_{timestamp_str()}.json\"\n",
    "        checkpoint_local_path = os.path.join(tempfile.gettempdir(), checkpoint_name)\n",
    "        best_est, best_params, history = checkpointed_random_search(\n",
    "            base_est, param_dist, X_train_sub, y_train_sub, cv,\n",
    "            n_iter=args.n_iter, random_state=RANDOM_STATE, checkpoint_path=checkpoint_local_path,\n",
    "            scorer=None, verbose=args.verbose, checkpoint_verbose=args.verbose,\n",
    "            run_tag=args.tag, out_dir=args.out_dir, requester_pays_project=getattr(args, \"requester_pays_project\", None)\n",
    "        )\n",
    "\n",
    "        if best_est is None:\n",
    "            raise RuntimeError(\"No estimator succeeded during tuning.\")\n",
    "\n",
    "        # Evaluate best on validation and test sets\n",
    "        y_val_prob = best_est.predict_proba(X_val)[:, 1] if hasattr(best_est, \"predict_proba\") else best_est.decision_function(X_val)\n",
    "        y_val_pred = (y_val_prob >= 0.5).astype(int)\n",
    "        val_metrics = evaluate_classification(y_val_bin, y_val_pred, y_val_prob)\n",
    "\n",
    "        y_test_prob = best_est.predict_proba(X_test)[:, 1] if hasattr(best_est, \"predict_proba\") else best_est.decision_function(X_test)\n",
    "        y_test_pred = (y_test_prob >= 0.5).astype(int)\n",
    "        test_metrics = evaluate_classification(y_test_bin, y_test_pred, y_test_prob)\n",
    "\n",
    "        results = {\n",
    "            \"tag\": args.tag,\n",
    "            \"timestamp\": timestamp_str(),\n",
    "            \"best_params\": best_params,\n",
    "            \"val_metrics\": val_metrics,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"history_len\": len(history)\n",
    "        }\n",
    "\n",
    "        # Save results JSON locally then upload/move to out_dir\n",
    "        tmp_results = tempfile.NamedTemporaryFile(delete=False, suffix=\".json\")\n",
    "        tmp_results.close()\n",
    "        _save_json_atomic(tmp_results.name, results)\n",
    "        res_name = f\"bayes_results_{args.tag}_{timestamp_str()}.json\"\n",
    "        save_local_or_gcs(tmp_results.name, args.out_dir, dest_basename=res_name, requester_pays_project=getattr(args, \"requester_pays_project\", None))\n",
    "\n",
    "        # Save the final estimator (joblib for sklearn or joblib wrapper for Bayes class)\n",
    "        tmp_model = tempfile.NamedTemporaryFile(delete=False, suffix=\".joblib\")\n",
    "        tmp_model.close()\n",
    "        try:\n",
    "            joblib.dump(best_est, tmp_model.name)\n",
    "            model_name = f\"bayes_model_{args.tag}_{timestamp_str()}.joblib\"\n",
    "            save_local_or_gcs(tmp_model.name, args.out_dir, dest_basename=model_name, requester_pays_project=getattr(args, \"requester_pays_project\", None))\n",
    "        except Exception as e:\n",
    "            print(\"Warning: failed to joblib.dump model:\", e)\n",
    "            try:\n",
    "                os.remove(tmp_model.name)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Mirror the local checkpoint if exists (we already attempted mirroring during search,\n",
    "        # but ensure final checkpoint is saved to out_dir)\n",
    "        if os.path.exists(checkpoint_local_path):\n",
    "            try:\n",
    "                dest_name = f\"bayes_search_checkpoint_{args.tag}_{timestamp_str()}.json\"\n",
    "                save_local_or_gcs(checkpoint_local_path, args.out_dir, dest_basename=dest_name, requester_pays_project=getattr(args, \"requester_pays_project\", None))\n",
    "            except Exception as e:\n",
    "                print(\"Warning: failed to upload final checkpoint:\", e)\n",
    "\n",
    "        print(\"Finished. Results and model saved with tag:\", args.tag)\n",
    "    finally:\n",
    "        try:\n",
    "            shutil.rmtree(local_tmp)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train Bayesian/logistic binary classifier (GCS-ready, tagged)\")\n",
    "    parser.add_argument(\"--npz\", type=str, default=DEFAULT_NPZ, help=\"Path to processed dataset .npz\")\n",
    "    parser.add_argument(\"--samples_dir\", type=str, default=DEFAULT_SAMPLES_DIR, help=\"Directory/gs:// path with samples CSVs\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=DEFAULT_OUT_DIR, help=\"Output directory (local or gs://) where results/models are saved\")\n",
    "    parser.add_argument(\"--n_iter\", type=int, default=8, help=\"Randomized search iterations\")\n",
    "    parser.add_argument(\"--n_jobs\", type=int, default=1, help=\"Parallel jobs for fit (if used)\")\n",
    "    parser.add_argument(\"--max_iter\", type=int, default=5000, help=\"Max iterations for sklearn LogisticRegression\")\n",
    "    parser.add_argument(\"--solver\", type=str, default=\"saga\", help=\"Solver for sklearn LogisticRegression\")\n",
    "    parser.add_argument(\"--tol\", type=float, default=1e-4, help=\"Tolerance for sklearn LogisticRegression\")\n",
    "    parser.add_argument(\"--scale_features\", action=\"store_true\", help=\"Whether to StandardScale features for sklearn fallback\")\n",
    "    parser.add_argument(\"--class_weight\", type=str, default=\"none\", help=\"class_weight for sklearn ('balanced' or 'none')\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n",
    "    parser.add_argument(\"--use_predefined_split\", action=\"store_true\", help=\"Use predefined split (train+val arrays) for CV\")\n",
    "    parser.add_argument(\"--n_splits\", type=int, default=5, help=\"n_splits for TimeSeriesSplit CV\")\n",
    "    parser.add_argument(\"--max_train_samples\", type=int, default=0, help=\"If >0, subsample training rows for tuning\")\n",
    "    parser.add_argument(\"--use_gcsfs\", action=\"store_true\", help=\"Force use of gcsfs instead of google-cloud-storage client\")\n",
    "    parser.add_argument(\"--requester_pays_project\", type=str, default=None, help=\"Project to use for requester pays buckets\")\n",
    "    # Bayesian specific options\n",
    "    parser.add_argument(\"--use_bayesian\", action=\"store_true\", help=\"Attempt to use PyMC3 Bayesian classifier if available\")\n",
    "    parser.add_argument(\"--prior_scale\", type=float, default=1.0, help=\"Prior scale for Bayesian classifier\")\n",
    "    parser.add_argument(\"--advi_iters\", type=int, default=10000, help=\"ADVI iterations for Bayesian classifier\")\n",
    "    parser.add_argument(\"--n_draws\", type=int, default=400, help=\"Posterior draws for Bayesian classifier\")\n",
    "    # TAG option\n",
    "    parser.add_argument(\"--tag\", type=str, default=\"pig\", help=\"Tag to attach to outputs (default: pig)\")\n",
    "\n",
    "    # Use parse_known_args() when running inside interactive environments (Jupyter/IPython)\n",
    "    # to ignore extra kernel arguments like '-f /.../kernel-*.json' which would otherwise\n",
    "    # cause argparse to raise \"unrecognized arguments\".\n",
    "    if \"__file__\" not in globals():\n",
    "        args, _ = parser.parse_known_args()\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9faa82-3863-4aa2-8425-18d267a49e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:35:25.920125: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-12 17:35:26.473153: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-12 17:35:28.456419: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-11-12 17:35:28.456556: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-11-12 17:35:28.456566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled (policy = mixed_float16). GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Loading processed arrays from: gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\n",
      "processed .npz contains binary targets (y_*_bin) — will use them for pos_weight and diagnostics.\n",
      "Loading sample CSVs from: gs://final_data_kgc2/models/Pigeon_binary/\n",
      "Listing image files in: gs://final_data_kgc2/Final_data/Pigeon_80m/\n",
      "Using pos_weight = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:35:59.265515: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-12 17:35:59.431765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20750 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized ConvLSTMSequence with 948281 valid entries (skipped 0).\n",
      "Initialized ConvLSTMSequence with 74421 valid entries (skipped 0).\n",
      "Initialized ConvLSTMSequence with 495447 valid entries (skipped 0).\n",
      "Channels total: 1\n",
      "Using LossScaleOptimizer for mixed precision (legacy Adam).\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " convlstm_input (InputLayer)  [(None, 14, 80, 80, 1)]  0         \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 80, 80, 32)        38144     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 80, 80, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 80, 80, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 80, 80, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 80, 80, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 80, 80, 1)         33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,681\n",
      "Trainable params: 47,553\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Epoch 1 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 17:43:15.027265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8900\n",
      "2025-11-12 17:43:15.733569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0840 - acc: 0.8391Epoch 1 metrics: pix_ap=0.3779683294758851, pix_roc=0.6378701410821424 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.43430527598787366, samp_ap_p90=0.26318096254396794, samp_roc_mean=0.6649771271576964, samp_roc_p90=0.23820014562973796 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch1_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch1_pig.json\n",
      "New best aggregated selection score: 0.26318096254396794 (previous -inf); saving model to /var/tmp/convlstm_work_mv9ep3sk/best_model_agg_pig.keras\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/best_model_agg_pig.keras -> gs://final_data_kgc2/convlstm_results/best_model_agg_pig.keras\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_sample_preds_epoch1_pig.csv -> gs://final_data_kgc2/convlstm_results/val_sample_preds_epoch1_pig.csv\n",
      "59268/59268 [==============================] - 11255s 190ms/step - loss: 0.0840 - acc: 0.8391 - val_loss: 1.1887 - val_acc: 0.7224 - lr: 1.0000e-04\n",
      "Epoch 2 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 2/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0503 - acc: 0.8885Epoch 2 metrics: pix_ap=0.3327656180982602, pix_roc=0.572165869549058 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.2954009534901705, samp_ap_p90=0.25340821153687176, samp_roc_mean=0.4046683000564727, samp_roc_p90=0.2305881021090322 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch2_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch2_pig.json\n",
      "59268/59268 [==============================] - 11217s 189ms/step - loss: 0.0503 - acc: 0.8885 - val_loss: 2.2712 - val_acc: 0.6889 - lr: 1.0000e-04\n",
      "Epoch 3 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 3/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0411 - acc: 0.8869Epoch 3 metrics: pix_ap=0.3599832410041669, pix_roc=0.614257184154841 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.29540104607804835, samp_ap_p90=0.40034344729986177, samp_roc_mean=0.4046746383328648, samp_roc_p90=0.6162953998184997 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch3_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch3_pig.json\n",
      "New best aggregated selection score: 0.40034344729986177 (previous 0.26318096254396794); saving model to /var/tmp/convlstm_work_mv9ep3sk/best_model_agg_pig.keras\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/best_model_agg_pig.keras -> gs://final_data_kgc2/convlstm_results/best_model_agg_pig.keras\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_sample_preds_epoch3_pig.csv -> gs://final_data_kgc2/convlstm_results/val_sample_preds_epoch3_pig.csv\n",
      "59268/59268 [==============================] - 11206s 189ms/step - loss: 0.0411 - acc: 0.8869 - val_loss: 2.5807 - val_acc: 0.6965 - lr: 1.0000e-04\n",
      "Epoch 4 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 4/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0361 - acc: 0.8854Epoch 4 metrics: pix_ap=0.36314211213486264, pix_roc=0.6232633610506495 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.29540119995471, samp_ap_p90=0.3529598399372488, samp_roc_mean=0.4046697772182437, samp_roc_p90=0.5784421113464505 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch4_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch4_pig.json\n",
      "59268/59268 [==============================] - 11138s 188ms/step - loss: 0.0361 - acc: 0.8854 - val_loss: 3.2467 - val_acc: 0.6778 - lr: 1.0000e-04\n",
      "Epoch 5 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 5/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0330 - acc: 0.8852\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 5 metrics: pix_ap=0.33784587206446304, pix_roc=0.5880863439530384 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.3109870764299273, samp_ap_p90=0.33894072881500226, samp_roc_mean=0.44402004141762547, samp_roc_p90=0.49816579305085673 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch5_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch5_pig.json\n",
      "59268/59268 [==============================] - 11156s 188ms/step - loss: 0.0330 - acc: 0.8852 - val_loss: 4.2478 - val_acc: 0.6757 - lr: 1.0000e-04\n",
      "Epoch 6 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 6/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0284 - acc: 0.8866Epoch 6 metrics: pix_ap=0.3483353057269798, pix_roc=0.6012911552297013 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.29540103871317314, samp_ap_p90=0.342781616840846, samp_roc_mean=0.40466869153337104, samp_roc_p90=0.5249881280068878 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch6_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch6_pig.json\n",
      "59268/59268 [==============================] - 11122s 188ms/step - loss: 0.0284 - acc: 0.8866 - val_loss: 4.6856 - val_acc: 0.6776 - lr: 5.0000e-05\n",
      "Epoch 7 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 7/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0272 - acc: 0.8849Epoch 7 metrics: pix_ap=0.36030036274504407, pix_roc=0.6162072184007394 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.2954012020166847, samp_ap_p90=0.3918816981090316, samp_roc_mean=0.4046729836794492, samp_roc_p90=0.6338462974012938 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch7_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch7_pig.json\n",
      "59268/59268 [==============================] - 11125s 188ms/step - loss: 0.0272 - acc: 0.8849 - val_loss: 5.0648 - val_acc: 0.6773 - lr: 5.0000e-05\n",
      "Epoch 8 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 8/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0254 - acc: 0.8826\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 9 metrics: pix_ap=0.37303601460214075, pix_roc=0.6263268733531221 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.3220711212977119, samp_ap_p90=0.39188161288602896, samp_roc_mean=0.4158660231458825, samp_roc_p90=0.6338459059243955 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch9_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch9_pig.json\n",
      "59268/59268 [==============================] - 11167s 188ms/step - loss: 0.0254 - acc: 0.8826 - val_loss: 4.7615 - val_acc: 0.6849 - lr: 5.0000e-05\n",
      "Epoch 10 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 10/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0236 - acc: 0.8825Epoch 10 metrics: pix_ap=0.3825951744880317, pix_roc=0.6348069973191456 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.34497725351865166, samp_ap_p90=0.39188161288602896, samp_roc_mean=0.4708097378166579, samp_roc_p90=0.6338459059243955 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch10_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch10_pig.json\n",
      "59268/59268 [==============================] - 11165s 188ms/step - loss: 0.0236 - acc: 0.8825 - val_loss: 4.5661 - val_acc: 0.6878 - lr: 2.5000e-05\n",
      "Epoch 11 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 11/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0232 - acc: 0.8819Epoch 11 metrics: pix_ap=0.3833992514817876, pix_roc=0.6325157524528511 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.344977168295649, samp_ap_p90=0.39188161288602896, samp_roc_mean=0.47080934633975957, samp_roc_p90=0.6338459059243955 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch11_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch11_pig.json\n",
      "59268/59268 [==============================] - 11193s 189ms/step - loss: 0.0232 - acc: 0.8819 - val_loss: 4.4836 - val_acc: 0.6843 - lr: 2.5000e-05\n",
      "Epoch 12 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 12/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0228 - acc: 0.8811Epoch 12 metrics: pix_ap=0.38798325931428684, pix_roc=0.6367289734976278 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.3449771703576237, samp_ap_p90=0.39188161288602896, samp_roc_mean=0.470812552800965, samp_roc_p90=0.6338459059243955 (n_samp=74421, n_samp_pos=23834)\n",
      "Uploaded /var/tmp/convlstm_work_mv9ep3sk/val_metrics_epoch12_pig.json -> gs://final_data_kgc2/convlstm_results/val_metrics_epoch12_pig.json\n",
      "59268/59268 [==============================] - 11141s 188ms/step - loss: 0.0228 - acc: 0.8811 - val_loss: 4.5101 - val_acc: 0.6825 - lr: 2.5000e-05\n",
      "Epoch 13 start: validation valid_samples=74421, total_val_pos_pixels=70150987\n",
      "Epoch 13/150\n",
      "59268/59268 [==============================] - ETA: 0s - loss: 0.0224 - acc: 0.8804\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 13 metrics: pix_ap=0.3779683294758851, pix_roc=0.6378701410821424 (n_pix=222655298, n_pix_pos=70150987); samp_ap_mean=0.43430527598787366, samp_ap_p90=0.26318096254396794, samp_roc_mean=0.6649771271576964, samp_roc_p90=0.23820014562973796 (n_samp=74421, n_samp_pos=23834)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ConvLSTM training script (GCS-ready) with additional JSON metrics written for validation & test,\n",
    "and optional artifact tagging to avoid overwriting outputs when running multiple lakes.\n",
    "\n",
    "New behavior:\n",
    " - Adds a CLI flag --tag (default \"pig\"). The tag is appended to artifact filenames:\n",
    "   e.g. predictions_test_convlstm_pig.csv, test_metrics_pig.json, best_model_pig.keras, history_pig.csv, etc.\n",
    " - CustomValMetrics accepts a tag and writes per-epoch val CSV/JSON files suffixed with the tag.\n",
    " - Test predictions and metrics are written with the tag so multiple runs (different lakes) don't overwrite each other.\n",
    "\n",
    "Everything else in the original script (GCS handling, Sequence loader, training callbacks) is preserved.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import tempfile\n",
    "import shutil\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "from glob import glob\n",
    "from types import SimpleNamespace\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# optional rasterio for .tif reading\n",
    "try:\n",
    "    import rasterio\n",
    "except Exception:\n",
    "    rasterio = None\n",
    "\n",
    "# optional PIL fallback for TIFF/other image formats\n",
    "try:\n",
    "    from PIL import Image\n",
    "except Exception:\n",
    "    Image = None\n",
    "\n",
    "# optional gcsfs\n",
    "try:\n",
    "    import gcsfs\n",
    "except Exception:\n",
    "    gcsfs = None\n",
    "\n",
    "# preferred storage client\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "except Exception:\n",
    "    storage = None\n",
    "\n",
    "# allow explicit service-account credentials for workers (if provided via env var)\n",
    "DEFAULT_GOOGLE_CREDS = None\n",
    "try:\n",
    "    from google.oauth2 import service_account as _sa\n",
    "    sa_path = os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "    if sa_path and os.path.exists(sa_path):\n",
    "        try:\n",
    "            DEFAULT_GOOGLE_CREDS = _sa.Credentials.from_service_account_file(sa_path)\n",
    "            print(\"Loaded service-account credentials from:\", sa_path)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load GOOGLE_APPLICATION_CREDENTIALS:\", e)\n",
    "            DEFAULT_GOOGLE_CREDS = None\n",
    "except Exception:\n",
    "    DEFAULT_GOOGLE_CREDS = None\n",
    "\n",
    "# sklearn metrics for aggregated validation evaluation and final metrics\n",
    "try:\n",
    "    from sklearn.metrics import (\n",
    "        average_precision_score,\n",
    "        roc_auc_score,\n",
    "        precision_recall_fscore_support,\n",
    "        precision_recall_curve,\n",
    "    )\n",
    "except Exception:\n",
    "    average_precision_score = None\n",
    "    roc_auc_score = None\n",
    "    precision_recall_fscore_support = None\n",
    "    precision_recall_curve = None\n",
    "\n",
    "# -------------------- GPU / mixed-precision setup --------------------\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        for g in gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(g, True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        print(\"Mixed precision enabled (policy = mixed_float16). GPUs detected:\", gpus)\n",
    "    else:\n",
    "        print(\"No GPUs detected; running in CPU/fp32 mode.\")\n",
    "except Exception as e:\n",
    "    print(\"Warning: GPU initialization failed or not available:\", e)\n",
    "\n",
    "# -------------------- Defaults --------------------\n",
    "DEFAULTS = {\n",
    "    \"processed_npz\": \"gs://final_data_kgc2/models/Pigeon_binary/processed_dataset_binary.npz\",\n",
    "    \"samples_dir\": \"gs://final_data_kgc2/models/Pigeon_binary/\",\n",
    "    \"image_dir\": \"gs://final_data_kgc2/Final_data/Pigeon_80m/\",\n",
    "    \"out_dir\": \"gs://final_data_kgc2/convlstm_results/\",\n",
    "    \"seq_len\": 14,\n",
    "    \"img_h\": 80,\n",
    "    \"img_w\": 80,\n",
    "    \"img_ch\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 150,\n",
    "    \"patience\": 12,\n",
    "    \"use_attention\": True,\n",
    "    \"use_env\": True,\n",
    "    \"use_mask_channel\": True,\n",
    "    \"threshold\": 20.0,\n",
    "    \"lr\": 1e-4,\n",
    "    \"clipnorm\": 1.0,\n",
    "    \"min_valid_pixels\": 0,\n",
    "    \"compute_pos_weight\": True,\n",
    "    \"pos_weight\": None,\n",
    "    \"pad_value\": 0.0\n",
    "}\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def is_gcs_path(p: str) -> bool:\n",
    "    return isinstance(p, str) and p.startswith(\"gs://\")\n",
    "\n",
    "def gcs_join(prefix: str, *parts: str) -> str:\n",
    "    p = prefix.rstrip('/')\n",
    "    for part in parts:\n",
    "        p = p.rstrip('/') + '/' + str(part).lstrip('/')\n",
    "    return p\n",
    "\n",
    "def _hash_path(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_date_from_name(name: str):\n",
    "    import re\n",
    "    date_pat1 = re.compile(r\"(\\d{4}-\\d{2}-\\d{2})\")\n",
    "    date_pat2 = re.compile(r\"(\\d{8})\")\n",
    "    m = date_pat1.search(name)\n",
    "    if m:\n",
    "        return pd.to_datetime(m.group(1)).normalize()\n",
    "    m = date_pat2.search(name)\n",
    "    if m:\n",
    "        return pd.to_datetime(m.group(1), format=\"%Y%m%d\").normalize()\n",
    "    raise ValueError(f\"Cannot parse date from filename: {name}\")\n",
    "\n",
    "# -------------------- Metrics helpers --------------------\n",
    "def safe_roc_auc(y_true, y_score):\n",
    "    try:\n",
    "        if average_precision_score is None or roc_auc_score is None:\n",
    "            return None\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            return None\n",
    "        return float(roc_auc_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def best_threshold_from_pr_curve(y_true, y_score):\n",
    "    \"\"\"Return (best_threshold, best_f1, precision_at_best, recall_at_best).\"\"\"\n",
    "    try:\n",
    "        if precision_recall_curve is None:\n",
    "            return None, 0.0, None, None\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "        if thresholds.size == 0:\n",
    "            return None, 0.0, None, None\n",
    "        p = precision[:-1]\n",
    "        r = recall[:-1]\n",
    "        denom = (p + r)\n",
    "        f1s = np.zeros_like(p)\n",
    "        valid = denom > 0\n",
    "        f1s[valid] = 2 * p[valid] * r[valid] / denom[valid]\n",
    "        best_idx = int(np.nanargmax(f1s))\n",
    "        return float(thresholds[best_idx]), float(f1s[best_idx]), float(p[best_idx]), float(r[best_idx])\n",
    "    except Exception:\n",
    "        return None, 0.0, None, None\n",
    "\n",
    "def compute_sample_metrics_from_arrays(y_true, y_score):\n",
    "    \"\"\"Compute precision/recall/f1 at 0.5, best threshold from PR-curve, AP and ROC.\"\"\"\n",
    "    out = {}\n",
    "    if len(y_true) == 0:\n",
    "        return {\n",
    "            \"n\": 0,\n",
    "            \"n_pos\": 0,\n",
    "            \"precision_at_0.5\": None,\n",
    "            \"recall_at_0.5\": None,\n",
    "            \"f1_at_0.5\": None,\n",
    "            \"best_threshold_by_f1\": None,\n",
    "            \"precision_at_best\": None,\n",
    "            \"recall_at_best\": None,\n",
    "            \"f1_at_best\": None,\n",
    "            \"average_precision\": None,\n",
    "            \"roc_auc\": None\n",
    "        }\n",
    "    # threshold 0.5\n",
    "    try:\n",
    "        y_pred05 = (y_score >= 0.5).astype(int)\n",
    "        if precision_recall_fscore_support is not None:\n",
    "            prec05, rec05, f105, _ = precision_recall_fscore_support(y_true, y_pred05, average=\"binary\", zero_division=0)\n",
    "        else:\n",
    "            prec05 = rec05 = f105 = None\n",
    "    except Exception:\n",
    "        prec05 = rec05 = f105 = None\n",
    "\n",
    "    # AP\n",
    "    ap = None\n",
    "    try:\n",
    "        if average_precision_score is not None:\n",
    "            ap = float(average_precision_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        ap = None\n",
    "\n",
    "    # ROC\n",
    "    roc = safe_roc_auc(y_true, y_score)\n",
    "\n",
    "    # best threshold\n",
    "    best_th, best_f1, p_best, r_best = best_threshold_from_pr_curve(y_true, y_score)\n",
    "    if best_th is None:\n",
    "        # fallback to 0.5 metrics\n",
    "        best_th = 0.5\n",
    "        best_f1 = float(f105) if f105 is not None else None\n",
    "        p_best = float(prec05) if prec05 is not None else None\n",
    "        r_best = float(rec05) if rec05 is not None else None\n",
    "\n",
    "    out = {\n",
    "        \"n\": int(len(y_true)),\n",
    "        \"n_pos\": int(int(np.sum(y_true == 1))),\n",
    "        \"precision_at_0.5\": (float(prec05) if prec05 is not None else None),\n",
    "        \"recall_at_0.5\": (float(rec05) if rec05 is not None else None),\n",
    "        \"f1_at_0.5\": (float(f105) if f105 is not None else None),\n",
    "        \"best_threshold_by_f1\": (float(best_th) if best_th is not None else None),\n",
    "        \"precision_at_best\": (float(p_best) if p_best is not None else None),\n",
    "        \"recall_at_best\": (float(r_best) if r_best is not None else None),\n",
    "        \"f1_at_best\": (float(best_f1) if best_f1 is not None else None),\n",
    "        \"average_precision\": (float(ap) if ap is not None else None),\n",
    "        \"roc_auc\": (float(roc) if roc is not None else None)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# -------------------- GCS helpers (atomic download + retry) --------------------\n",
    "def _make_storage_client(project: str | None = None):\n",
    "    \"\"\"Create a google.cloud.storage.Client using DEFAULT_GOOGLE_CREDS if available.\"\"\"\n",
    "    if storage is None:\n",
    "        raise RuntimeError(\"google-cloud-storage not installed\")\n",
    "    if DEFAULT_GOOGLE_CREDS is not None:\n",
    "        # storage.Client accepts project and credentials kwargs\n",
    "        return storage.Client(project=project, credentials=DEFAULT_GOOGLE_CREDS)\n",
    "    return storage.Client(project=project)\n",
    "\n",
    "def _make_gcsfs(token=None, project: str | None = None):\n",
    "    \"\"\"Create gcsfs filesystem using provided token or DEFAULT_GOOGLE_CREDS if available.\"\"\"\n",
    "    if gcsfs is None:\n",
    "        raise RuntimeError(\"gcsfs not installed\")\n",
    "    if token is None and DEFAULT_GOOGLE_CREDS is not None:\n",
    "        token = DEFAULT_GOOGLE_CREDS\n",
    "    # gcsfs accepts google-auth Credentials objects as token\n",
    "    return gcsfs.GCSFileSystem(token=token, project=project)\n",
    "\n",
    "def upload_file_to_gcs(local_path: str, gs_uri: str, requester_pays_project: str | None = None):\n",
    "    if not is_gcs_path(gs_uri):\n",
    "        raise ValueError(\"gs_uri must be gs:// path\")\n",
    "    if storage is not None:\n",
    "        client = _make_storage_client(project=requester_pays_project)\n",
    "        _, rest = gs_uri.split(\"gs://\", 1)\n",
    "        bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        blob.upload_from_filename(local_path)\n",
    "        return\n",
    "    if gcsfs is not None:\n",
    "        fs = _make_gcsfs()\n",
    "        fs.put(local_path, gs_uri)\n",
    "        return\n",
    "    raise RuntimeError(\"Install google-cloud-storage or gcsfs to upload to GCS\")\n",
    "\n",
    "def upload_dir_to_gcs(local_dir: str, gs_prefix: str, requester_pays_project: str | None = None):\n",
    "    for root, _dirs, files in os.walk(local_dir):\n",
    "        for fn in files:\n",
    "            local_fp = os.path.join(root, fn)\n",
    "            rel = os.path.relpath(local_fp, local_dir)\n",
    "            gs_dst = gcs_join(gs_prefix, rel)\n",
    "            upload_file_to_gcs(local_fp, gs_dst, requester_pays_project=requester_pays_project)\n",
    "\n",
    "def fetch_gcs_to_local(gcs_path: str, local_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    \"\"\"\n",
    "    Download a gs:// path to local_path atomically.\n",
    "    Prefer google-cloud-storage client; fallback to gcsfs. Download to a .tmp file and\n",
    "    os.replace to final path to avoid race conditions where another worker reads\n",
    "    a zero-length file.\n",
    "    \"\"\"\n",
    "    if not is_gcs_path(gcs_path):\n",
    "        raise ValueError(\"gcs_path must be gs://...\")\n",
    "    _, rest = gcs_path.split(\"gs://\", 1)\n",
    "    bucket_name, _, blob_path = rest.partition(\"/\")\n",
    "\n",
    "    # Preferred path: google-cloud-storage client\n",
    "    if storage is not None and not use_gcsfs:\n",
    "        client = _make_storage_client(project=requester_pays_project)\n",
    "        bucket = client.lookup_bucket(bucket_name)\n",
    "        if bucket is None:\n",
    "            raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name} (check credentials/project)\")\n",
    "        blob = bucket.blob(blob_path)\n",
    "        if not blob.exists():\n",
    "            raise FileNotFoundError(f\"Blob not found: {gcs_path}\")\n",
    "        # quick sanity checks\n",
    "        if blob.size is not None and blob.size < 16:\n",
    "            raise RuntimeError(f\"Blob seems unexpectedly small ({blob.size} bytes). Possible permission/404 response.\")\n",
    "        if blob.content_type and blob.content_type.startswith(\"text/\"):\n",
    "            raise RuntimeError(f\"Blob content_type is '{blob.content_type}' — download may be an HTML error page.\")\n",
    "        tmp_path = local_path + \".tmp\"\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        try:\n",
    "            blob.download_to_filename(tmp_path)\n",
    "            # ensure file non-empty\n",
    "            if not os.path.exists(tmp_path) or os.path.getsize(tmp_path) == 0:\n",
    "                if os.path.exists(tmp_path):\n",
    "                    try:\n",
    "                        os.remove(tmp_path)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                raise RuntimeError(f\"Downloaded file for {gcs_path} is zero bytes or missing.\")\n",
    "            os.replace(tmp_path, local_path)\n",
    "        except Exception:\n",
    "            # cleanup partial tmp file, then re-raise\n",
    "            try:\n",
    "                if os.path.exists(tmp_path):\n",
    "                    os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "            raise\n",
    "        return local_path\n",
    "\n",
    "    # fallback to gcsfs (also write atomically)\n",
    "    if gcsfs is None:\n",
    "        raise RuntimeError(\"gcsfs not installed and storage client not available\")\n",
    "    fs = _make_gcsfs(project=requester_pays_project)\n",
    "    tmp_path = local_path + \".tmp\"\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    try:\n",
    "        with fs.open(gcs_path, \"rb\") as src, open(tmp_path, \"wb\") as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "    except Exception:\n",
    "        try:\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.remove(tmp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise\n",
    "    if not os.path.exists(tmp_path) or os.path.getsize(tmp_path) == 0:\n",
    "        try:\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.remove(tmp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "        raise RuntimeError(f\"gcsfs download produced zero-length file for {gcs_path}\")\n",
    "    os.replace(tmp_path, local_path)\n",
    "    return local_path\n",
    "\n",
    "# -------------------- Listing image files --------------------\n",
    "def list_image_files(image_dir: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    file_map = OrderedDict()\n",
    "    if is_gcs_path(image_dir):\n",
    "        if storage is not None and not use_gcsfs:\n",
    "            client = _make_storage_client(project=requester_pays_project)\n",
    "            _, rest = image_dir.split(\"gs://\", 1)\n",
    "            bucket_name, _, prefix = rest.partition(\"/\")\n",
    "            prefix = prefix.rstrip('/') + '/'\n",
    "            blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "            for b in blobs:\n",
    "                name = b.name\n",
    "                if name.lower().endswith((\".npy\", \".tif\", \".tiff\")):\n",
    "                    bn = os.path.basename(name)\n",
    "                    try:\n",
    "                        d = parse_date_from_name(bn)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    file_map[pd.Timestamp(d)] = f\"gs://{bucket_name}/{name}\"\n",
    "            return file_map\n",
    "        # use gcsfs fallback with explicit token if available\n",
    "        fs = _make_gcsfs(project=requester_pays_project)\n",
    "        pattern = image_dir.rstrip('/') + '/*'\n",
    "        files = fs.glob(pattern)\n",
    "        for f in sorted(files):\n",
    "            bn = os.path.basename(f)\n",
    "            if not bn.lower().endswith((\".npy\", \".tif\", \".tiff\")):\n",
    "                continue\n",
    "            try:\n",
    "                d = parse_date_from_name(bn)\n",
    "            except Exception:\n",
    "                continue\n",
    "            file_map[pd.Timestamp(d)] = f if f.startswith(\"gs://\") else \"gs://\" + f\n",
    "        return file_map\n",
    "    # local path\n",
    "    files = sorted(glob(os.path.join(image_dir, \"*\")))\n",
    "    for f in files:\n",
    "        if not f.lower().endswith((\".npy\", \".tif\", \".tiff\")):\n",
    "            continue\n",
    "        try:\n",
    "            d = parse_date_from_name(os.path.basename(f))\n",
    "        except Exception:\n",
    "            continue\n",
    "        file_map[pd.Timestamp(d)] = f\n",
    "    return file_map\n",
    "\n",
    "# -------------------- Local cache-aware loader --------------------\n",
    "CACHE_DIR = None\n",
    "def _ensure_cache_dir():\n",
    "    global CACHE_DIR\n",
    "    if CACHE_DIR is None:\n",
    "        CACHE_DIR = tempfile.mkdtemp(prefix=\"image_cache_\")\n",
    "    return CACHE_DIR\n",
    "\n",
    "@lru_cache(maxsize=2048)\n",
    "def cached_load(path: str, target_h: int | None = None, target_w: int | None = None, requester_pays_project: str | None = None, use_gcsfs: bool = False, pad_value: float = 0.0):\n",
    "    \"\"\"\n",
    "    Load an image path (local or gs://) into a float32 numpy array.\n",
    "    For GCS paths, download atomically into a cache dir. Retry once with fallback\n",
    "    method in case of transient failures.\n",
    "    \"\"\"\n",
    "    cache_dir = _ensure_cache_dir()\n",
    "    if is_gcs_path(path):\n",
    "        h = _hash_path(path)\n",
    "        ext = os.path.splitext(path)[1] or \".npy\"\n",
    "        local_fp = os.path.join(cache_dir, f\"{h}{ext}\")\n",
    "        if not os.path.exists(local_fp):\n",
    "            # first attempt: preferred client\n",
    "            try:\n",
    "                fetch_gcs_to_local(path, local_fp, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "            except Exception as e:\n",
    "                # cleanup any partial files and retry once with toggled use_gcsfs\n",
    "                try:\n",
    "                    if os.path.exists(local_fp):\n",
    "                        os.remove(local_fp)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    fetch_gcs_to_local(path, local_fp, requester_pays_project=requester_pays_project, use_gcsfs=not use_gcsfs)\n",
    "                except Exception as e2:\n",
    "                    raise RuntimeError(f\"Failed to download {path}: {e} ; retry also failed: {e2}\")\n",
    "        # ensure local file is non-empty\n",
    "        if not os.path.exists(local_fp) or os.path.getsize(local_fp) == 0:\n",
    "            try:\n",
    "                if os.path.exists(local_fp):\n",
    "                    os.remove(local_fp)\n",
    "            except Exception:\n",
    "                pass\n",
    "            raise RuntimeError(f\"Downloaded file for {path} is missing or zero bytes; check blob/permissions.\")\n",
    "    else:\n",
    "        local_fp = path\n",
    "\n",
    "    # Fast path: .npy files\n",
    "    if local_fp.lower().endswith(\".npy\"):\n",
    "        try:\n",
    "            arr = np.load(local_fp)\n",
    "            return arr.astype(\"float32\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to np.load file {local_fp}: {e}\")\n",
    "\n",
    "    # Try rasterio first, but catch and fall back\n",
    "    rasterio_err = None\n",
    "    if rasterio is not None:\n",
    "        try:\n",
    "            with rasterio.open(local_fp) as src:\n",
    "                arr = src.read(1).astype(\"float32\")\n",
    "                if src.nodata is not None:\n",
    "                    arr[arr == src.nodata] = np.nan\n",
    "                if target_h is not None and target_w is not None and arr.shape != (target_h, target_w):\n",
    "                    arr = np.array(tf.image.resize(arr[..., np.newaxis], (target_h, target_w)))[..., 0]\n",
    "                return arr.astype(\"float32\")\n",
    "        except Exception as e:\n",
    "            rasterio_err = e\n",
    "\n",
    "    # Try numpy.load in case the file is a .npy saved without .npy extension\n",
    "    try:\n",
    "        arr_try = np.load(local_fp)\n",
    "        if isinstance(arr_try, np.ndarray):\n",
    "            if arr_try.ndim == 3:\n",
    "                arr_try = arr_try[..., 0]\n",
    "            arr = arr_try.astype(\"float32\")\n",
    "            if target_h is not None and target_w is not None and arr.shape != (target_h, target_w):\n",
    "                arr = np.array(tf.image.resize(arr[..., np.newaxis], (target_h, target_w)))[..., 0]\n",
    "            print(f\"Warning: rasterio failed to open {local_fp}; loaded with numpy.load instead.\")\n",
    "            return arr.astype(\"float32\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try PIL fallback\n",
    "    if Image is not None:\n",
    "        try:\n",
    "            with Image.open(local_fp) as im:\n",
    "                im_arr = np.array(im)\n",
    "                if im_arr.ndim == 3:\n",
    "                    im_arr = im_arr[..., 0]\n",
    "                arr = im_arr.astype(\"float32\")\n",
    "                if target_h is not None and target_w is not None and arr.shape != (target_h, target_w):\n",
    "                    arr = np.array(tf.image.resize(arr[..., np.newaxis], (target_h, target_w)))[..., 0]\n",
    "                print(f\"Warning: rasterio failed to open {local_fp}; loaded with PIL.Image instead.\")\n",
    "                return arr.astype(\"float32\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Diagnostics if everything failed\n",
    "    header = b\"\"\n",
    "    try:\n",
    "        with open(local_fp, \"rb\") as fh:\n",
    "            header = fh.read(512)\n",
    "    except Exception:\n",
    "        pass\n",
    "    size = os.path.getsize(local_fp) if os.path.exists(local_fp) else None\n",
    "    hint = \"\"\n",
    "    try:\n",
    "        htxt = header.decode(\"utf-8\", errors=\"ignore\").lower()\n",
    "        if \"<html\" in htxt or \"access denied\" in htxt or \"error\" in htxt:\n",
    "            hint = \"Downloaded file looks like an HTML error page (AccessDenied/404) — check GCS path and credentials or requester-pays.\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    err_msg = f\"Failed to read image file {local_fp} with rasterio/numpy/PIL. \"\n",
    "    if rasterio_err is not None:\n",
    "        err_msg += f\"rasterio error: {rasterio_err}. \"\n",
    "    err_msg += f\"File size: {size}. Header sample (utf8): {repr(header[:200])}. {hint}\"\n",
    "    raise RuntimeError(err_msg)\n",
    "\n",
    "# -------------------- Sequence generator (threshold-before-resize) --------------------\n",
    "class ConvLSTMSequence(Sequence):\n",
    "    def __init__(self, samples_df, X_features, image_map, params, use_mask_channel=False, requester_pays_project: str | None = None, use_gcsfs: bool = False, pad_value: float = 0.0, image_binary: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.df = samples_df.reset_index(drop=True)\n",
    "        self.X_feat = X_features\n",
    "        self.image_map = image_map\n",
    "        self.dates_available = sorted(list(image_map.keys()))\n",
    "        self.time_steps = params.seq_len\n",
    "        self.img_h = params.img_h\n",
    "        self.img_w = params.img_w\n",
    "        self.channels = params.img_ch\n",
    "        self.use_env = params.use_env\n",
    "        self.bs = params.batch_size\n",
    "        self.threshold_bloom = params.threshold_bloom\n",
    "        self.min_valid_pixels = getattr(params, \"min_valid_pixels\", 0)\n",
    "        self.pos_weight = getattr(params, \"pos_weight\", 1.0)\n",
    "        self.use_mask_channel = use_mask_channel\n",
    "        self.req_project = requester_pays_project\n",
    "        self.use_gcsfs = use_gcsfs\n",
    "        self.pad_value = pad_value\n",
    "        self.image_binary = bool(image_binary)\n",
    "\n",
    "        self.entries = []\n",
    "        skipped = 0\n",
    "        for i, row in self.df.iterrows():\n",
    "            date_t = pd.to_datetime(row[\"date_t\"]).normalize()\n",
    "            history_start = date_t - pd.Timedelta(days=self.time_steps-1)\n",
    "            history_dates = [d for d in self.dates_available if history_start <= d <= date_t]\n",
    "            if not history_dates:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            date_target = pd.to_datetime(row[\"date_target\"]).normalize()\n",
    "            if date_target not in image_map:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            tgt_path = image_map[date_target]\n",
    "            try:\n",
    "                tgt = cached_load(tgt_path, self.img_h, self.img_w, requester_pays_project=self.req_project, use_gcsfs=self.use_gcsfs)\n",
    "                if tgt.ndim == 3:\n",
    "                    tgt = tgt[..., 0]\n",
    "                n_valid = int(np.sum(~np.isnan(tgt)))\n",
    "            except Exception:\n",
    "                n_valid = 0\n",
    "            if n_valid < self.min_valid_pixels:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            sample_y_bin = None\n",
    "            sample_y_raw = None\n",
    "            if \"y_bin\" in row.index:\n",
    "                try:\n",
    "                    sample_y_bin = int(row[\"y_bin\"])\n",
    "                except Exception:\n",
    "                    sample_y_bin = None\n",
    "            if \"y_raw\" in row.index:\n",
    "                sample_y_raw = row[\"y_raw\"]\n",
    "            self.entries.append({\n",
    "                \"row_idx\": i,\n",
    "                \"history_dates\": history_dates,\n",
    "                \"target_path\": tgt_path,\n",
    "                \"date_t\": date_t,\n",
    "                \"date_target\": date_target,\n",
    "                \"sample_y_bin\": sample_y_bin,\n",
    "                \"sample_y_raw\": sample_y_raw\n",
    "            })\n",
    "        if len(self.entries) == 0:\n",
    "            raise ValueError(\"No valid samples found; check image_dir, sample dates, and min_valid_pixels.\")\n",
    "        print(f\"Initialized ConvLSTMSequence with {len(self.entries)} valid entries (skipped {skipped}).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.entries)/self.bs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.entries[idx*self.bs:(idx+1)*self.bs]\n",
    "        B = len(batch)\n",
    "        env_dims = self.X_feat.shape[1] if (self.use_env and self.X_feat is not None and self.X_feat.ndim>1) else 0\n",
    "        mask_channel = 1 if self.use_mask_channel else 0\n",
    "        channels_total = self.channels + env_dims + mask_channel\n",
    "\n",
    "        Xb = np.full((B, self.time_steps, self.img_h, self.img_w, channels_total), fill_value=self.pad_value, dtype=\"float32\")\n",
    "        yb = np.full((B, self.img_h, self.img_w, 1), np.nan, dtype=\"float32\")\n",
    "        sample_weight = np.zeros((B, self.img_h, self.img_w, 1), dtype=\"float32\")\n",
    "\n",
    "        for i, entry in enumerate(batch):\n",
    "            # --- history images: load/resized images as before ---\n",
    "            history_imgs, history_valid = [], []\n",
    "            for history_date in entry[\"history_dates\"]:\n",
    "                if history_date in self.image_map:\n",
    "                    img = cached_load(self.image_map[history_date], self.img_h, self.img_w, requester_pays_project=self.req_project, use_gcsfs=self.use_gcsfs)\n",
    "                    if img.ndim == 3:\n",
    "                        img = img[..., 0]\n",
    "                    history_imgs.append(np.nan_to_num(img, nan=self.pad_value).astype(\"float32\"))\n",
    "                    history_valid.append((~np.isnan(img)).astype(\"float32\"))\n",
    "            pad_len = self.time_steps - len(history_imgs)\n",
    "            if pad_len > 0:\n",
    "                pad_img = np.full((self.img_h, self.img_w), fill_value=self.pad_value, dtype=\"float32\")\n",
    "                pad_mask = np.zeros((self.img_h, self.img_w), dtype=\"float32\")\n",
    "                history_imgs = [pad_img]*pad_len + history_imgs\n",
    "                history_valid = [pad_mask]*pad_len + history_valid\n",
    "            seq_stack = np.stack(history_imgs, axis=0)[..., np.newaxis]\n",
    "\n",
    "            # env features\n",
    "            if self.use_env and env_dims > 0:\n",
    "                env_vec = self.X_feat[entry[\"row_idx\"]]\n",
    "                env_maps = np.repeat(env_vec[np.newaxis, np.newaxis, :], self.img_h, axis=0)\n",
    "                env_maps = np.repeat(env_maps, self.img_w, axis=1)\n",
    "                env_maps = np.repeat(env_maps[np.newaxis, ...], self.time_steps, axis=0)\n",
    "                seq_stack = np.concatenate([seq_stack, env_maps], axis=-1)\n",
    "\n",
    "            # mask channel\n",
    "            if self.use_mask_channel:\n",
    "                mask_seq = np.stack(history_valid, axis=0)[..., np.newaxis]\n",
    "                seq_stack = np.concatenate([seq_stack, mask_seq], axis=-1)\n",
    "\n",
    "            Xb[i] = seq_stack\n",
    "\n",
    "            # --- TARGET: load raw (no resize), threshold at native resolution, then nearest-neighbor resize ---\n",
    "            try:\n",
    "                tgt_raw = cached_load(entry[\"target_path\"], None, None, requester_pays_project=self.req_project, use_gcsfs=self.use_gcsfs)\n",
    "            except Exception:\n",
    "                tgt_raw = None\n",
    "\n",
    "            if tgt_raw is None:\n",
    "                y_img = np.zeros((self.img_h, self.img_w), dtype=\"float32\")\n",
    "                valid_mask_resized = np.zeros((self.img_h, self.img_w), dtype=\"float32\")\n",
    "            else:\n",
    "                if tgt_raw.ndim == 3:\n",
    "                    tgt_raw = tgt_raw[..., 0]\n",
    "                valid_mask_raw = ~np.isnan(tgt_raw)\n",
    "                if valid_mask_raw.sum() == 0:\n",
    "                    y_img = np.zeros((self.img_h, self.img_w), dtype=\"float32\")\n",
    "                    valid_mask_resized = np.zeros((self.img_h, self.img_w), dtype=\"float32\")\n",
    "                else:\n",
    "                    if self.image_binary:\n",
    "                        tgtf = tgt_raw.astype(\"float32\")\n",
    "                        y_mask_raw = valid_mask_raw & ((np.isclose(tgtf, 1.0)) | (np.isclose(tgtf, 255.0)) | (tgtf != 0.0))\n",
    "                    else:\n",
    "                        thr = float(self.threshold_bloom)\n",
    "                        y_mask_raw = valid_mask_raw & (tgt_raw.astype(\"float32\") >= thr)\n",
    "\n",
    "                    # resize boolean mask and valid mask to model resolution using nearest neighbor\n",
    "                    import tensorflow as _tf\n",
    "                    y_mask_raw_f = y_mask_raw.astype(\"float32\")[..., np.newaxis]\n",
    "                    valid_mask_raw_f = valid_mask_raw.astype(\"float32\")[..., np.newaxis]\n",
    "                    try:\n",
    "                        y_mask_resized = _tf.image.resize(y_mask_raw_f, (self.img_h, self.img_w), method=\"nearest\").numpy()[..., 0]\n",
    "                        valid_mask_resized = _tf.image.resize(valid_mask_raw_f, (self.img_h, self.img_w), method=\"nearest\").numpy()[..., 0] > 0.5\n",
    "                    except Exception:\n",
    "                        rh, rw = tgt_raw.shape\n",
    "                        ys = (np.linspace(0, rh-1, self.img_h)).round().astype(int)\n",
    "                        xs = (np.linspace(0, rw-1, self.img_w)).round().astype(int)\n",
    "                        mask_nn = y_mask_raw[np.ix_(ys, xs)]\n",
    "                        valid_nn = valid_mask_raw[np.ix_(ys, xs)]\n",
    "                        y_mask_resized = mask_nn.astype(\"float32\")\n",
    "                        valid_mask_resized = valid_nn.astype(\"bool\")\n",
    "\n",
    "                    y_img = np.where(y_mask_resized > 0.5, 1.0, 0.0).astype(\"float32\")\n",
    "\n",
    "            yb[i, ..., 0] = y_img\n",
    "            sw = valid_mask_resized.astype(\"float32\")\n",
    "            if self.pos_weight is not None and self.pos_weight != 1.0:\n",
    "                pos_mask = (y_img == 1.0).astype(\"float32\")\n",
    "                sw = sw + pos_mask * (self.pos_weight - 1.0)\n",
    "            sample_weight[i, ..., 0] = sw\n",
    "\n",
    "        return Xb, yb, sample_weight\n",
    "\n",
    "    def on_epoch_end(self): return\n",
    "\n",
    "# -------------------- NPZ and CSV loaders (support gs://) --------------------\n",
    "def load_npz(npz_path: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> dict:\n",
    "    if is_gcs_path(npz_path):\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix=\".npz\", delete=False)\n",
    "        tmp.close()\n",
    "        fetch_gcs_to_local(npz_path, tmp.name, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "        data = np.load(tmp.name, allow_pickle=True)\n",
    "        result = {k: data[k] for k in data.files}\n",
    "        try:\n",
    "            os.unlink(tmp.name)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return result\n",
    "    else:\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        return {k: data[k] for k in data.files}\n",
    "\n",
    "def _samples_files_exist(prefix: str, requester_pays_project: str | None = None, use_gcsfs: bool = False) -> bool:\n",
    "    try:\n",
    "        for name in (\"samples_train.csv\", \"samples_val.csv\", \"samples_test.csv\"):\n",
    "            if is_gcs_path(prefix):\n",
    "                if storage is not None and not use_gcsfs:\n",
    "                    client = _make_storage_client(project=requester_pays_project)\n",
    "                    _, rest = prefix.split(\"gs://\", 1)\n",
    "                    bucket_name, _, prefix_path = rest.partition(\"/\")\n",
    "                    prefix_path = prefix_path.rstrip('/') + '/'\n",
    "                    blob = client.bucket(bucket_name).blob(prefix_path + name)\n",
    "                    if not blob.exists():\n",
    "                        return False\n",
    "                else:\n",
    "                    fs = _make_gcsfs(project=requester_pays_project)\n",
    "                    target = prefix.rstrip('/') + '/' + name\n",
    "                    if not fs.exists(target):\n",
    "                        return False\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(prefix, name)):\n",
    "                    return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_samples_csvs(samples_dir: str, requester_pays_project: str | None = None, use_gcsfs: bool = False):\n",
    "    def load_one(path_or_dir, name):\n",
    "        if is_gcs_path(str(path_or_dir)):\n",
    "            if storage is not None and not use_gcsfs:\n",
    "                client = _make_storage_client(project=requester_pays_project)\n",
    "                _, rest = str(path_or_dir).split(\"gs://\", 1)\n",
    "                bucket_name, _, prefix = rest.partition(\"/\")\n",
    "                prefix = prefix.rstrip('/') + '/'\n",
    "                blob_path = prefix + name\n",
    "                bucket = client.lookup_bucket(bucket_name)\n",
    "                if bucket is None:\n",
    "                    raise FileNotFoundError(f\"Bucket not found: gs://{bucket_name}\")\n",
    "                blob = bucket.blob(blob_path)\n",
    "                if not blob.exists():\n",
    "                    raise FileNotFoundError(f\"Samples CSV not found: gs://{bucket_name}/{blob_path}\")\n",
    "                data = blob.download_as_bytes()\n",
    "                return pd.read_csv(io.BytesIO(data), parse_dates=['date_t', 'date_target'])\n",
    "            fs = _make_gcsfs(project=requester_pays_project)\n",
    "            target = str(path_or_dir).rstrip('/') + '/' + name\n",
    "            with fs.open(target, \"rb\") as f:\n",
    "                return pd.read_csv(f, parse_dates=['date_t', 'date_target'])\n",
    "        else:\n",
    "            p = Path(path_or_dir) / name\n",
    "            return pd.read_csv(str(p), parse_dates=['date_t', 'date_target'])\n",
    "    return load_one(samples_dir, \"samples_train.csv\"), load_one(samples_dir, \"samples_val.csv\"), load_one(samples_dir, \"samples_test.csv\")\n",
    "\n",
    "# -------------------- Focal loss --------------------\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n",
    "        mod = tf.pow(1.0 - p_t, gamma)\n",
    "        alpha_factor = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n",
    "        loss = alpha_factor * mod * bce\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss_fn\n",
    "\n",
    "# -------------------- Custom callback & ValStats (unchanged except added metrics write and tag) --------------------\n",
    "class CustomValMetrics(callbacks.Callback):\n",
    "    def __init__(self, val_seq: Sequence, local_best_path: str, local_work: str, agg_percentile: float = 90.0, requester_pays_project: str | None = None, upload_fn=None, tag: str = \"\"):\n",
    "        super().__init__()\n",
    "        self.val_seq = val_seq\n",
    "        self.best_score = -np.inf\n",
    "        self.local_best_path = local_best_path\n",
    "        self.local_work = local_work\n",
    "        self.agg_percentile = float(agg_percentile)\n",
    "        self.requester_pays_project = requester_pays_project\n",
    "        self.upload_fn = upload_fn\n",
    "        # tag to append to epoch artifacts (e.g., \"pig\")\n",
    "        self.tag = tag if (tag is not None and len(str(tag))>0) else \"\"\n",
    "\n",
    "    def _tagged_name(self, base: str):\n",
    "        if not self.tag:\n",
    "            return base\n",
    "        base_noext, ext = os.path.splitext(base)\n",
    "        return f\"{base_noext}_{self.tag}{ext}\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pix_trues = []\n",
    "        pix_probs = []\n",
    "        sample_trues = []\n",
    "        sample_probs_mean = []\n",
    "        sample_probs_p = []\n",
    "        sample_rows = []\n",
    "\n",
    "        for b_idx in range(len(self.val_seq)):\n",
    "            Xb, yb, sw = self.val_seq[b_idx]\n",
    "            probs = self.model.predict(Xb, verbose=0)\n",
    "            for j in range(Xb.shape[0]):\n",
    "                entry_idx = b_idx * self.val_seq.bs + j\n",
    "                if entry_idx >= len(self.val_seq.entries):\n",
    "                    continue\n",
    "                row_idx = self.val_seq.entries[entry_idx][\"row_idx\"]\n",
    "                mask = (sw[j, ..., 0] > 0)\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                pix_trues.append(yb[j, ..., 0][mask].reshape(-1))\n",
    "                pix_probs.append(probs[j, ..., 0][mask].reshape(-1))\n",
    "                vals = probs[j, ..., 0][mask]\n",
    "                mean_prob = float(np.nanmean(vals))\n",
    "                try:\n",
    "                    p_val = float(np.nanpercentile(vals, self.agg_percentile))\n",
    "                except Exception:\n",
    "                    p_val = mean_prob\n",
    "                sample_label = None\n",
    "                try:\n",
    "                    sample_label = int(self.val_seq.df.iloc[row_idx]['y_bin'])\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        sample_label = int(self.val_seq.entries[entry_idx].get(\"sample_y_bin\"))\n",
    "                    except Exception:\n",
    "                        sample_label = None\n",
    "                if sample_label is not None:\n",
    "                    sample_trues.append(sample_label)\n",
    "                    sample_probs_mean.append(mean_prob)\n",
    "                    sample_probs_p.append(p_val)\n",
    "                    sample_rows.append({\n",
    "                        \"row_idx\": int(row_idx),\n",
    "                        \"date_t\": str(self.val_seq.entries[entry_idx][\"date_t\"]),\n",
    "                        \"date_target\": str(self.val_seq.entries[entry_idx][\"date_target\"]),\n",
    "                        \"mean_prob\": mean_prob,\n",
    "                        f\"p{int(self.agg_percentile)}_prob\": p_val,\n",
    "                        \"label\": sample_label\n",
    "                    })\n",
    "\n",
    "        if len(pix_trues) > 0:\n",
    "            y_pix = np.concatenate(pix_trues).ravel()\n",
    "            p_pix = np.concatenate(pix_probs).ravel()\n",
    "        else:\n",
    "            y_pix = np.array([])\n",
    "            p_pix = np.array([])\n",
    "\n",
    "        pix_ap = pix_roc = None\n",
    "        if y_pix.size > 0 and average_precision_score is not None:\n",
    "            try:\n",
    "                pix_ap = float(average_precision_score(y_pix, p_pix))\n",
    "            except Exception:\n",
    "                pix_ap = None\n",
    "        if y_pix.size > 0 and roc_auc_score is not None:\n",
    "            try:\n",
    "                pix_roc = float(roc_auc_score(y_pix, p_pix))\n",
    "            except Exception:\n",
    "                pix_roc = None\n",
    "\n",
    "        samp_ap_mean = samp_roc_mean = samp_ap_p = samp_roc_p = None\n",
    "        if len(sample_trues) > 0 and average_precision_score is not None:\n",
    "            try:\n",
    "                samp_ap_mean = float(average_precision_score(sample_trues, sample_probs_mean))\n",
    "            except Exception:\n",
    "                samp_ap_mean = None\n",
    "            try:\n",
    "                samp_ap_p = float(average_precision_score(sample_trues, sample_probs_p))\n",
    "            except Exception:\n",
    "                samp_ap_p = None\n",
    "        if len(sample_trues) > 0 and roc_auc_score is not None:\n",
    "            try:\n",
    "                if len(set(sample_trues)) > 1:\n",
    "                    samp_roc_mean = float(roc_auc_score(sample_trues, sample_probs_mean))\n",
    "                else:\n",
    "                    samp_roc_mean = None\n",
    "            except Exception:\n",
    "                samp_roc_mean = None\n",
    "            try:\n",
    "                if len(set(sample_trues)) > 1:\n",
    "                    samp_roc_p = float(roc_auc_score(sample_trues, sample_probs_p))\n",
    "                else:\n",
    "                    samp_roc_p = None\n",
    "            except Exception:\n",
    "                samp_roc_p = None\n",
    "\n",
    "        n_pix = int(y_pix.size)\n",
    "        n_pix_pos = int(np.sum(y_pix == 1)) if n_pix > 0 else 0\n",
    "        n_samp = len(sample_trues)\n",
    "        n_samp_pos = int(np.sum(np.array(sample_trues) == 1)) if n_samp > 0 else 0\n",
    "\n",
    "        print(f\"Epoch {epoch+1} metrics: pix_ap={pix_ap}, pix_roc={pix_roc} (n_pix={n_pix}, n_pix_pos={n_pix_pos}); \"\n",
    "              f\"samp_ap_mean={samp_ap_mean}, samp_ap_p{int(self.agg_percentile)}={samp_ap_p}, samp_roc_mean={samp_roc_mean}, samp_roc_p{int(self.agg_percentile)}={samp_roc_p} \"\n",
    "              f\"(n_samp={n_samp}, n_samp_pos={n_samp_pos})\")\n",
    "\n",
    "        # Save sample-level preds CSV for this epoch (existing behaviour) but tagged\n",
    "        if len(sample_rows) > 0:\n",
    "            preds_df = pd.DataFrame(sample_rows)\n",
    "            epoch_preds_base = f\"val_sample_preds_epoch{epoch+1}.csv\"\n",
    "            epoch_preds_local = os.path.join(self.local_work, self._tagged_name(epoch_preds_base))\n",
    "            preds_df.to_csv(epoch_preds_local, index=False)\n",
    "        else:\n",
    "            epoch_preds_local = None\n",
    "\n",
    "        # --- NEW: compute and save validation metrics JSON for this epoch (tagged) ---\n",
    "        try:\n",
    "            val_metrics = {}\n",
    "            # pixel metrics (if present)\n",
    "            if y_pix.size > 0:\n",
    "                val_metrics[\"pixel\"] = compute_sample_metrics_from_arrays(y_pix, p_pix)\n",
    "            else:\n",
    "                val_metrics[\"pixel\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "            # sample metrics - mean & percentile aggregation\n",
    "            if len(sample_trues) > 0:\n",
    "                val_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(np.array(sample_trues, dtype=int), np.array(sample_probs_mean, dtype=float))\n",
    "                val_metrics[\"sample_p\"] = compute_sample_metrics_from_arrays(np.array(sample_trues, dtype=int), np.array(sample_probs_p, dtype=float))\n",
    "            else:\n",
    "                val_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                val_metrics[\"sample_p\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "\n",
    "            val_metrics[\"n_pix\"] = int(n_pix)\n",
    "            val_metrics[\"n_pix_pos\"] = int(n_pix_pos)\n",
    "            val_metrics[\"n_samp\"] = int(n_samp)\n",
    "            val_metrics[\"n_samp_pos\"] = int(n_samp_pos)\n",
    "\n",
    "            val_metrics_base = f\"val_metrics_epoch{epoch+1}.json\"\n",
    "            val_metrics_local = os.path.join(self.local_work, self._tagged_name(val_metrics_base))\n",
    "            with open(val_metrics_local, \"w\") as fh:\n",
    "                json.dump(val_metrics, fh, indent=2)\n",
    "            # attempt upload via upload_fn\n",
    "            if self.upload_fn is not None and callable(self.upload_fn):\n",
    "                try:\n",
    "                    self.upload_fn(val_metrics_local)\n",
    "                except Exception as e:\n",
    "                    print(\"Upload of val metrics JSON failed:\", e)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to compute/save validation metrics JSON for epoch\", epoch+1, \":\", e)\n",
    "\n",
    "        sel_score = samp_ap_p if (samp_ap_p is not None) else (samp_ap_mean if (samp_ap_mean is not None) else (pix_ap if (pix_ap is not None) else float(\"-inf\")))\n",
    "\n",
    "        if sel_score > self.best_score:\n",
    "            print(f\"New best aggregated selection score: {sel_score} (previous {self.best_score}); saving model to {self.local_best_path}\")\n",
    "            self.best_score = sel_score\n",
    "            try:\n",
    "                self.model.save(self.local_best_path)\n",
    "                if self.upload_fn is not None and callable(self.upload_fn):\n",
    "                    try:\n",
    "                        self.upload_fn(self.local_best_path)\n",
    "                    except Exception as e:\n",
    "                        print(\"Upload of best model failed:\", e)\n",
    "                    if epoch_preds_local is not None:\n",
    "                        try:\n",
    "                            self.upload_fn(epoch_preds_local)\n",
    "                        except Exception as e:\n",
    "                            print(\"Upload of best preds CSV failed:\", e)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to save best model locally:\", e)\n",
    "\n",
    "class ValStats(callbacks.Callback):\n",
    "    def __init__(self, val_seq: Sequence):\n",
    "        super().__init__()\n",
    "        self.val_seq = val_seq\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        n_samp_valid = 0\n",
    "        n_pos_pix = 0\n",
    "        for i in range(len(self.val_seq)):\n",
    "            _, yb, sw = self.val_seq[i]\n",
    "            pos_pix = (yb[..., 0] == 1) & (sw[..., 0] > 0)\n",
    "            n_pos_pix += int(np.sum(pos_pix))\n",
    "            n_samp_valid += int(np.sum(np.sum(sw[..., 0], axis=(1,2)) > 0))\n",
    "        print(f\"Epoch {epoch+1} start: validation valid_samples={n_samp_valid}, total_val_pos_pixels={n_pos_pix}\")\n",
    "        if n_pos_pix == 0:\n",
    "            print(\"WARNING: Validation set contains ZERO positive pixels. This will make ROC/AUC undefined.\")\n",
    "            print(\" - Check sample CSVs and processed .npz (y_bin), or use --image_binary if target files are already binary.\")\n",
    "            print(\" - Consider lowering --threshold or enabling --image_binary if appropriate.\")\n",
    "\n",
    "# -------------------- Model builder (same as before) --------------------\n",
    "def build_convlstm_model(H, W, channels, seq_len=14, use_attention=True, lr=1e-4, clipnorm: float = 0.0, lstm_filters=32):\n",
    "    inp = layers.Input(shape=(seq_len, H, W, channels), name=\"convlstm_input\")\n",
    "    x = layers.ConvLSTM2D(filters=lstm_filters, kernel_size=(3,3), padding=\"same\",\n",
    "                          return_sequences=False, activation=\"relu\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if use_attention:\n",
    "        filters = int(x.shape[-1])\n",
    "        se = layers.GlobalAveragePooling2D()(x)\n",
    "        se = layers.Dense(max(4, filters//8), activation=\"relu\")(se)\n",
    "        se = layers.Dense(filters, activation=\"sigmoid\")(se)\n",
    "        se = layers.Reshape((1,1,filters))(se)\n",
    "        x = layers.multiply([x, se])\n",
    "    x = layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\", dtype=\"float32\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "    return model\n",
    "\n",
    "# -------------------- Main (unchanged except metrics write and tagging at end) --------------------\n",
    "def main(args):\n",
    "    local_work = tempfile.mkdtemp(prefix=\"convlstm_work_\")\n",
    "    try:\n",
    "        tag = getattr(args, \"tag\", \"\")\n",
    "        # small helper to tag filenames consistently\n",
    "        def tag_name(base_filename: str) -> str:\n",
    "            if not tag:\n",
    "                return base_filename\n",
    "            base_noext, ext = os.path.splitext(base_filename)\n",
    "            return f\"{base_noext}_{tag}{ext}\"\n",
    "\n",
    "        class P: pass\n",
    "        P.seq_len = args.seq_len\n",
    "        P.img_h = args.img_h\n",
    "        P.img_w = args.img_w\n",
    "        P.img_ch = args.img_ch\n",
    "        P.batch_size = args.batch_size\n",
    "        P.use_env = args.use_env\n",
    "        P.threshold_bloom = args.threshold\n",
    "        P.min_valid_pixels = args.min_valid_pixels\n",
    "        P.pos_weight = args.pos_weight if args.pos_weight is not None else 1.0\n",
    "\n",
    "        if getattr(args, \"pos_weight_override\", None) is not None:\n",
    "            P.pos_weight = float(args.pos_weight_override)\n",
    "\n",
    "        requester_pays_project = getattr(args, \"requester_pays_project\", None)\n",
    "        use_gcsfs = getattr(args, \"use_gcsfs\", False)\n",
    "\n",
    "        print(\"Loading processed arrays from:\", args.processed_npz)\n",
    "        data = {}\n",
    "        try:\n",
    "            data = load_npz(args.processed_npz, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs) or {}\n",
    "        except Exception as e:\n",
    "            print(\"Warning: failed to load processed npz:\", e)\n",
    "            data = {}\n",
    "\n",
    "        X_train = data.get(\"X_train\")\n",
    "        X_val = data.get(\"X_val\")\n",
    "        X_test = data.get(\"X_test\")\n",
    "\n",
    "        npz_has_ybin = all(k in data for k in (\"y_train_bin\", \"y_val_bin\", \"y_test_bin\"))\n",
    "        if npz_has_ybin:\n",
    "            print(\"processed .npz contains binary targets (y_*_bin) — will use them for pos_weight and diagnostics.\")\n",
    "            y_train_bin_npz = np.asarray(data[\"y_train_bin\"])\n",
    "            y_val_bin_npz = np.asarray(data[\"y_val_bin\"])\n",
    "            y_test_bin_npz = np.asarray(data[\"y_test_bin\"])\n",
    "        else:\n",
    "            y_train_bin_npz = y_val_bin_npz = y_test_bin_npz = None\n",
    "\n",
    "        samples_dir_to_use = args.samples_dir\n",
    "        if not _samples_files_exist(samples_dir_to_use, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs):\n",
    "            if is_gcs_path(args.processed_npz):\n",
    "                parent_prefix = os.path.dirname(args.processed_npz)\n",
    "            else:\n",
    "                parent_prefix = os.path.dirname(os.path.abspath(args.processed_npz))\n",
    "            if _samples_files_exist(parent_prefix, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs):\n",
    "                print(f\"Warning: samples not found under {samples_dir_to_use}; falling back to {parent_prefix}\")\n",
    "                samples_dir_to_use = parent_prefix\n",
    "\n",
    "        print(\"Loading sample CSVs from:\", samples_dir_to_use)\n",
    "        train_df, val_df, test_df = load_samples_csvs(samples_dir_to_use, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "\n",
    "        print(\"Listing image files in:\", args.image_dir)\n",
    "        image_map = list_image_files(args.image_dir, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs)\n",
    "        if len(image_map) == 0:\n",
    "            raise RuntimeError(\"No image files found under image_dir.\")\n",
    "\n",
    "        # compute pos_weight: prefer npz y_train_bin if present, else use train_df\n",
    "        if getattr(args, \"compute_pos_weight\", False):\n",
    "            computed_pw = None\n",
    "            if npz_has_ybin:\n",
    "                pos = int((y_train_bin_npz == 1).sum())\n",
    "                neg = int((y_train_bin_npz == 0).sum())\n",
    "                if pos > 0:\n",
    "                    computed_pw = float(neg / pos)\n",
    "            elif \"y_bin\" in train_df.columns:\n",
    "                pos = int((train_df[\"y_bin\"] == 1).sum())\n",
    "                neg = int((train_df[\"y_bin\"] == 0).sum())\n",
    "                if pos > 0:\n",
    "                    computed_pw = float(neg / pos)\n",
    "            if computed_pw is not None:\n",
    "                P.pos_weight = computed_pw\n",
    "        print(\"Using pos_weight =\", P.pos_weight)\n",
    "\n",
    "        train_seq = ConvLSTMSequence(train_df, X_train if X_train is not None else np.zeros((len(train_df),1)), image_map, P, use_mask_channel=args.use_mask_channel, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs, pad_value=args.pad_value, image_binary=args.image_binary)\n",
    "        val_seq = ConvLSTMSequence(val_df, X_val if X_val is not None else np.zeros((len(val_df),1)), image_map, P, use_mask_channel=args.use_mask_channel, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs, pad_value=args.pad_value, image_binary=args.image_binary)\n",
    "        test_seq = ConvLSTMSequence(test_df, X_test if X_test is not None else np.zeros((len(test_df),1)), image_map, P, use_mask_channel=args.use_mask_channel, requester_pays_project=requester_pays_project, use_gcsfs=use_gcsfs, pad_value=args.pad_value, image_binary=args.image_binary)\n",
    "\n",
    "        channels_total = args.img_ch + (X_train.shape[1] if (X_train is not None and args.use_env and X_train.ndim > 1) else 0) + (1 if args.use_mask_channel else 0)\n",
    "        print(\"Channels total:\", channels_total)\n",
    "\n",
    "        model = build_convlstm_model(P.img_h, P.img_w, channels_total, seq_len=P.seq_len, use_attention=args.use_attention, lr=args.lr, clipnorm=args.clipnorm)\n",
    "\n",
    "        from tensorflow.keras.optimizers import legacy as optimizers_legacy\n",
    "        base_opt = optimizers_legacy.Adam(learning_rate=args.lr)\n",
    "        policy = mixed_precision.global_policy()\n",
    "        if policy.name == \"mixed_float16\":\n",
    "            opt = mixed_precision.LossScaleOptimizer(base_opt)\n",
    "            print(\"Using LossScaleOptimizer for mixed precision (legacy Adam).\")\n",
    "        else:\n",
    "            opt = base_opt\n",
    "\n",
    "        if getattr(args, \"use_focal_loss\", False):\n",
    "            loss_fn = focal_loss(gamma=2.0, alpha=0.25)\n",
    "            print(\"Using focal loss (gamma=2.0, alpha=0.25).\")\n",
    "        else:\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        model.compile(optimizer=opt, loss=loss_fn, metrics=[tf.keras.metrics.BinaryAccuracy(name=\"acc\")])\n",
    "        model.summary()\n",
    "\n",
    "        # Tag filenames early so callbacks and checkpointing use consistent names\n",
    "        local_ckpt = os.path.join(local_work, tag_name(f\"best_model.keras\"))\n",
    "        local_best_agg = os.path.join(local_work, tag_name(f\"best_model_agg.keras\"))\n",
    "        csvlog = os.path.join(local_work, tag_name(f\"history.csv\"))\n",
    "        tb_logdir = os.path.join(local_work, tag_name(f\"tensorboard\"))\n",
    "        os.makedirs(tb_logdir, exist_ok=True)\n",
    "\n",
    "        def _upload_to_out(local_path):\n",
    "            try:\n",
    "                dst = gcs_join(args.out_dir, os.path.basename(local_path))\n",
    "                upload_file_to_gcs(local_path, dst, requester_pays_project=requester_pays_project)\n",
    "                print(\"Uploaded\", local_path, \"->\", dst)\n",
    "            except Exception as e:\n",
    "                print(\"Upload failed for\", local_path, \":\", e)\n",
    "\n",
    "        cbs = [\n",
    "            callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=args.patience, restore_best_weights=True),\n",
    "            callbacks.ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.5, patience=max(3, args.patience // 3), min_lr=1e-6, verbose=1),\n",
    "            callbacks.ModelCheckpoint(local_ckpt, monitor=\"val_loss\", mode=\"min\", save_best_only=True),\n",
    "            callbacks.CSVLogger(csvlog),\n",
    "            callbacks.TensorBoard(log_dir=tb_logdir),\n",
    "            ValStats(val_seq=val_seq),\n",
    "            CustomValMetrics(val_seq=val_seq, local_best_path=local_best_agg, local_work=local_work, agg_percentile=args.agg_percentile, requester_pays_project=requester_pays_project, upload_fn=_upload_to_out, tag=tag)\n",
    "        ]\n",
    "\n",
    "        fit_kwargs = {}\n",
    "        if args.workers > 0:\n",
    "            fit_kwargs[\"workers\"] = args.workers\n",
    "            fit_kwargs[\"use_multiprocessing\"] = args.use_multiprocessing\n",
    "\n",
    "        history = model.fit(train_seq, validation_data=val_seq, epochs=args.epochs, callbacks=cbs, verbose=1, **fit_kwargs)\n",
    "        local_final = os.path.join(local_work, tag_name(\"final_model.keras\"))\n",
    "        model.save(local_final)\n",
    "\n",
    "        # --- Test set predictions and metrics collection (no model rerun) ---\n",
    "        preds = []\n",
    "        # containers for pixel-level metrics accumulation\n",
    "        pix_trues = []\n",
    "        pix_probs = []\n",
    "        # containers for sample-level aggregation (mean and percentile)\n",
    "        sample_trues = []\n",
    "        sample_probs_mean = []\n",
    "        sample_probs_p = []\n",
    "        sample_rows = []\n",
    "\n",
    "        for i in range(len(test_seq)):\n",
    "            Xb, yb, sw = test_seq[i]\n",
    "            yprob = model.predict(Xb, verbose=0)\n",
    "            for j in range(Xb.shape[0]):\n",
    "                entry_idx = i * test_seq.bs + j\n",
    "                if entry_idx >= len(test_seq.entries):\n",
    "                    continue\n",
    "                row_idx = test_seq.entries[entry_idx][\"row_idx\"]\n",
    "                date_t = test_seq.entries[entry_idx][\"date_t\"]\n",
    "                date_target = test_seq.entries[entry_idx][\"date_target\"]\n",
    "                # valid mask: use sample_weight > 0 (consistent with validation callback)\n",
    "                valid_mask = (sw[j, ..., 0] > 0)\n",
    "                if valid_mask.sum() > 0:\n",
    "                    vals = yprob[j, ..., 0][valid_mask]\n",
    "                    # pixel-level accumulation\n",
    "                    pix_trues.append(yb[j, ..., 0][valid_mask].reshape(-1))\n",
    "                    pix_probs.append(vals.reshape(-1))\n",
    "                    # sample-level aggregated metrics\n",
    "                    mean_prob = float(np.nanmean(vals))\n",
    "                    try:\n",
    "                        p_val = float(np.nanpercentile(vals, args.agg_percentile))\n",
    "                    except Exception:\n",
    "                        p_val = mean_prob\n",
    "                else:\n",
    "                    mean_prob = float(\"nan\")\n",
    "                    p_val = float(\"nan\")\n",
    "                preds.append({\"row_idx\": int(row_idx), \"date_t\": str(date_t), \"date_target\": str(date_target), \"mean_prob\": mean_prob})\n",
    "                # sample label\n",
    "                sample_label = None\n",
    "                try:\n",
    "                    sample_label = int(test_df.iloc[row_idx]['y_bin'])\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        sample_label = int(test_seq.entries[entry_idx].get(\"sample_y_bin\"))\n",
    "                    except Exception:\n",
    "                        sample_label = None\n",
    "                if sample_label is not None:\n",
    "                    sample_trues.append(sample_label)\n",
    "                    sample_probs_mean.append(mean_prob)\n",
    "                    sample_probs_p.append(p_val)\n",
    "                    sample_rows.append({\n",
    "                        \"row_idx\": int(row_idx),\n",
    "                        \"date_t\": str(date_t),\n",
    "                        \"date_target\": str(date_target),\n",
    "                        \"mean_prob\": mean_prob,\n",
    "                        f\"p{int(args.agg_percentile)}_prob\": p_val,\n",
    "                        \"label\": sample_label\n",
    "                    })\n",
    "\n",
    "        preds_df = pd.DataFrame(preds)\n",
    "        preds_local = os.path.join(local_work, tag_name(\"predictions_test_convlstm.csv\"))\n",
    "        preds_df.to_csv(preds_local, index=False)\n",
    "\n",
    "        # --- NEW: compute and save final test metrics JSON from preds_df + test_df (tagged) ---\n",
    "        try:\n",
    "            # Pixel-level metrics\n",
    "            test_metrics = {}\n",
    "            if len(pix_trues) > 0:\n",
    "                y_pix = np.concatenate(pix_trues).ravel()\n",
    "                p_pix = np.concatenate(pix_probs).ravel()\n",
    "                test_metrics[\"pixel\"] = compute_sample_metrics_from_arrays(y_pix, p_pix)\n",
    "            else:\n",
    "                test_metrics[\"pixel\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "\n",
    "            # Sample-level metrics from aggregated per-sample values (mean and percentile)\n",
    "            if len(sample_trues) > 0:\n",
    "                test_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(np.array(sample_trues, dtype=int), np.array(sample_probs_mean, dtype=float))\n",
    "                test_metrics[\"sample_p\"] = compute_sample_metrics_from_arrays(np.array(sample_trues, dtype=int), np.array(sample_probs_p, dtype=float))\n",
    "            else:\n",
    "                # fallback: try merging preds_df with test_df using row_idx (useful if you didn't aggregate)\n",
    "                merged_test = None\n",
    "                try:\n",
    "                    if \"row_idx\" in preds_df.columns:\n",
    "                        test_idx = test_df.reset_index(drop=True).reset_index().rename(columns={\"index\": \"row_idx\"})\n",
    "                        merged_test = pd.merge(preds_df, test_idx, on=\"row_idx\", how=\"left\", suffixes=(\"\", \"_samp\"))\n",
    "                    else:\n",
    "                        # fallback to date columns join\n",
    "                        join_keys = []\n",
    "                        if (\"date_t\" in preds_df.columns) and (\"date_t\" in test_df.columns):\n",
    "                            join_keys.append(\"date_t\")\n",
    "                        if (\"date_target\" in preds_df.columns) and (\"date_target\" in test_df.columns):\n",
    "                            join_keys.append(\"date_target\")\n",
    "                        if len(join_keys) > 0:\n",
    "                            merged_test = pd.merge(preds_df, test_df, on=join_keys, how=\"left\", suffixes=(\"\", \"_samp\"))\n",
    "                    if merged_test is not None:\n",
    "                        if \"mean_prob\" in merged_test.columns and \"y_bin\" in merged_test.columns:\n",
    "                            merged_sub = merged_test[[\"mean_prob\", \"y_bin\"]].dropna().rename(columns={\"mean_prob\": \"prob\", \"y_bin\": \"label\"})\n",
    "                            if not merged_sub.empty:\n",
    "                                y_samp = merged_sub[\"label\"].astype(int).values\n",
    "                                y_score = merged_sub[\"prob\"].astype(float).values\n",
    "                                test_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(y_samp, y_score)\n",
    "                                # no percentile column available in this path; duplicate mean results\n",
    "                                test_metrics[\"sample_p\"] = test_metrics[\"sample_mean\"]\n",
    "                            else:\n",
    "                                test_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                                test_metrics[\"sample_p\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                        else:\n",
    "                            test_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                            test_metrics[\"sample_p\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                    else:\n",
    "                        test_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                        test_metrics[\"sample_p\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                except Exception:\n",
    "                    test_metrics[\"sample_mean\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "                    test_metrics[\"sample_p\"] = compute_sample_metrics_from_arrays(np.array([]), np.array([]))\n",
    "\n",
    "            # counts\n",
    "            test_metrics[\"n_pix\"] = int(np.sum([len(x) for x in pix_trues])) if len(pix_trues) > 0 else 0\n",
    "            test_metrics[\"n_pix_pos\"] = int(np.sum(np.concatenate(pix_trues) == 1)) if len(pix_trues) > 0 else 0\n",
    "            test_metrics[\"n_samp\"] = int(len(sample_trues))\n",
    "            test_metrics[\"n_samp_pos\"] = int(np.sum(np.array(sample_trues) == 1)) if len(sample_trues) > 0 else 0\n",
    "\n",
    "            metrics_local = os.path.join(local_work, tag_name(\"test_metrics.json\"))\n",
    "            with open(metrics_local, \"w\") as fh:\n",
    "                json.dump(test_metrics, fh, indent=2)\n",
    "            print(\"Test metrics:\")\n",
    "            print(json.dumps(test_metrics, indent=2))\n",
    "            # upload metrics JSON\n",
    "            try:\n",
    "                _upload_to_out(metrics_local)\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(\"Failed to compute/save test metrics JSON:\", e)\n",
    "\n",
    "        print(\"Uploading artifacts to:\", args.out_dir)\n",
    "        if is_gcs_path(args.out_dir):\n",
    "            try:\n",
    "                upload_file_to_gcs(local_final, gcs_join(args.out_dir, os.path.basename(local_final)), requester_pays_project=requester_pays_project)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to upload final model:\", e)\n",
    "            try:\n",
    "                upload_file_to_gcs(local_ckpt, gcs_join(args.out_dir, os.path.basename(local_ckpt)), requester_pays_project=requester_pays_project)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to upload ckpt:\", e)\n",
    "            try:\n",
    "                upload_file_to_gcs(preds_local, gcs_join(args.out_dir, os.path.basename(preds_local)), requester_pays_project=requester_pays_project)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to upload preds csv:\", e)\n",
    "            try:\n",
    "                upload_file_to_gcs(metrics_local, gcs_join(args.out_dir, os.path.basename(metrics_local)), requester_pays_project=requester_pays_project)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to upload metrics json:\", e)\n",
    "            try:\n",
    "                upload_file_to_gcs(csvlog, gcs_join(args.out_dir, os.path.basename(csvlog)), requester_pays_project=requester_pays_project)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to upload csvlog:\", e)\n",
    "            try:\n",
    "                upload_dir_to_gcs(tb_logdir, gcs_join(args.out_dir, \"tensorboard\"), requester_pays_project=requester_pays_project)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to upload tensorboard logs:\", e)\n",
    "        else:\n",
    "            try:\n",
    "                os.makedirs(args.out_dir, exist_ok=True)\n",
    "                shutil.move(local_final, os.path.join(args.out_dir, os.path.basename(local_final)))\n",
    "            except Exception as e:\n",
    "                print(\"Failed to move final model locally:\", e)\n",
    "            try:\n",
    "                shutil.move(local_ckpt, os.path.join(args.out_dir, os.path.basename(local_ckpt)))\n",
    "            except Exception as e:\n",
    "                print(\"Failed to move checkpoint locally:\", e)\n",
    "            try:\n",
    "                shutil.move(preds_local, os.path.join(args.out_dir, os.path.basename(preds_local)))\n",
    "            except Exception as e:\n",
    "                print(\"Failed to move preds csv locally:\", e)\n",
    "            try:\n",
    "                shutil.move(metrics_local, os.path.join(args.out_dir, os.path.basename(metrics_local)))\n",
    "            except Exception as e:\n",
    "                print(\"Failed to move metrics json locally:\", e)\n",
    "            try:\n",
    "                shutil.move(csvlog, os.path.join(args.out_dir, os.path.basename(csvlog)))\n",
    "            except Exception as e:\n",
    "                print(\"Failed to move csv log locally:\", e)\n",
    "            dst_tb = os.path.join(args.out_dir, \"tensorboard\")\n",
    "            try:\n",
    "                if os.path.exists(dst_tb):\n",
    "                    shutil.rmtree(dst_tb)\n",
    "                shutil.move(tb_logdir, dst_tb)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to move tensorboard logs locally:\", e)\n",
    "\n",
    "        print(\"All artifacts uploaded to\", args.out_dir)\n",
    "    finally:\n",
    "        try:\n",
    "            shutil.rmtree(local_work)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# -------------------- CLI --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    interactive = (\"ipykernel\" in sys.modules) or (\"google.colab\" in sys.modules)\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument(\"--processed_npz\", type=str, default=DEFAULTS[\"processed_npz\"], help=\"Processed dataset .npz (local or gs://)\")\n",
    "    parser.add_argument(\"--samples_dir\", type=str, default=DEFAULTS[\"samples_dir\"], help=\"Folder with samples_train/val/test CSVs (local or gs://)\")\n",
    "    parser.add_argument(\"--image_dir\", type=str, default=DEFAULTS[\"image_dir\"], help=\"Folder/prefix with .npy or .tif files (local or gs://)\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=DEFAULTS[\"out_dir\"], help=\"Output directory (local or gs://)\")\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=DEFAULTS[\"seq_len\"])\n",
    "    parser.add_argument(\"--img_h\", type=int, default=DEFAULTS[\"img_h\"])\n",
    "    parser.add_argument(\"--img_w\", type=int, default=DEFAULTS[\"img_w\"])\n",
    "    parser.add_argument(\"--img_ch\", type=int, default=DEFAULTS[\"img_ch\"])\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=DEFAULTS[\"batch_size\"])\n",
    "    parser.add_argument(\"--epochs\", type=int, default=DEFAULTS[\"epochs\"])\n",
    "    parser.add_argument(\"--patience\", type=int, default=DEFAULTS[\"patience\"])\n",
    "    parser.add_argument(\"--use_attention\", action=\"store_true\", dest=\"use_attention\")\n",
    "    parser.add_argument(\"--use_env\", action=\"store_true\", dest=\"use_env\")\n",
    "    parser.add_argument(\"--use_mask_channel\", action=\"store_true\", dest=\"use_mask_channel\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=DEFAULTS[\"threshold\"])\n",
    "    parser.add_argument(\"--lr\", type=float, default=DEFAULTS[\"lr\"])\n",
    "    parser.add_argument(\"--clipnorm\", type=float, default=DEFAULTS[\"clipnorm\"])\n",
    "    parser.add_argument(\"--min_valid_pixels\", type=int, default=DEFAULTS[\"min_valid_pixels\"])\n",
    "    parser.add_argument(\"--compute_pos_weight\", action=\"store_true\")\n",
    "    parser.add_argument(\"--pos_weight\", type=float, default=DEFAULTS[\"pos_weight\"])\n",
    "    parser.add_argument(\"--pos_weight_override\", type=float, default=None, help=\"Force positive pixel weight (overrides computed pos weight)\")\n",
    "    parser.add_argument(\"--use_focal_loss\", action=\"store_true\", help=\"Use focal loss instead of BCE\")\n",
    "    parser.add_argument(\"--agg_percentile\", type=float, default=90.0, help=\"Sample-level aggregation percentile (e.g. 90)\")\n",
    "    parser.add_argument(\"--pad_value\", type=float, default=DEFAULTS[\"pad_value\"], help=\"Value to use when padding missing historical images (default 0.0)\")\n",
    "    parser.add_argument(\"--image_binary\", action=\"store_true\", help=\"Treat target images as already binary (0/1). If set, skip thresholding and treat non-zero pixels as positive.\")\n",
    "    parser.add_argument(\"--use_gcsfs\", action=\"store_true\", help=\"Attempt gcsfs for reads if storage client fails (default: prefer storage client)\")\n",
    "    parser.add_argument(\"--requester_pays_project\", type=str, default=None, help=\"GCP project id for requester-pays buckets (optional)\")\n",
    "    parser.add_argument(\"--scale_env_features\", action=\"store_true\", help=\"If set, scale env features using StandardScaler (saved to out_dir)\")\n",
    "    parser.add_argument(\"--workers\", type=int, default=4, help=\"Number of worker processes for Sequence data loading (0 disables multiprocessing).\")\n",
    "    parser.add_argument(\"--use_multiprocessing\", action=\"store_true\", help=\"If set, use multiprocessing for Sequence data loading (requires workers>0).\")\n",
    "    parser.add_argument(\"--tag\", type=str, default=\"pig\", help=\"Optional tag to append to artifact filenames to avoid overwriting (default: pig). Use empty string to disable tagging.\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if interactive and unknown:\n",
    "        pass\n",
    "    if interactive:\n",
    "        defaults_ns = SimpleNamespace(**DEFAULTS)\n",
    "        for k, v in vars(args).items():\n",
    "            if v is not None:\n",
    "                setattr(defaults_ns, k, v)\n",
    "        args = defaults_ns\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0677ec1-9103-484f-b9c9-f75558fdbc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
